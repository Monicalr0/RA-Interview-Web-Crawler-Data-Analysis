Title,Description,Views,Votes,Answers,URL,Tag
Can scrapy be used to scrape dynamic content from websites that are using AJAX?,"
I have recently been learning Python and am dipping my hand into building a web-scraper.  It's nothing fancy at all; its only purpose is to get the data off of a betting website and have this data put into Excel.
Most of the issues are solvable and I'm having a good little mess around. However I'm hitting a massive hurdle over one issue. If a site loads a table of horses and lists current betting prices this information is not in any source file. The clue is that this data is live sometimes, with the numbers being updated obviously from some remote server. The HTML on my PC simply has a hole where their servers are pushing through all the interesting data that I need.
Now my experience with dynamic web content is low, so this thing is something I'm having trouble getting my head around. 
I think Java or Javascript is a key, this pops up often. 
The scraper is simply a odds comparison engine.  Some sites have APIs but I need this for those that don't. I'm using the scrapy library with Python 2.7
I do apologize if this question is too open-ended. In short, my question is: how can scrapy be used to scrape this dynamic data so that I can use it?  So that I can scrape this betting odds data in real-time?
",146k,"
            164
        ","['\nHere is a simple example of  scrapy with an AJAX request. Let see the site rubin-kazan.ru.\nAll messages are loaded with an AJAX request. My goal is to fetch these messages with all their attributes (author, date, ...):\n\nWhen I analyze the source code of the page I can\'t see all these messages because the web page uses AJAX technology. But I can with Firebug from Mozilla Firefox (or an equivalent tool in other browsers) to analyze the HTTP request that generate the messages on the web page:\n\nIt doesn\'t reload the whole page but only the parts of the page that contain messages. For this purpose I click an arbitrary number of page on the bottom:\n\nAnd I observe the HTTP request that is responsible for message body:\n\nAfter finish, I analyze the headers of the request (I must quote that this URL I\'ll extract from source page from var section, see the code below):\n\nAnd the form data content of the request (the HTTP method is ""Post""):\n\nAnd the content of response, which is a JSON file:\n\nWhich presents all the information I\'m looking for.\nFrom now, I must implement all this knowledge in scrapy. Let\'s define the spider for this purpose:\nclass spider(BaseSpider):\n    name = \'RubiGuesst\'\n    start_urls = [\'http://www.rubin-kazan.ru/guestbook.html\']\n\n    def parse(self, response):\n        url_list_gb_messages = re.search(r\'url_list_gb_messages=""(.*)""\', response.body).group(1)\n        yield FormRequest(\'http://www.rubin-kazan.ru\' + url_list_gb_messages, callback=self.RubiGuessItem,\n                          formdata={\'page\': str(page + 1), \'uid\': \'\'})\n\n    def RubiGuessItem(self, response):\n        json_file = response.body\n\nIn parse function I have the response for first request.\nIn RubiGuessItem I have the JSON file with all information. \n', ""\nWebkit based browsers (like Google Chrome or Safari) has built-in developer tools. In Chrome you can open it Menu->Tools->Developer Tools. The Network tab allows you to see all information about every request and response:\n\nIn the bottom of the picture you can see that I've filtered request down to XHR - these are requests made by javascript code.\nTip: log is cleared every time you load a page, at the bottom of the picture, the black dot button will preserve log.\nAfter analyzing requests and responses you can simulate these requests from your web-crawler and extract valuable data. In many cases it will be easier to get your data than parsing HTML, because that data does not contain presentation logic and is formatted to be accessed by javascript code.\nFirefox has similar extension, it is called firebug. Some will argue that firebug is even more powerful but I like the simplicity of webkit.\n"", '\nMany times when crawling we run into problems where content that is rendered on the page is generated with Javascript and therefore scrapy is unable to crawl for it (eg. ajax requests, jQuery craziness).\nHowever, if you use Scrapy along with the web testing framework Selenium then we are able to crawl anything displayed in a normal web browser.\nSome things to note:\n\nYou must have the Python version of Selenium RC installed for this to work, and you must have set up Selenium properly.  Also this is just a template crawler.  You could get much crazier and more advanced with things but I just wanted to show the basic idea.  As the code stands now you will be doing two requests for any given url.  One request is made by Scrapy and the other is made by Selenium.  I am sure there are ways around this so that you could possibly just make Selenium do the one and only request but I did not bother to implement that and by doing two requests you get to crawl the page with Scrapy too.\nThis is quite powerful because now you have the entire rendered DOM available for you to crawl and you can still use all the nice crawling features in Scrapy.  This will make for slower crawling of course but depending on how much you need the rendered DOM it might be worth the wait.\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.http import Request\n\nfrom selenium import selenium\n\nclass SeleniumSpider(CrawlSpider):\n    name = ""SeleniumSpider""\n    start_urls = [""http://www.domain.com""]\n\n    rules = (\n        Rule(SgmlLinkExtractor(allow=(\'\\.html\', )), callback=\'parse_page\',follow=True),\n    )\n\n    def __init__(self):\n        CrawlSpider.__init__(self)\n        self.verificationErrors = []\n        self.selenium = selenium(""localhost"", 4444, ""*chrome"", ""http://www.domain.com"")\n        self.selenium.start()\n\n    def __del__(self):\n        self.selenium.stop()\n        print self.verificationErrors\n        CrawlSpider.__del__(self)\n\n    def parse_page(self, response):\n        item = Item()\n\n        hxs = HtmlXPathSelector(response)\n        #Do some XPath selection with Scrapy\n        hxs.select(\'//div\').extract()\n\n        sel = self.selenium\n        sel.open(response.url)\n\n        #Wait for javscript to load in Selenium\n        time.sleep(2.5)\n\n        #Do some crawling of javascript created content with Selenium\n        sel.get_text(""//div"")\n        yield item\n\n# Snippet imported from snippets.scrapy.org (which no longer works)\n# author: wynbennett\n# date  : Jun 21, 2011\n\n\nReference: http://snipplr.com/view/66998/\n', '\nAnother solution would be to implement a download handler or download handler middleware. (see scrapy docs for more information on downloader middleware) The following is an example class using selenium with headless phantomjs webdriver: \n1) Define class within the middlewares.py script.\nfrom selenium import webdriver\nfrom scrapy.http import HtmlResponse\n\nclass JsDownload(object):\n\n    @check_spider_middleware\n    def process_request(self, request, spider):\n        driver = webdriver.PhantomJS(executable_path=\'D:\\phantomjs.exe\')\n        driver.get(request.url)\n        return HtmlResponse(request.url, encoding=\'utf-8\', body=driver.page_source.encode(\'utf-8\'))\n\n2) Add JsDownload() class to variable DOWNLOADER_MIDDLEWARE within settings.py:\nDOWNLOADER_MIDDLEWARES = {\'MyProj.middleware.MiddleWareModule.MiddleWareClass\': 500}\n\n3) Integrate the HTMLResponse within your_spider.py. Decoding the response body will get you the desired output.\nclass Spider(CrawlSpider):\n    # define unique name of spider\n    name = ""spider""\n\n    start_urls = [""https://www.url.de""] \n\n    def parse(self, response):\n        # initialize items\n        item = CrawlerItem()\n\n        # store data as items\n        item[""js_enabled""] = response.body.decode(""utf-8"") \n\nOptional Addon: \nI wanted the ability to tell different spiders which middleware to use so I implemented this wrapper:\ndef check_spider_middleware(method):\n@functools.wraps(method)\ndef wrapper(self, request, spider):\n    msg = \'%%s %s middleware step\' % (self.__class__.__name__,)\n    if self.__class__ in spider.middleware:\n        spider.log(msg % \'executing\', level=log.DEBUG)\n        return method(self, request, spider)\n    else:\n        spider.log(msg % \'skipping\', level=log.DEBUG)\n        return None\n\nreturn wrapper\n\nfor wrapper to work all spiders must have at minimum:\nmiddleware = set([])\n\nto include a middleware:\nmiddleware = set([MyProj.middleware.ModuleName.ClassName])\n\nAdvantage: \nThe main advantage to implementing it this way rather than in the spider is that you only end up making one request. In A T\'s solution for example: The download handler processes the request and then hands off the response to the spider. The spider then makes a brand new request in it\'s parse_page function -- That\'s two requests for the same content.\n', '\nI was using a custom downloader middleware, but wasn\'t very happy with it, as I didn\'t manage to make the cache work with it.\nA better approach was to implement a custom download handler.\nThere is a working example here. It looks like this:\n# encoding: utf-8\nfrom __future__ import unicode_literals\n\nfrom scrapy import signals\nfrom scrapy.signalmanager import SignalManager\nfrom scrapy.responsetypes import responsetypes\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom selenium import webdriver\nfrom six.moves import queue\nfrom twisted.internet import defer, threads\nfrom twisted.python.failure import Failure\n\n\nclass PhantomJSDownloadHandler(object):\n\n    def __init__(self, settings):\n        self.options = settings.get(\'PHANTOMJS_OPTIONS\', {})\n\n        max_run = settings.get(\'PHANTOMJS_MAXRUN\', 10)\n        self.sem = defer.DeferredSemaphore(max_run)\n        self.queue = queue.LifoQueue(max_run)\n\n        SignalManager(dispatcher.Any).connect(self._close, signal=signals.spider_closed)\n\n    def download_request(self, request, spider):\n        """"""use semaphore to guard a phantomjs pool""""""\n        return self.sem.run(self._wait_request, request, spider)\n\n    def _wait_request(self, request, spider):\n        try:\n            driver = self.queue.get_nowait()\n        except queue.Empty:\n            driver = webdriver.PhantomJS(**self.options)\n\n        driver.get(request.url)\n        # ghostdriver won\'t response when switch window until page is loaded\n        dfd = threads.deferToThread(lambda: driver.switch_to.window(driver.current_window_handle))\n        dfd.addCallback(self._response, driver, spider)\n        return dfd\n\n    def _response(self, _, driver, spider):\n        body = driver.execute_script(""return document.documentElement.innerHTML"")\n        if body.startswith(""<head></head>""):  # cannot access response header in Selenium\n            body = driver.execute_script(""return document.documentElement.textContent"")\n        url = driver.current_url\n        respcls = responsetypes.from_args(url=url, body=body[:100].encode(\'utf8\'))\n        resp = respcls(url=url, body=body, encoding=""utf-8"")\n\n        response_failed = getattr(spider, ""response_failed"", None)\n        if response_failed and callable(response_failed) and response_failed(resp, driver):\n            driver.close()\n            return defer.fail(Failure())\n        else:\n            self.queue.put(driver)\n            return defer.succeed(resp)\n\n    def _close(self):\n        while not self.queue.empty():\n            driver = self.queue.get_nowait()\n            driver.close()\n\nSuppose your scraper is called ""scraper"". If you put the mentioned code inside a file called handlers.py on the root of the ""scraper"" folder, then you could add to your settings.py:\nDOWNLOAD_HANDLERS = {\n    \'http\': \'scraper.handlers.PhantomJSDownloadHandler\',\n    \'https\': \'scraper.handlers.PhantomJSDownloadHandler\',\n}\n\nAnd voilà, the JS parsed DOM, with scrapy cache, retries, etc.\n', ""\n\nhow can scrapy be used to scrape this dynamic data so that I can use\n  it?\n\nI wonder why no one has posted the solution using Scrapy only. \nCheck out the blog post from Scrapy team SCRAPING INFINITE SCROLLING PAGES\n. The example scraps http://spidyquotes.herokuapp.com/scroll website which uses infinite scrolling. \nThe idea is to use Developer Tools of your browser and notice the AJAX requests, then based on that information create the requests for Scrapy.\nimport json\nimport scrapy\n\n\nclass SpidyQuotesSpider(scrapy.Spider):\n    name = 'spidyquotes'\n    quotes_base_url = 'http://spidyquotes.herokuapp.com/api/quotes?page=%s'\n    start_urls = [quotes_base_url % 1]\n    download_delay = 1.5\n\n    def parse(self, response):\n        data = json.loads(response.body)\n        for item in data.get('quotes', []):\n            yield {\n                'text': item.get('text'),\n                'author': item.get('author', {}).get('name'),\n                'tags': item.get('tags'),\n            }\n        if data['has_next']:\n            next_page = data['page'] + 1\n            yield scrapy.Request(self.quotes_base_url % next_page)\n\n"", '\nData that generated from external url which is API calls HTML response as POST method.\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass TestSpider(scrapy.Spider):\n    name = \'test\'  \n    def start_requests(self):\n        url = \'https://howlongtobeat.com/search_results?page=1\'\n        payload = ""queryString=&t=games&sorthead=popular&sortd=0&plat=&length_type=main&length_min=&length_max=&v=&f=&g=&detail=&randomize=0""\n        headers = {\n            ""content-type"":""application/x-www-form-urlencoded"",\n            ""user-agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36""\n        }\n\n        yield scrapy.Request(url,method=\'POST\', body=payload,headers=headers,callback=self.parse)\n\n    def parse(self, response):\n        cards = response.css(\'div[class=""search_list_details""]\')\n\n        for card in cards: \n            game_name = card.css(\'a[class=text_white]::attr(title)\').get()\n            yield {\n                ""game_name"":game_name\n            }\n           \n\nif __name__ == ""__main__"":\n    process =CrawlerProcess()\n    process.crawl(TestSpider)\n    process.start()\n\n', ""\nThere are a few more modern alternatives in 2022 that I think should be mentioned, and I would like to list some pros and cons for the methods discussed in the more popular answers to this question.\n\nThe top answer and several others discuss using the browsers dev tools or packet capturing software to try to identify patterns in response url's, and try to re-construct them to use as scrapy.Requests.\n\nPros: This is still the best option in my opinion, and when it is available it is quick and often times simpler than even the traditional approach i.e. extracting content from the HTML using xpath and css selectors.\n\nCons: Unfortunately this is only available on a fraction of dynamic sites and frequently websites have security measures in place that make using this strategy difficult.\n\n\n\nUsing Selenium Webdriver is the other approach mentioned a lot in previous answers.\n\nPros: It's easy to implement, and integrate into the scrapy workflow. Additionally there are a ton of examples, and requires very little configuration if you use 3rd-party extensions like scrapy-selenium\n\nCons: It's slow!  One of scrapy's key features is it's asynchronous workflow that makes it easy to crawl dozens or even hundreds of pages in seconds. Using selenium cuts this down significantly.\n\n\n\n\nThere are two new methods that defenitely worth consideration, scrapy-splash and scrapy-playwright.\nscrapy-splash:\n\nA scrapy plugin that integrates splash, a javascript rendering service created  and maintained by the developers of scrapy, into the scrapy workflow. The plugin can be installed from pypi with pip3 install scrapy-splash, while splash needs to run in it's own process, and is easiest to run from a docker container.\n\nscrapy-playwright:\n\nPlaywright is a browser automation tool kind of like selenium, but without the crippling decrease in speed that comes with using selenium. Playwright has no issues fitting into the asynchronous scrapy workflow making sending requests just as quick as using scrapy alone. It is also much easier to install and integrate than selenium. The scrapy-playwright plugin is maintained by the developers of scrapy as well, and after installing via pypi with pip3 install scrapy-playwright is as easy as running playwright install in the terminal.\n\nMore details and many examples can be found at each of the plugin's github pages https://github.com/scrapy-plugins/scrapy-playwright and https://github.com/scrapy-plugins/scrapy-splash.\np.s.  Both projects tend to work better in a linux environment in my experience. for windows users i recommend using it with The Windows Subsystem for Linux(wsl).\n"", '\nYes, Scrapy can scrape dynamic websites, website that are rendered through JavaScript.\nThere are Two approaches to scrapy these kind of websites.\n\nyou can use splash to render Javascript code and then parse the rendered HTML.\nyou can find the doc and project here Scrapy splash, git\n\nas previously stated, by monitoring the network calls, yes, you can find the API call that fetch the data and mock that call in your scrapy spider might help you to get desired data.\n\n\n', '\nI handle the ajax request by using Selenium and the Firefox web driver. It is not that fast if you need the crawler as a daemon, but much better than any manual solution.\n']",https://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax,screen-scraping
PhantomJS failing to open HTTPS site,"
I'm using the following code based on loadspeed.js example to open up a https:// site which requires http server authentication as well.
var page = require('webpage').create(), system = require('system'), t, address;

page.settings.userName = 'myusername';
page.settings.password = 'mypassword';

if (system.args.length === 1) {
    console.log('Usage: scrape.js <some URL>');
    phantom.exit();
} else {
    t = Date.now();
    address = system.args[1];
    page.open(address, function (status) {
        if (status !== 'success') {
            console.log('FAIL to load the address');
        } else {
            t = Date.now() - t;
            console.log('Page title is ' + page.evaluate(function () {
                return document.title;
            }));
            console.log('Loading time ' + t + ' msec');
        }
        phantom.exit();
    });
}  

Its failing to load the page all the time. What could be wrong here? Are secured sites to be handled any differently? The site can be accessed successfully from browser though.
I'm just starting with Phantom right now and find it too good to stop playing around even though i'm not moving forward with this issue.
",77k,"
            104
        ","[""\nI tried Fred's and Cameron Tinker's answers, but only --ssl-protocol=any option seem to help me:\nphantomjs --ssl-protocol=any test.js\n\nAlso I think it should be way safer to use --ssl-protocol=any as you still are using encryption, but --ignore-ssl-errors=true will ignore (duh) all ssl errors, including malicious ones.\n"", ""\nThe problem is most likely due to SSL certificate errors. If you start phantomjs with the --ignore-ssl-errors=yes option, it should proceed to load the page as it would if there were no SSL errors:\nphantomjs --ignore-ssl-errors=yes [phantomOptions] script.js [scriptOptions]\n\nI've seen a few websites having problems with incorrectly implementing their SSL certificates or they've expired, etc. A complete list of command line options for phantomjs is available here: http://phantomjs.org/api/command-line.html.\n"", '\nNote that as of 2014-10-16, PhantomJS defaults to using SSLv3 to open HTTPS connections. With the POODLE vulnerability recently announced, many servers are disabling SSLv3 support.\nTo get around that, you should be able to run PhantomJS with:\nphantomjs --ssl-protocol=tlsv1\n\nHopefully, PhantomJS will be updated soon to make TLSv1 the default instead of SSLv3.\n', '\nexperienced same issue...\n--ignore-ssl-errors=yes was not enough to fix it for me,\nhad to do two more things:\n\n1) change user-agent\n\n2) tried all ssl-protocols, the only one that worked was tlsv1 for the page in question\n\nHope this helps...\n', '\nI experienced the same problem (casperjs 1.1.0-beta3/phantomjs 1.9.7). Using --ignore-ssl-errors=yes and --ssl-protocol=tlsv1 solved it. Using only one of the options did not solve it for me.\n', '\nI was receiving \n\nError creating SSL context"" from phantomJS (running on CentOS 6.6)\n\nBuilding from source fixed it for me. Don\'t forget to use the phantomjs that you built. (instead of the /usr/local/bin/phantomjs if you have it)\nsudo yum -y install gcc gcc-c++ make flex bison gperf ruby openssl-devel freetype-devel fontconfig-devel libicu-devel sqlite-devel libpng-devel libjpeg-devel\ngit clone git://github.com/ariya/phantomjs.git\ncd phantomjs\ngit checkout 2.0\n./build.sh\ncd bin/\n./phantomjs <your JS file>\n\n', '\nIf someone is using Phantomjs with Sahi the --ignore-ssl-errors option needs to go in your browser_types.xml file. It worked for me.\n<browserType>\n    <name>phantomjs</name>\n    <displayName>PhantomJS</displayName>\n    <icon>safari.png</icon>\n    <path>/usr/local/Cellar/phantomjs/1.9.2/bin/phantomjs</path>\n    <options>--ignore-ssl-errors=yes --debug=yes --proxy=localhost:9999 /usr/local/Cellar/phantomjs/phantom-sahi.js</options>\n    <processName>""PhantomJS""</processName>\n    <capacity>100</capacity>\n    <force>true</force>\n</browserType>\n\n', ""\nWhat about shebang?\nIf you're using shebang to execute phantomjs scripts, use the following shebang line\n#!/usr/bin/phantomjs --ignore-ssl-errors=yes\n    \nvar system = require('system');\nvar webpage = require('webpage');\n\n// ... rest of your script\n\nUse any of the above answers. i personally like --ignore-ssl-errors=yes since it's irrelevant to validate my loopback web servers' self-signed certificate.\n"", '\nNone of the other answers here helped me; it may be that the specific site(s) I was working with were too picky with their HTTP headers. This is what worked:\nvar page = webpage.create();\npage.customHeaders = {\n    ""Connection"": ""keep-alive""\n};\n\nI found out that PhantomJS was using ""Keep-Alive"" (capitalized), and the connection was not being kept alive. :)\n', '\nI was getting SSL Handshake Failed yesterday. I tried many combinations of phantomJS options (--ignore-ssl-errors=yes etc.), but none of them worked.\nUpgrading to phantomJS 2.1.1 fixed it. \nI used the phantomJS installation instructions at https://gist.github.com/julionc/7476620, changing the phantomJS version to 2.1.1.\n', '\nOn the machine you are trying to run phantomjs on to connect to a remote server, run ""openssl ciphers."" Copy and paste the ciphers listed into the --ssl-ciphers="""" command line option. This tells the connecting web server which ciphers are available to use to communicate with your client. If you don\'t set the ones available on your own machine, it can use any cipher your machine does not understand that the default modern browsers do that are used for the default setting.\n', '\nphantomjs --web-security=false --ignore-ssl-errors=true scripts.js\n', '\nThe only thing that worked for me was upping phantomjs from 1.9x to 2.x ;)\n']",https://stackoverflow.com/questions/12021578/phantomjs-failing-to-open-https-site,screen-scraping
Web scraping with Python [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?
",214k,"
            196
        ","[""\nUse urllib2 in combination with the brilliant BeautifulSoup library:\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n# or if you're using BeautifulSoup4:\n# from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen('http://example.com').read())\n\nfor row in soup('table', {'class': 'spad'})[0].tbody('tr'):\n    tds = row('td')\n    print tds[0].string, tds[1].string\n    # will print date and sunrise\n\n"", ""\nI'd really recommend Scrapy.\nQuote from a deleted answer:\n\n\nScrapy crawling is fastest than mechanize because uses asynchronous operations (on top of Twisted).\nScrapy has better and fastest support for parsing (x)html on top of libxml2.\nScrapy is a mature framework with full unicode, handles redirections, gzipped responses, odd encodings, integrated http cache, etc.\nOnce you are into Scrapy, you can write a spider in less than 5 minutes that download images, creates thumbnails and export the extracted data directly to csv or json.\n\n\n"", '\nI collected together scripts from my web scraping work into this bit-bucket library.\nExample script for your case:\nfrom webscraping import download, xpath\nD = download.Download()\n\nhtml = D.get(\'http://example.com\')\nfor row in xpath.search(html, \'//table[@class=""spad""]/tbody/tr\'):\n    cols = xpath.search(row, \'/td\')\n    print \'Sunrise: %s, Sunset: %s\' % (cols[1], cols[2])\n\nOutput:\nSunrise: 08:39, Sunset: 16:08\nSunrise: 08:39, Sunset: 16:09\nSunrise: 08:39, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:10\nSunrise: 08:40, Sunset: 16:11\nSunrise: 08:40, Sunset: 16:12\nSunrise: 08:40, Sunset: 16:13\n\n', ""\nI would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.\nFor your case, it would be something like:\nfrom pyquery import *\n\nhtml = PyQuery(url='http://www.example.com/')\ntrs = html('table.spad tbody tr')\n\nfor tr in trs:\n  tds = tr.getchildren()\n  print tds[1].text, tds[2].text\n\nOutput:\n5:16 AM 9:28 PM\n5:15 AM 9:30 PM\n5:13 AM 9:31 PM\n5:12 AM 9:33 PM\n5:11 AM 9:34 PM\n5:10 AM 9:35 PM\n5:09 AM 9:37 PM\n\n"", ""\nYou can use urllib2 to make the HTTP requests, and then you'll have web content.\nYou can get it like this:\nimport urllib2\nresponse = urllib2.urlopen('http://example.com')\nhtml = response.read()\n\nBeautiful Soup is a python HTML parser that is supposed to be good for screen scraping.\nIn particular, here is their tutorial on parsing an HTML document.\nGood luck!\n"", '\nI use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.\nExample for scraping your website:\n\nimport sys\nfrom pprint import pprint\nfrom scrapemark import scrape\n\npprint(scrape(""""""\n    <table class=""spad"">\n        <tbody>\n            {*\n                <tr>\n                    <td>{{[].day}}</td>\n                    <td>{{[].sunrise}}</td>\n                    <td>{{[].sunset}}</td>\n                    {# ... #}\n                </tr>\n            *}\n        </tbody>\n    </table>\n"""""", url=sys.argv[1] ))\n\nUsage:\npython2 sunscraper.py http://www.example.com/\n\nResult:\n[{\'day\': u\'1. Dez 2012\', \'sunrise\': u\'08:18\', \'sunset\': u\'16:10\'},\n {\'day\': u\'2. Dez 2012\', \'sunrise\': u\'08:19\', \'sunset\': u\'16:10\'},\n {\'day\': u\'3. Dez 2012\', \'sunrise\': u\'08:21\', \'sunset\': u\'16:09\'},\n {\'day\': u\'4. Dez 2012\', \'sunrise\': u\'08:22\', \'sunset\': u\'16:09\'},\n {\'day\': u\'5. Dez 2012\', \'sunrise\': u\'08:23\', \'sunset\': u\'16:08\'},\n {\'day\': u\'6. Dez 2012\', \'sunrise\': u\'08:25\', \'sunset\': u\'16:08\'},\n {\'day\': u\'7. Dez 2012\', \'sunrise\': u\'08:26\', \'sunset\': u\'16:07\'}]\n\n', '\nMake your life easier by using CSS Selectors\nI know I have come late to party but I have a nice suggestion for you.\nUsing BeautifulSoup is already been suggested I would rather prefer using CSS Selectors to scrape data inside HTML\nimport urllib2\nfrom bs4 import BeautifulSoup\n\nmain_url = ""http://www.example.com""\n\nmain_page_html  = tryAgain(main_url)\nmain_page_soup = BeautifulSoup(main_page_html)\n\n# Scrape all TDs from TRs inside Table\nfor tr in main_page_soup.select(""table.class_of_table""):\n   for td in tr.select(""td#id""):\n       print(td.text)\n       # For acnhors inside TD\n       print(td.select(""a"")[0].text)\n       # Value of Href attribute\n       print(td.select(""a"")[0][""href""])\n\n# This is method that scrape URL and if it doesnt get scraped, waits for 20 seconds and then tries again. (I use it because my internet connection sometimes get disconnects)\ndef tryAgain(passed_url):\n    try:\n        page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n        return page\n    except Exception:\n        while 1:\n            print(""Trying again the URL:"")\n            print(passed_url)\n            try:\n                page  = requests.get(passed_url,headers = random.choice(header), timeout = timeout_time).text\n                print(""-------------------------------------"")\n                print(""---- URL was successfully scraped ---"")\n                print(""-------------------------------------"")\n                return page\n            except Exception:\n                time.sleep(20)\n                continue \n\n', '\nIf we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:\nimport requests ; from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(requests.get(\'https://www.flipkart.com/\').text, ""lxml"")\nfor link in soup.select(\'div._2kSfQ4\'):\n    print(link.text)\n\nThis is the partial search results:\nPuma, USPA, Adidas & moreUp to 70% OffMen\'s Shoes\nShirts, T-Shirts...Under ₹599For Men\nNike, UCB, Adidas & moreUnder ₹999Men\'s Sandals, Slippers\nPhilips & moreStarting ₹99LED Bulbs & Emergency Lights\n\n', '\nHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who\'s class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.\nimport requests\nfrom bs4 import BeautifulSoup\ndef crawl_flipkart():\n    url = \'https://www.flipkart.com/\'\n    source_code = requests.get(url)\n    plain_text = source_code.text\n    soup = BeautifulSoup(plain_text, ""lxml"")\n    for link in soup.findAll(\'a\', {\'class\': \'_3NFO0d\'}):\n        href = link.get(\'href\')\n        print(href)\n\ncrawl_flipkart()\n\n', '\nPython has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. \n\n\nThe easier one is to use requests and beautiful soup. Before starting give one hour of time to go through the documentation, it will solve most of your doubts. BS4 offer wide range of parsers that you can opt for. Use user-agent and sleep to make scraping easier. BS4 returns a bs.tag so use variable[0]. If there is js running, you wont be able to scrape using requests and bs4 directly. You  could get the api link then parse the JSON to get the information you need or try selenium.  \n']",https://stackoverflow.com/questions/2081586/web-scraping-with-python,screen-scraping
scrape html generated by javascript with python,"
I need to scrape a site with python. I obtain the source html code with the urlib module, but I need to scrape also some html code that is generated by a javascript function (which is included in the html source). What this functions does ""in"" the site is that when you press a button it outputs some html code. How can I ""press"" this button with python code? Can scrapy help me? I captured the POST request with firebug but when I try to pass it on the url I get a 403 error. Any suggestions?
",18k,"
            18
        ","['\nIn Python, I think Selenium 1.0 is the way to go. It’s a library that allows you to control a real web browser from your language of choice.\nYou need to have the web browser in question installed on the machine your script runs on, but it looks like the most reliable way to programmatically interrogate websites that use a lot of JavaScript.\n', ""\nSince there is no comprehensive answer here, I'll go ahead and write one.\nTo scrape off JS rendered pages, we will need a browser that has a JavaScript engine (e.i, support JavaScript rendering)\nOptions like Mechanize, url2lib will not work since they DO NOT support JavaScript. \nSo here's what you do:\nSetup PhantomJS to run with Selenium. After installing the dependencies for both of them (refer this), you can use the following code as an example to fetch the fully rendered website.\nfrom selenium import webdriver\n\ndriver = webdriver.PhantomJS()\ndriver.get('http://jokes.cc.com/')\nsoupFromJokesCC = BeautifulSoup(driver.page_source) #page_source fetches page after rendering is complete\ndriver.save_screenshot('screen.png') # save a screenshot to disk\n\ndriver.quit()\n\n"", '\nI have had to do this before (in .NET) and you are basically going to have to host a browser, get it to click the button, and then interrogate the DOM (document object model) of the browser to get at the generated HTML.\nThis is definitely one of the downsides to web apps moving towards an Ajax/Javascript approach to generating HTML client-side.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. And here is a full example to execute JavaScript and extract the final HTML.\n', ""\nFor Scrapy (great python scraping framework) there is scrapyjs: an additional downloader handler / middleware handler able to scraping javascript generated content.\nIt's based on webkit engine by pygtk, python-webkit, and python-jswebkit and it's quite simple.\n""]",https://stackoverflow.com/questions/2148493/scrape-html-generated-by-javascript-with-python,screen-scraping
jsoup posting and cookie,"
I'm trying to use jsoup to login to a site and then scrape information, I am running into in a problem, I can login successfully and create a Document from index.php but I cannot get other pages on the site. I know I need to set a cookie after I post and then load it when I'm trying to open another page on the site. But how do I do this? The following code lets me login and get index.php
Document doc = Jsoup.connect(""http://www.example.com/login.php"")
               .data(""username"", ""myUsername"", 
                     ""password"", ""myPassword"")
               .post();

I know I can use apache httpclient to do this but I don't want to. 
",64k,"
            52
        ","['\nWhen you login to the site, it is probably setting an authorised session cookie that needs to be sent on subsequent requests to maintain the session.\nYou can get the cookie like this:\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"", ""password"", ""myPassword"")\n    .method(Method.POST)\n    .execute();\n\nDocument doc = res.parse();\nString sessionId = res.cookie(""SESSIONID""); // you will need to check what the right cookie name is\n\nAnd then send it on the next request like:\nDocument doc2 = Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookie(""SESSIONID"", sessionId)\n    .get();\n\n', '\n//This will get you the response.\nResponse res = Jsoup\n    .connect(""loginPageUrl"")\n    .data(""loginField"", ""login@login.com"", ""passField"", ""pass1234"")\n    .method(Method.POST)\n    .execute();\n\n//This will get you cookies\nMap<String, String> loginCookies = res.cookies();\n\n//And this is the easiest way I\'ve found to remain in session\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"")\n      .cookies(loginCookies)\n      .get();\n\n', '\nWhere the code was:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies().get(); \n\nI was having difficulties until I changed it to:\nDocument doc = Jsoup.connect(""urlYouNeedToBeLoggedInToAccess"").cookies(cookies).get();\n\nNow it is working flawlessly.\n', '\nHere is what you can try...\nimport org.jsoup.Connection;\n\n\nConnection.Response res = null;\n    try {\n        res = Jsoup\n                .connect(""http://www.example.com/login.php"")\n                .data(""username"", ""your login id"", ""password"", ""your password"")\n                .method(Connection.Method.POST)\n                .execute();\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n\nNow save all your cookies and make request to the other page you want.\n//Store Cookies\ncookies = res.cookies();\n\nMaking request to another page.\ntry {\n    Document doc = Jsoup.connect(""your-second-page-link"").cookies(cookies).get();\n}\ncatch(Exception e){\n    e.printStackTrace();\n}\n\nAsk if further help needed.\n', '\nConnection.Response res = Jsoup.connect(""http://www.example.com/login.php"")\n    .data(""username"", ""myUsername"")\n    .data(""password"", ""myPassword"")\n    .method(Connection.Method.POST)\n    .execute();\n//Connecting to the server with login details\nDocument doc = res.parse();\n//This will give the redirected file\nMap<String,String> cooki=res.cookies();\n//This gives the cookies stored into cooki\nDocument docs= Jsoup.connect(""http://www.example.com/otherPage"")\n    .cookies(cooki)\n    .get();\n//This gives the data of the required website\n\n', '\nWhy reconnect?\nif there are any cookies to avoid 403 Status i do so.\n                Document doc = null;\n                int statusCode = -1;\n                String statusMessage = null;\n                String strHTML = null;\n        \n                try {\n    // connect one time.                \n                    Connection con = Jsoup.connect(urlString);\n    // get response.\n                    Connection.Response res = con.execute();        \n    // get cookies\n                    Map<String, String> loginCookies = res.cookies();\n\n    // print cookie content and status message\n                    if (loginCookies != null) {\n                        for (Map.Entry<String, String> entry : loginCookies.entrySet()) {\n                            System.out.println(entry.getKey() + "":"" + entry.getValue().toString() + ""\\n"");\n                        }\n                    }\n        \n                    statusCode = res.statusCode();\n                    statusMessage = res.statusMessage();\n                    System.out.print(""Status CODE\\n"" + statusCode + ""\\n\\n"");\n                    System.out.print(""Status Message\\n"" + statusMessage + ""\\n\\n"");\n        \n    // set login cookies to connection here\n                    con.cookies(loginCookies).userAgent(""Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0"");\n        \n    // now do whatever you want, get document for example\n                    doc = con.get();\n    // get HTML\n                    strHTML = doc.head().html();\n\n                } catch (org.jsoup.HttpStatusException hse) {\n                    hse.printStackTrace();\n                } catch (IOException ioe) {\n                    ioe.printStackTrace();\n                }\n\n']",https://stackoverflow.com/questions/6432970/jsoup-posting-and-cookie,screen-scraping
How to implement a web scraper in PHP? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



What built-in PHP functions are useful for web scraping?  What are some good resources (web or print) for getting up to speed on web scraping with PHP?
",81k,"
            61
        ","['\nScraping generally encompasses 3 steps: \n\nfirst you GET or POST your request\nto a specified URL \nnext you receive\n    the html that is returned as the\n    response\nfinally you parse out of\n    that html the text you\'d like to\n    scrape.\n\nTo accomplish steps 1 and 2, below is a simple php class which uses Curl to fetch webpages using either GET or POST.  After you get the HTML back, you just use Regular Expressions to accomplish step 3 by parsing out the text you\'d like to scrape.\nFor regular expressions, my favorite tutorial site is the following:\nRegular Expressions Tutorial\nMy Favorite program for working with RegExs is Regex Buddy.  I would advise you to try the demo of that product even if you have no intention of buying it.  It is an invaluable tool and will even generate code for your regexs you make in your language of choice (including php).\nUsage:\n\n\n$curl = new Curl();\n$html = $curl->get(""http://www.google.com"");\n\n// now, do your regex work against $html\n\nPHP Class:\n\n\n<?php\n\nclass Curl\n{       \n\n    public $cookieJar = """";\n\n    public function __construct($cookieJarFile = \'cookies.txt\') {\n        $this->cookieJar = $cookieJarFile;\n    }\n\n    function setup()\n    {\n\n\n        $header = array();\n        $header[0] = ""Accept: text/xml,application/xml,application/xhtml+xml,"";\n        $header[0] .= ""text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5"";\n        $header[] =  ""Cache-Control: max-age=0"";\n        $header[] =  ""Connection: keep-alive"";\n        $header[] = ""Keep-Alive: 300"";\n        $header[] = ""Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7"";\n        $header[] = ""Accept-Language: en-us,en;q=0.5"";\n        $header[] = ""Pragma: ""; // browsers keep this blank.\n\n\n        curl_setopt($this->curl, CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US; rv:1.8.1.7) Gecko/20070914 Firefox/2.0.0.7\');\n        curl_setopt($this->curl, CURLOPT_HTTPHEADER, $header);\n        curl_setopt($this->curl,CURLOPT_COOKIEJAR, $this->cookieJar); \n        curl_setopt($this->curl,CURLOPT_COOKIEFILE, $this->cookieJar);\n        curl_setopt($this->curl,CURLOPT_AUTOREFERER, true);\n        curl_setopt($this->curl,CURLOPT_FOLLOWLOCATION, true);\n        curl_setopt($this->curl,CURLOPT_RETURNTRANSFER, true);  \n    }\n\n\n    function get($url)\n    { \n        $this->curl = curl_init($url);\n        $this->setup();\n\n        return $this->request();\n    }\n\n    function getAll($reg,$str)\n    {\n        preg_match_all($reg,$str,$matches);\n        return $matches[1];\n    }\n\n    function postForm($url, $fields, $referer=\'\')\n    {\n        $this->curl = curl_init($url);\n        $this->setup();\n        curl_setopt($this->curl, CURLOPT_URL, $url);\n        curl_setopt($this->curl, CURLOPT_POST, 1);\n        curl_setopt($this->curl, CURLOPT_REFERER, $referer);\n        curl_setopt($this->curl, CURLOPT_POSTFIELDS, $fields);\n        return $this->request();\n    }\n\n    function getInfo($info)\n    {\n        $info = ($info == \'lasturl\') ? curl_getinfo($this->curl, CURLINFO_EFFECTIVE_URL) : curl_getinfo($this->curl, $info);\n        return $info;\n    }\n\n    function request()\n    {\n        return curl_exec($this->curl);\n    }\n}\n\n?>\n\n\n', '\nI recommend Goutte, a simple PHP Web Scraper.\nExample Usage:-\nCreate a Goutte Client instance (which extends\nSymfony\\Component\\BrowserKit\\Client):\nuse Goutte\\Client;\n\n$client = new Client();\n\nMake requests with the request() method:\n$crawler = $client->request(\'GET\', \'http://www.symfony-project.org/\');\n\nThe request method returns a Crawler object\n(Symfony\\Component\\DomCrawler\\Crawler).\nClick on links:\n$link = $crawler->selectLink(\'Plugins\')->link();\n$crawler = $client->click($link);\n\nSubmit forms:\n$form = $crawler->selectButton(\'sign in\')->form();\n$crawler = $client->submit($form, array(\'signin[username]\' => \'fabien\', \'signin[password]\' => \'xxxxxx\'));\n\nExtract data:\n$nodes = $crawler->filter(\'.error_list\');\n\nif ($nodes->count())\n{\n  die(sprintf(""Authentification error: %s\\n"", $nodes->text()));\n}\n\nprintf(""Nb tasks: %d\\n"", $crawler->filter(\'#nb_tasks\')->text());\n\n', '\nScraperWiki is a pretty interesting project.\nHelps you build scrapers online in Python, Ruby or PHP - i was able to get a simple attempt up in a few minutes.\n', ""\nIf you need something that is easy to maintain, rather than fast to execute, it could help to use a scriptable browser, such as SimpleTest's.\n"", ""\nScraping can be pretty complex, depending on what you want to do. Have a read of this tutorial series on The Basics Of Writing A Scraper In PHP and see if you can get to grips with it.\nYou can use similar methods to automate form sign ups, logins, even fake clicking on Ads! The main limitations with using CURL though are that it doesn't support using javascript, so if you are trying to scrape a site that uses AJAX for pagination for example it can become a little tricky...but again there are ways around that!\n"", '\nhere is another one: a simple PHP Scraper without Regex.\n', '\nfile_get_contents() can take a remote URL and give you the source. You can then use regular expressions (with the Perl-compatible functions) to grab what you need.\nOut of curiosity, what are you trying to scrape?\n', ""\nI'd either use libcurl or Perl's LWP (libwww for perl). Is there a libwww for php?\n"", '\nScraper class from my framework:\n<?php\n\n/*\n    Example:\n\n    $site = $this->load->cls(\'scraper\', \'http://www.anysite.com\');\n    $excss = $site->getExternalCSS();\n    $incss = $site->getInternalCSS();\n    $ids = $site->getIds();\n    $classes = $site->getClasses();\n    $spans = $site->getSpans(); \n\n    print \'<pre>\';\n    print_r($excss);\n    print_r($incss);\n    print_r($ids);\n    print_r($classes);\n    print_r($spans);        \n\n*/\n\nclass scraper\n{\n    private $url = \'\';\n\n    public function __construct($url)\n    {\n        $this->url = file_get_contents(""$url"");\n    }\n\n    public function getInternalCSS()\n    {\n        $tmp = preg_match_all(\'/(style="")(.*?)("")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getExternalCSS()\n    {\n        $tmp = preg_match_all(\'/(href="")(\\w.*\\.css)""/i\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getIds()\n    {\n        $tmp = preg_match_all(\'/(id=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getClasses()\n    {\n        $tmp = preg_match_all(\'/(class=""(\\w*)"")/is\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n    public function getSpans(){\n        $tmp = preg_match_all(\'/(<span>)(.*)(<\\/span>)/\', $this->url, $patterns);\n        $result = array();\n        array_push($result, $patterns[2]);\n        array_push($result, count($patterns[2]));\n        return $result;\n    }\n\n}\n?>\n\n', '\nThe curl library allows you to download web pages. You should look into regular expressions for doing the scraping.\n']",https://stackoverflow.com/questions/26947/how-to-implement-a-web-scraper-in-php,screen-scraping
Headless Browser for Python (Javascript support REQUIRED!) [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.


Closed 8 years ago.







                        Improve this question
                    



I need a headless browser which is fairly easy to use (I am still fairly new to Python and programming in general) which will allow me to navigate to a page, log into a form that requires Javascript, and then scrape the resulting web page by searching for results matching certain criteria, clicking check boxes, and clicking to download files. All of this requires Javascript.
I hear a headless browser is what I want - requirements/preferences are that I be able to run it from Python, and preferably that the resultant script will be compilable by py2exe (I am writing this program for other users).
So far Windmill looks like it MIGHT be what I want, but I am not sure.
Any ideas appreciated!
",43k,"
            57
        ","['\nI use webkit as a headless browser in Python via pyqt / pyside:\nhttp://www.riverbankcomputing.co.uk/software/pyqt/download\nhttp://developer.qt.nokia.com/wiki/Category:LanguageBindings::PySide::Downloads\nI particularly like webkit because it is simple to setup. For Ubuntu you just use: sudo apt-get install python-qt4\nHere is an example script:\nhttp://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n', '\nThe answer to this question was Spynner\n', '\nI\'m in the midst of writing a Python driver for Zombie.js, ""a lightweight framework for testing client-side JavaScript code in a simulated environment"".\nI\'m currently at a standstill on a resolution to a bug in Node.js (before I write more tests and more code), but feel free to keep an eye on my project as it progresses:\nhttps://github.com/ryanpetrello/python-zombie\n', '\nThere are not too many headless browsers yet that support Javascript.\nYou could try Zombie.js or Phantomjs. Those are not Python, but plain Javascript and those really can do the job.\n', '\nTry using phantomjs, it has great javascript support.  Then you could run it as a subprocess of a python script \nhttp://docs.python.org/library/subprocess.html\nthat could boss it around.\n', '\nYou can use HTQL in combination with IRobotSoft webscraper.  Check here for examples: http://htql.net/\n']",https://stackoverflow.com/questions/6025082/headless-browser-for-python-javascript-support-required,screen-scraping
Is there a PHP equivalent of Perl's WWW::Mechanize?,"
I'm looking for a library that has functionality similar to Perl's WWW::Mechanize, but for PHP. Basically, it should allow me to submit HTTP GET and POST requests with a simple syntax, and then parse the resulting page and return in a simple format all forms and their fields, along with all links on the page.
I know about CURL, but it's a little too barebones, and the syntax is pretty ugly (tons of curl_foo($curl_handle, ...) statements
Clarification:
I want something more high-level than the answers so far. For example, in Perl, you could do something like:
# navigate to the main page
$mech->get( 'http://www.somesite.com/' ); 

# follow a link that contains the text 'download this'
$mech->follow_link( text_regex => qr/download this/i );

# submit a POST form, to log into the site
$mech->submit_form(
    with_fields      => {
        username    => 'mungo',
        password    => 'lost-and-alone',
    }
);

# save the results as a file
$mech->save_content('somefile.zip');

To do the same thing using HTTP_Client or wget or CURL would be a lot of work, I'd have to manually parse the pages to find the links, find the form URL, extract all the hidden fields, and so on. The reason I'm asking for a PHP solution is that I have no experience with Perl, and I could probably build what I need with a lot of work, but it would be much quicker if I could do the above in PHP.
",15k,"
            26
        ","[""\nSimpleTest's ScriptableBrowser can be used independendly from the testing framework. I've used it for numerous automation-jobs.\n"", '\nI feel compelled to answer this, even though its an old post... I\'ve been working with PHP curl a lot and it is not as good anywhere near comparable to something like WWW:Mechanize, which I am switching to (I think I am going to go with the Ruby language implementation).. Curl is outdated as it requires too much ""grunt work"" to automate anything, the simpletest scriptable browser looked promising to me but in testing it, it won\'t work on most web forms I try it on... honestly, I think PHP is lacking in this category of scraping, web automation so its best to look at a different language, just wanted to post this since I have spent countless hours on this topic and maybe it will save someone else some time in the future.\n', '\nIt\'s 2016 now and there\'s Mink. It even supports different engines from headless pure-PHP ""browser"" (without JavaScript), over Selenium (which needs a browser like Firefox or Chrome) to a headless ""browser.js"" in NPM, which DOES support JavaScript.\n', '\nTry looking in the PEAR library. If all else fails, create an object wrapper for curl.\nYou can so something simple like this:\nclass curl {\n    private $resource;\n\n    public function __construct($url) {\n        $this->resource = curl_init($url);\n    }\n\n    public function __call($function, array $params) {\n        array_unshift($params, $this->resource);\n        return call_user_func_array(""curl_$function"", $params);\n    }\n}\n\n', ""\nTry one of the following:\n\nPEAR's HTTP_Request\nZend_Http_Client\n\n(Yes, it's ZendFramework code, but it doesn't make your class slower using it since it just loads the required libs.)\n"", '\nLook into Snoopy:\nhttp://sourceforge.net/projects/snoopy/\n', '\nCurl is the way to go for simple requests. It runs cross platform, has a PHP extension and is widely adopted and tested.\nI created a nice class that can GET and POST an array of data (INCLUDING FILES!) to a url by just calling CurlHandler::Get($url, $data) || CurlHandler::Post($url, $data). There\'s an optional HTTP User authentication option too :)\n/**\n * CURLHandler handles simple HTTP GETs and POSTs via Curl \n * \n * @package Pork\n * @author SchizoDuckie\n * @copyright SchizoDuckie 2008\n * @version 1.0\n * @access public\n */\nclass CURLHandler\n{\n\n    /**\n     * CURLHandler::Get()\n     * \n     * Executes a standard GET request via Curl.\n     * Static function, so that you can use: CurlHandler::Get(\'http://www.google.com\');\n     * \n     * @param string $url url to get\n     * @return string HTML output\n     */\n    public static function Get($url)\n    {\n       return self::doRequest(\'GET\', $url);\n    }\n\n    /**\n     * CURLHandler::Post()\n     * \n     * Executes a standard POST request via Curl.\n     * Static function, so you can use CurlHandler::Post(\'http://www.google.com\', array(\'q\'=>\'StackOverFlow\'));\n     * If you want to send a File via post (to e.g. PHP\'s $_FILES), prefix the value of an item with an @ ! \n     * @param string $url url to post data to\n     * @param Array $vars Array with key=>value pairs to post.\n     * @return string HTML output\n     */\n    public static function Post($url, $vars, $auth = false) \n    {\n       return self::doRequest(\'POST\', $url, $vars, $auth);\n    }\n\n    /**\n     * CURLHandler::doRequest()\n     * This is what actually does the request\n     * <pre>\n     * - Create Curl handle with curl_init\n     * - Set options like CURLOPT_URL, CURLOPT_RETURNTRANSFER and CURLOPT_HEADER\n     * - Set eventual optional options (like CURLOPT_POST and CURLOPT_POSTFIELDS)\n     * - Call curl_exec on the interface\n     * - Close the connection\n     * - Return the result or throw an exception.\n     * </pre>\n     * @param mixed $method Request Method (Get/ Post)\n     * @param mixed $url URI to get or post to\n     * @param mixed $vars Array of variables (only mandatory in POST requests)\n     * @return string HTML output\n     */\n    public static function doRequest($method, $url, $vars=array(), $auth = false)\n    {\n        $curlInterface = curl_init();\n\n        curl_setopt_array ($curlInterface, array( \n            CURLOPT_URL => $url,\n            CURLOPT_RETURNTRANSFER => 1,\n            CURLOPT_FOLLOWLOCATION =>1,\n            CURLOPT_HEADER => 0));\n        if (strtoupper($method) == \'POST\')\n        {\n            curl_setopt_array($curlInterface, array(\n                CURLOPT_POST => 1,\n                CURLOPT_POSTFIELDS => http_build_query($vars))\n            );  \n        }\n        if($auth !== false)\n        {\n              curl_setopt($curlInterface, CURLOPT_USERPWD, $auth[\'username\'] . "":"" . $auth[\'password\']);\n        }\n        $result = curl_exec ($curlInterface);\n        curl_close ($curlInterface);\n\n        if($result === NULL)\n        {\n            throw new Exception(\'Curl Request Error: \'.curl_errno($curlInterface) . "" - "" . curl_error($curlInterface));\n        }\n        else\n        {\n            return($result);\n        }\n    }\n\n}\n\n?>\n\n[edit] Read the clarification only now... You probably want to go with one of the tools mentioned above that automates stuff. You could also decide to use a clientside firefox extension like ChickenFoot for more flexibility. I\'ll leave the example class above here for future searches.\n', '\nIf you\'re using CakePHP in your project, or if you\'re inclined to extract the relevant library you can use their curl wrapper HttpSocket. It has the simple page-fetching syntax you describe, e.g., \n# This is the sugar for importing the library within CakePHP       \nApp::import(\'Core\', \'HttpSocket\');\n$HttpSocket = new HttpSocket();\n\n$result = $HttpSocket->post($login_url,\narray(\n  ""username"" => ""username"",\n  ""password"" => ""password""\n)\n);\n\n...although it doesn\'t have a way to parse the response page. For that I\'m going to use simplehtmldom: http://net.tutsplus.com/tutorials/php/html-parsing-and-screen-scraping-with-the-simple-html-dom-library/ which describes itself as having a jQuery-like syntax.\nI tend to agree that the bottom line is that PHP doesn\'t have the awesome scraping/automation libraries that Perl/Ruby have.\n', ""\nIf you're on a *nix system you could use shell_exec() with wget, which has a lot of nice options.\n""]",https://stackoverflow.com/questions/199045/is-there-a-php-equivalent-of-perls-wwwmechanize,screen-scraping
Executing Javascript from Python,"
I have HTML webpages that I am crawling using xpath. The etree.tostring of a certain node gives me this string:
<script>
<!--
function escramble_758(){
  var a,b,c
  a='+1 '
  b='84-'
  a+='425-'
  b+='7450'
  c='9'
  document.write(a+c+b)
}
escramble_758()
//-->
</script>

I just need the output of escramble_758(). I can write a regex to figure out the whole thing, but I want my code to remain tidy. What is the best alternative?
I am zipping through the following libraries, but I didnt see an exact solution. Most of them are trying to emulate browser, making things snail slow.

http://code.google.com/p/python-spidermonkey/ (clearly says it's not yet possible to call a function defined in Javascript)
http://code.google.com/p/webscraping/ (don't see anything for Javascript, I may be wrong)
http://pypi.python.org/pypi/selenium (Emulating browser)

Edit: An example will be great.. (barebones will do)
",176k,"
            67
        ","['\nYou can also use Js2Py which is written in pure python and is able to both execute and translate javascript to python. Supports virtually whole JavaScript even labels, getters, setters and other rarely used features. \nimport js2py\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n"""""".replace(""document.write"", ""return "")\n\nresult = js2py.eval_js(js)  # executing JavaScript and converting the result to python string \n\nAdvantages of Js2Py include portability and extremely easy integration with python (since basically JavaScript is being translated to python). \nTo install:\npip install js2py\n\n', '\nUsing PyV8, I can do this. However, I have to replace document.write with return because there\'s no DOM and therefore no document.\nimport PyV8\nctx = PyV8.JSContext()\nctx.enter()\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\n}\nescramble_758()\n""""""\n\nprint ctx.eval(js.replace(""document.write"", ""return ""))\n\nOr you could create a mock document object\nclass MockDocument(object):\n\n    def __init__(self):\n        self.value = \'\'\n\n    def write(self, *args):\n        self.value += \'\'.join(str(i) for i in args)\n\n\nclass Global(PyV8.JSClass):\n    def __init__(self):\n        self.document = MockDocument()\n\nscope = Global()\nctx = PyV8.JSContext(scope)\nctx.enter()\nctx.eval(js)\nprint scope.document.value\n\n', '\nOne more solution as PyV8 seems to be unmaintained and dependent on the old version of libv8. \nPyMiniRacer It\'s a wrapper around the v8 engine and it works with the new version and is actively maintained.\npip install py-mini-racer\nfrom py_mini_racer import py_mini_racer\nctx = py_mini_racer.MiniRacer()\nctx.eval(""""""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n"""""")\nctx.call(""escramble_758"")\n\nAnd yes, you have to replace document.write with return as others suggested\n', '\nYou can use js2py context to execute your js code and get output from document.write with mock document object:\nimport js2py\n\njs = """"""\nvar output;\ndocument = {\n    write: function(value){\n        output = value;\n    }\n}\n"""""" + your_script\n\ncontext = js2py.EvalJs()\ncontext.execute(js)\nprint(context.output)\n\n', '\nYou can use requests-html which will download and use chromium underneath.\nfrom requests_html import HTML\n\nhtml = HTML(html=""<a href=\'http://www.example.com/\'>"")\n\nscript = """"""\nfunction escramble_758(){\n    var a,b,c\n    a=\'+1 \'\n    b=\'84-\'\n    a+=\'425-\'\n    b+=\'7450\'\n    c=\'9\'\n    return a+c+b;\n}\n""""""\n\nval = html.render(script=script, reload=False)\nprint(val)\n# +1 425-984-7450\n\nMore on this read here\n', '\nquickjs should be the best option after quickjs come out.  Just pip install quickjs and you are ready to go.\nmodify based on the example on README.\nfrom quickjs import Function\n\njs = """"""\nfunction escramble_758(){\nvar a,b,c\na=\'+1 \'\nb=\'84-\'\na+=\'425-\'\nb+=\'7450\'\nc=\'9\'\ndocument.write(a+c+b)\nescramble_758()\n}\n""""""\n\nescramble_758 = Function(\'escramble_758\', js.replace(""document.write"", ""return ""))\n\nprint(escramble_758())\n\nhttps://github.com/PetterS/quickjs\n', '\nReally late to the party but you can use a successor of pyv8 which is regularly maintained by a reputable organization (Subjective) named CloudFlare. Here is the repository URL:\nhttps://github.com/cloudflare/stpyv8\n']",https://stackoverflow.com/questions/10136319/executing-javascript-from-python,screen-scraping
Scraping ajax pages using python,"
I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:
I only have one url, example.com you go from page to page by clicking submit, the url doesn't change since they're using ajax to display the content. I want to scrape the content of each page, how to do it? 
Lets say that i want to scrape only the numbers, is there anything other than scrapy that would do it? If not, would you give me a snippet on how to do it, just because their website is down so i can't reach the docs.
",47k,"
            18
        ","['\nFirst of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.\nSpeaking about handling ajax while web scraping. Basically, the idea is rather simple: \n\nopen browser developer tools, network tab\ngo to the target site\nclick submit button and see what XHR request is going to the server\nsimulate this XHR request in your spider\n\nAlso see:\n\nCan scrapy be used to scrape dynamic content from websites that are using AJAX?\nPagination using scrapy\n\nHope that helps.\n', '\nI found the answer very useful but I would like to make it more simple.\nresponse = requests.post(request_url, data=payload, headers=request_headers)\n\nrequest.post takes three parameters url, data and headers. Values for these three attributes can be found in the XHR request.\nCopy the whole request header and form data to load into the above variables and you are good to go\n']",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,screen-scraping
HTML Scraping in Php [duplicate],"






This question already has answers here:
                        
                    



How do you parse and process HTML/XML in PHP?

                                (31 answers)
                            

Closed 9 years ago.



I've been doing some HTML scraping in PHP using regular expressions.  This works, but the result is finicky and fragile.  Has anyone used any packages that provide a more robust solution?  A config driven solution would be ideal, but I'm not picky.
",46k,"
            39
        ","['\nI would recomend PHP Simple HTML DOM Parser after you have scraped the HTML from the page. It supports invalid HTML, and provides a very easy way to handle HTML elements. \n', ""\nIf the page you're scraping is valid X(HT)ML, then any of PHP's built-in XML parsers will do. \nI haven't had much success with PHP libraries for scraping. If you're adventurous though, you can try simplehtmldom. I'd recommend Hpricot for Ruby or Beautiful Soup for Python, which are both excellent parsers for HTML.\n"", ""\nI would also recommend 'Simple HTML DOM Parser.' It is a good option particularly if your familiar with jQuery or JavaScript selectors then you will find yourself at home.\nI have even blogged about it in the past.\n"", '\nI had some fun working with htmlSQL, which is not so much a high end solution, but really simple to work with.\n', ""\nUsing PHP for HTML scraping, I'd recommend cURL + regexp or cURL + some DOM parsers though I personally use cURL + regexp. If you have a profound taste of regexp, it's actually more accurate sometimes.\n"", ""\nI've had very good with results with the Simple Html DOM Parser mentioned above as well. And then there's the \xa0tidy Extension for PHP as well which works really well too.\n"", '\nI had to use curl on my host 1and1.\nhttp://www.quickscrape.com/ is what I came up with using the Simple DOM class!\n']",https://stackoverflow.com/questions/34120/html-scraping-in-php,screen-scraping
What's a good tool to screen-scrape with Javascript support? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



Is there a good test suite or tool set that can automate website navigation -- with Javascript support -- and collect the HTML from the pages?
Of course I can scrape straight HTML with BeautifulSoup.  But this does me no good for sites that require Javascript. :)
",26k,"
            28
        ","[""\nYou could use Selenium or Watir to drive a real browser.\nTher are also some JavaScript-based headless browsers:\n\nPhantomJS is a headless Webkit browser.\n\npjscrape is a scraping framework based on PhantomJS and jQuery.\nCasperJS is a navigation scripting & testing utility bsaed on PhantomJS, if you need to do a little more than point at URLs to be scraped.\n\n\nZombie for Node.js\n\nPersonally, I'm most familiar with Selenium, which has support for writing automation scripts in a good number of languagues and has more mature tooling, such as the excellent Selenium IDE extension for Firefox, which can be used to write and run testcases, and can export test scripts to many languages.\n"", '\nUsing HtmlUnit is also a possibility.\n\nHtmlUnit is a ""GUI-Less browser for\n  Java programs"". It models HTML\n  documents and provides an API that\n  allows you to invoke pages, fill out\n  forms, click links, etc... just like\n  you do in your ""normal"" browser.\nIt has fairly good JavaScript support\n  (which is constantly improving) and is\n  able to work even with quite complex\n  AJAX libraries, simulating either\n  Firefox or Internet Explorer depending\n  on the configuration you want to use.\nIt is typically used for testing\n  purposes or to retrieve information\n  from web sites.\n\n', '\nSelenium now wraps htmlunit so you don´t need start a browser anymore. The new WebDriver api is very easy to use too. The first example use htmlunit driver \n', ""\nIt would be very difficult to code a solution that would work with any arbitrary site out there.  Each navigation menu implementation can be quite unique.  I've worked a great deal with scrapers, and, provided you know the site you wish to target, here is how I'd approach it.\nUsually, if you analyze the particular javascript used in a nav menu, it is fairly easy to use regular expressions to pull out the entire set of variables that are used to build the navmenu.  I have never used Beautiful Soup, but from your description it sounds like it may only work on HTML elements and not be able to work inside the script tags.\nIf you're still having problems, or need to emulate some form POSTs or ajax, get Firefox and install the LiveHttpHeaders plugin.  This plugin will allow you to manually browse the site and capture the urls being navigated along with any cookies that are being passed during your manual browsing.  That is what you need your scraperbot to send in a request to get a valid response from the target webserver(s).   This will also capture any ajax calls being made, and in many cases the same ajax calls must be implementated in your scraper to get your desired responses.\n"", '\nMozenda is a great tool to use as well.\n', ""\nKeep in mind that and javascript fanciness is messing with the brower's internal DOM model of the page, and does nothing to the raw HTML.\n"", ""\nI've been using Selenium for this and it find that it works great.\nSelenium runs in Browser and will work with Firefox, Webkit and IE.\nhttp://selenium.openqa.org/\n"", '\n@insin Watir is not IE only.\nhttps://stackoverflow.com/questions/81566#83387\n']",https://stackoverflow.com/questions/125177/whats-a-good-tool-to-screen-scrape-with-javascript-support,screen-scraping
Download image file from the HTML page source,"
I am writing a scraper that downloads all the image files from a HTML page and saves them to a specific folder. All the images are part of the HTML page.
",101k,"
            48
        ","['\nHere is some code to download all the images from the supplied URL, and save them in the specified output folder. You can modify it to your own needs.\n""""""\ndumpimages.py\n    Downloads all the images on the supplied URL, and saves them to the\n    specified output file (""/test/"" by default)\n\nUsage:\n    python dumpimages.py http://example.com/ [output]\n""""""\nfrom bs4 import BeautifulSoup as bs\nfrom urllib.request import (\n    urlopen, urlparse, urlunparse, urlretrieve)\nimport os\nimport sys\n\ndef main(url, out_folder=""/test/""):\n    """"""Downloads all the images at \'url\' to /test/""""""\n    soup = bs(urlopen(url))\n    parsed = list(urlparse(url))\n\n    for image in soup.findAll(""img""):\n        print(""Image: %(src)s"" % image)\n        filename = image[""src""].split(""/"")[-1]\n        parsed[2] = image[""src""]\n        outpath = os.path.join(out_folder, filename)\n        if image[""src""].lower().startswith(""http""):\n            urlretrieve(image[""src""], outpath)\n        else:\n            urlretrieve(urlunparse(parsed), outpath)\n\ndef _usage():\n    print(""usage: python dumpimages.py http://example.com [outpath]"")\n\nif __name__ == ""__main__"":\n    url = sys.argv[-1]\n    out_folder = ""/test/""\n    if not url.lower().startswith(""http""):\n        out_folder = sys.argv[-1]\n        url = sys.argv[-2]\n        if not url.lower().startswith(""http""):\n            _usage()\n            sys.exit(-1)\n    main(url, out_folder)\n\nEdit: You can specify the output folder now.\n', '\nRyan\'s solution is good, but fails if the image source URLs are absolute URLs or anything that doesn\'t give a good result when simply concatenated to the main page URL.  urljoin recognizes absolute vs. relative URLs, so replace the loop in the middle with:\nfor image in soup.findAll(""img""):\n    print ""Image: %(src)s"" % image\n    image_url = urlparse.urljoin(url, image[\'src\'])\n    filename = image[""src""].split(""/"")[-1]\n    outpath = os.path.join(out_folder, filename)\n    urlretrieve(image_url, outpath)\n\n', '\nYou have to download the page and parse html document, find your image with regex and download it.. You can use urllib2 for downloading and Beautiful Soup for parsing html file.\n', '\nAnd this is function for download one image:\ndef download_photo(self, img_url, filename):\n    file_path = ""%s%s"" % (DOWNLOADED_IMAGE_PATH, filename)\n    downloaded_image = file(file_path, ""wb"")\n\n    image_on_web = urllib.urlopen(img_url)\n    while True:\n        buf = image_on_web.read(65536)\n        if len(buf) == 0:\n            break\n        downloaded_image.write(buf)\n    downloaded_image.close()\n    image_on_web.close()\n\n    return file_path\n\n', '\nUse htmllib to extract all img tags (override do_img), then use urllib2 to download all the images.\n', ""\nIf the request need an authorization refer to this one:\nr_img = requests.get(img_url, auth=(username, password)) \nf = open('000000.jpg','wb') \nf.write(r_img.content) \nf.close()\n\n"", '\nBased on code here\nRemoving some lines of code, you\'ll get only the images img tags.\nUses Python 3+ Requests, BeautifulSoup and other standard libraries.\nimport os, sys\nimport requests\nfrom urllib import parse\nfrom bs4 import BeautifulSoup\nimport re\n\ndef savePageImages(url, imagespath=\'images\'):\n    def soupfindnSave(pagefolder, tag2find=\'img\', inner=\'src\'):\n        if not os.path.exists(pagefolder): # create only once\n            os.mkdir(pagefolder)\n        for res in soup.findAll(tag2find):  \n            if res.has_attr(inner): # check inner tag (file object) MUST exists\n                try:\n                    filename, ext = os.path.splitext(os.path.basename(res[inner])) # get name and extension\n                    filename = re.sub(\'\\W+\', \'\', filename) + ext # clean special chars from name\n                    fileurl = parse.urljoin(url, res.get(inner))\n                    filepath = os.path.join(pagefolder, filename)\n                    if not os.path.isfile(filepath): # was not downloaded\n                        with open(filepath, \'wb\') as file:\n                            filebin = session.get(fileurl)\n                            file.write(filebin.content)\n                except Exception as exc:\n                    print(exc, file=sys.stderr)   \n    session = requests.Session()\n    #... whatever other requests config you need here\n    response = session.get(url)\n    soup = BeautifulSoup(response.text, ""html.parser"")\n    soupfindnSave(imagespath, \'img\', \'src\')\n\nUse like this bellow to save the google.com page images in a folder google_images:\nsavePageImages(\'https://www.google.com\', \'google_images\')\n\n', '\nimport urllib.request as req\n\nwith req.urlopen(image_link) as d, open(image_location, ""wb"") as image_object:\n    data = d.read()\n    image_object.write(data)\n\n']",https://stackoverflow.com/questions/257409/download-image-file-from-the-html-page-source,screen-scraping
How to scroll down with Phantomjs to load dynamic content,"
I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-

Setting viewportSize to a large height right after var page = require('webpage').create();


page.viewportSize = {             width: 1600,            height: 10000,
          };


Using page.scrollPosition = { top: 10000, left: 0 } inside page.open but have no effect like-


page.open('http://example.com/?q=houston', function(status) {
   if (status == ""success"") {
      page.scrollPosition = { top: 10000, left: 0 };  
   }
});



Also tried putting it inside page.evaluate function but that gives 


Reference error: Can't find variable page


Tried using jQuery and JS code inside page.evaluate and page.open but to no avail-


$(""html, body"").animate({ scrollTop: $(document).height() }, 10,
  function() {
          //console.log('check for execution');
      });

as it is and also inside document.ready. Similarly for JS code-
window.scrollBy(0,10000)

as it is and also inside window.onload
I am really struck on it for 2 days now and not able to find a way. Any help or hint would be appreciated.
Update
I have found a helpful piece of code at https://groups.google.com/forum/?fromgroups=#!topic/phantomjs/8LrWRW8ZrA0
var hitRockBottom = false; while (!hitRockBottom) {
    // Scroll the page (not sure if this is the best way to do so...)
    page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };

    // Check if we've hit the bottom
    hitRockBottom = page.evaluate(function() {
        return document.querySelector("".has-more-items"") === null;
    }); }

Where .has-more-items is the element class I want to access which is available at the bottom of the page initially and as we scroll down, it moves further down until all data is loaded and then becomes unavailable.
However, when I tested it is clear that it is running into infinite loops without scrolling down (I render pictures to check). I have tried to replace page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 }; with codes from below as well (one at a time)
window.document.body.scrollTop = '1000';
location.href = "".has-more-items"";
page.scrollPosition = { top: page.scrollPosition + 1000, left: 0 };
document.location.href="".has-more-items"";

But nothing seems to work.
",36k,"
            47
        ","['\nFound a way to do it and tried to adapt to your situation. I didn\'t test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.\npage.open(\'http://example.com/?q=houston\', function () {\n\n  // Check for the bottom div and scroll down from time to time\n  window.setInterval(function() {\n      // Check if there is a div with class="".has-more-items"" \n      // (not sure if there\'s a better way of doing this)\n      var count = page.content.match(/class="".has-more-items""/g);\n\n      if(count === null) { // Didn\'t find\n        page.evaluate(function() {\n          // Scroll to the bottom of page\n          window.document.body.scrollTop = document.body.scrollHeight;\n        });\n      }\n      else { // Found\n        // Do what you want\n        ...\n        phantom.exit();\n      }\n  }, 500); // Number of milliseconds to wait between scrolls\n\n});\n\n', '\nI know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.\nIt is not written for PhantomJS by default, so that will have to be modified. However, for a beginner or someone who doesn\'t have root access, an Iframe with injected javascript (run Google Chrome with --disable-javascript parameter) is a good alternative method for scraping a smaller set of ajax pages. The main benefit is that it\'s easily debuggable, because you have a visual overview of what\'s going on with your scraper.\nfunction ScrollForAjax () {\n\n    scrollintervals = 50;\n    scrollmaxtime = 1000;\n\n    if(typeof(scrolltime)==""undefined""){\n        scrolltime = 0;\n    }\n\n    scrolldocheight1 = $(iframeselector).contents().find(""body"").height();\n\n    $(""body"").scrollTop(scrolldocheight1);\n    setTimeout(function(){\n\n        scrolldocheight2 = $(""body"").height();\n\n        if(scrolltime===scrollmaxtime || scrolltime>scrollmaxtime){\n            scrolltime = 0;\n            $(""body"").scrollTop(0);\n            ScrapeCurrentPage(iframeselector);\n        }\n\n        else if(scrolldocheight2>scrolldocheight1){\n            scrolltime = 0;\n            ScrollForAjax (iframeselector);\n        }\n\n        else if(scrolldocheight1>=scrolldocheight2){\n            ScrollForAjax (iframeselector);\n        }\n\n    },scrollintervals);\n\n    scrolltime += scrollintervals;\n}\n\nscrollmaxtime is a timeout variable. Hope this is useful to someone :)\n', '\nThe ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.\nThe following works for me in the Firefox/Chrome console; but, doesn\'t work in CasperJS (within casper.evaluate function).\n$(document).scrollTop($(document).height());\n\nWhat did work for me in CasperJS was:\ncasper.scrollToBottom();\ncasper.wait(1000, function waitCb() {\n  casper.capture(""loadedContent.png"");\n});\n\nWhich, also worked when moving casper.capture into Casper\'s then function.\nHowever, the above solution won\'t work on some sites like Twitter; jQuery seems to break the casper.scrollToBottom() function, and I had to remove the clientScripts reference to jQuery when working within Twitter.\nvar casper = require(\'casper\').create({\n    clientScripts: [\n       // \'jquery.js\'\n    ]\n});\n\nSome websites (e.g. BoingBoing.net) seem to work fine with jQuery and CasperJS scrollToBottom(). Not sure why some sites work and others don\'t.\n', ""\nThe code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.\npage.open(pageUrl).then(function (status) {\n    var count = 0;\n    // Scrolls to the bottom of page\n    function scroll2btm() {\n        if (count < 500) {\n            page.evaluate(function(limit) {\n                window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight);\n                return document.getElementsByClassName('pinWrapper').length; // use desired contents (eg. pin) selector for count presence number\n            }).then(function(c) {\n                count = c;\n                console.log(count); // print no of content found to check\n            });\n            setTimeout(scroll2btm,3000);\n        } else {\n            // required number of item found\n        }\n    }\n    scroll2btm();\n});\n\n""]",https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,screen-scraping
Simple Screen Scraping using jQuery,"
I have been playing with the idea of using a simple screen-scraper using jQuery and I am wondering if the following is possible.
I have simple HTML page and am making an attempt (if this is possible) to grab the contents of all of the list items from another page, like so:
Main Page:
<!-- jQuery -->
<script type='text/javascript'>
$(document).ready(function(){
$.getJSON(""[URL to other page]"",
  function(data){

    //Iterate through the <li> inside of the URL's data
    $.each(data.items, function(item){
      $(""<li/>"").value().appendTo(""#data"");
    });

  });
});
</script>

<!-- HTML -->
<html>
    <body>
       <div id='data'></div>
    </body>
</html>

Other Page:
//Html
<body>
    <p><b>Items to Scrape</b></p>   
    <ul>
        <li>I want to scrape what is here</li>
        <li>and what is here</li>
        <li>and here as well</li>
        <li>and append it in the main page</li>
    </ul>
</body>

So, is it possible using jQuery to pull all of the list item contents from an external page and append them inside of a div?
",101k,"
            46
        ","['\nUse $.ajax to load the other page into a variable, then create a temporary element and use .html() to set the contents to the value returned. Loop through the  element\'s children of nodeType 1 and keep their first children\'s nodeValues. If the external page is not on your web server you will need to proxy the file with your own web server.\nSomething like this:\n$.ajax({\n     url: ""/thePageToScrape.html"",\n     dataType: \'text\',\n     success: function(data) {\n          var elements = $(""<div>"").html(data)[0].getElementsByTagName(""ul"")[0].getElementsByTagName(""li"");\n          for(var i = 0; i < elements.length; i++) {\n               var theText = elements[i].firstChild.nodeValue;\n               // Do something here\n          }\n     }\n});\n\n', '\nSimple scraping with jQuery...\n// Get HTML from page\n$.get( \'http://example.com/\', function( html ) {\n\n    // Loop through elements you want to scrape content from\n    $(html).find(""ul"").find(""li"").each( function(){\n\n        var text = $(this).text();\n        // Do something with content\n\n    } )\n\n} );\n\n', '\n$.get(""/path/to/other/page"",function(data){\n  $(\'#data\').append($(\'li\',data));\n}\n\n', ""\nIf this is for the same domain then no problem - the jQuery solution is good.\nBut otherwise you can't access content from an arbitrary website because this is considered a security risk. See same origin policy. \nThere are of course server side workarounds such as a web proxy or CORS headers.\nOf if you're lucky they will support jsonp.\nBut if you want a client side solution to work with an arbitrary website and web browser then you are out of luck. There is a proposal to relax this policy, but this won't effect current web browsers.\n"", ""\nYou may want to consider pjscrape:\nhttp://nrabinowitz.github.io/pjscrape/\nIt allows you to do this from the command-line, using javascript and jQuery. It does this by using PhantomJS, which is a headless webkit browser (it has no window, and it exists only for your script's usage, so you can load complex websites that use AJAX and it will work just as if it were a real browser).\nThe examples are self-explanatory and I believe this works on all platforms (including Windows).\n"", '\nUse YQL or Yahoo pipes to make the cross domain request for the raw page html content. The yahoo pipe or YQL query will spit this back as a JSON that can be processed by jquery to extract and display the required data.\nOn the downside: YQL and Yahoo pipes OBEY the robots.txt file for the target domain\nand if the page is to long the Yahoo Pipes regex commands will not run.\n', '\nI am sure you will hit the CORS issue with requests in many cases.\nFrom here try to resolve CORS issue.\nvar name = ""kk"";\nvar url = ""http://anyorigin.com/go?url="" + encodeURIComponent(""https://www.yoursite.xyz/"") + name + ""&callback=?"";\n$.get(url, function(response) {\n  console.log(response);\n});\n\n']",https://stackoverflow.com/questions/5667880/simple-screen-scraping-using-jquery,screen-scraping
web scraping dynamic content with python,"
I'd like to use Python to scrape the contents of the ""Were you looking for these authors:"" box on web pages like this one: http://academic.research.microsoft.com/Search?query=lander
Unfortunately the contents of the box get loaded dynamically by JavaScript. Usually in this situation I can read the Javascript to figure out what's going on, or I can use an browser extension like Firebug to figure out where the dynamic content is coming from. No such luck this time...the Javascript is pretty convoluted and Firebug doesn't give many clues about how to get at the content.
Are there any tricks that will make this task easy? 
",21k,"
            6
        ","['\nInstead of trying to reverse engineer it, you can use ghost.py to directly interact with JavaScript on the page.\nIf you run the following query in a chrome console, you\'ll see it returns everything you want.\ndocument.getElementsByClassName(\'inline-text-org\');\n\nReturns\n[<div class=\u200b""inline-text-org"" title=\u200b""University of Manchester"">\u200bUniversity of Manchester\u200b</div>, \n <div class=\u200b""inline-text-org"" title=\u200b""University of California Irvine"">\u200bUniversity of California ...\u200b</div>\u200b\n  etc...\n\nYou can run JavaScript through python in a real life DOM using ghost.py.\nThis is really cool:\nfrom ghost import Ghost\nghost = Ghost()\npage, resources = ghost.open(\'http://academic.research.microsoft.com/Search?query=lander\')\nresult, resources = ghost.evaluate(\n    ""document.getElementsByClassName(\'inline-text-org\');"")\n\n', ""\nA very similar question was asked earlier here.\nQuoted is selenium, originally a testing environment for web-apps.\nI usually use Chrome's Developer Mode, which IMHO already gives even more details than Firefox.\n"", ""\nFor scraping dynamic content, you need not a simple scraper but a full-fledged headless browser.\ndhamaniasad/HeadlessBrowsers: A list of (almost) all headless web browsers in existence is the fullest list of these that I've seen; it lists which languages each has bindings for.\n(Note that more than a few of the listed projects are abandoned!)\n""]",https://stackoverflow.com/questions/17608572/web-scraping-dynamic-content-with-python,screen-scraping
How I can get web page's content and save it into the string variable,"
How I can get the content of the web page using ASP.NET?  I need to write a program to get the HTML of a webpage and store it into a string variable.
",182k,"
            78
        ","['\nYou can use the WebClient\nUsing System.Net;\n\nusing(WebClient client = new WebClient()) {\n    string downloadString = client.DownloadString(""http://www.gooogle.com"");\n}\n\n', '\nI\'ve run into issues with Webclient.Downloadstring before. If you do, you can try this:\nWebRequest request = WebRequest.Create(""http://www.google.com"");\nWebResponse response = request.GetResponse();\nStream data = response.GetResponseStream();\nstring html = String.Empty;\nusing (StreamReader sr = new StreamReader(data))\n{\n    html = sr.ReadToEnd();\n}\n\n', '\nI recommend not using WebClient.DownloadString. This is because (at least in .NET 3.5) DownloadString is not smart enough to use/remove the BOM, should it be present. This can result in the BOM (Ã¯Â»Â¿) incorrectly appearing as part of the string when UTF-8 data is returned (at least without a charset) - ick!\nInstead, this slight variation will work correctly with BOMs:\nstring ReadTextFromUrl(string url) {\n    // WebClient is still convenient\n    // Assume UTF8, but detect BOM - could also honor response charset I suppose\n    using (var client = new WebClient())\n    using (var stream = client.OpenRead(url))\n    using (var textReader = new StreamReader(stream, Encoding.UTF8, true)) {\n        return textReader.ReadToEnd();\n    }\n}\n\n', '\nWebclient client = new Webclient();\nstring content = client.DownloadString(url);\n\nPass the URL of page who you want to get. You can parse the result using htmlagilitypack.\n']",https://stackoverflow.com/questions/4510212/how-i-can-get-web-pages-content-and-save-it-into-the-string-variable,screen-scraping
"Beautiful Soup cannot find a CSS class if the object has other classes, too","
if a page has <div class=""class1""> and <p class=""class1"">, then soup.findAll(True, 'class1') will find them both.
If it has <p class=""class1 class2"">, though, it will not be found.  How do I find all objects with a certain class, regardless of whether they have other classes, too?
",16k,"
            40
        ","[""\nUnfortunately, BeautifulSoup treats this as a class with a space in it 'class1 class2' rather than two classes ['class1','class2'].  A workaround is to use a regular expression to search for the class instead of a string.\nThis works: \nsoup.findAll(True, {'class': re.compile(r'\\bclass1\\b')})\n\n"", '\nJust in case anybody comes across this question. BeautifulSoup now supports this:\nPython 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)]\nType ""copyright"", ""credits"" or ""license"" for more information.\n\nIn [1]: import bs4\n\nIn [2]: soup = bs4.BeautifulSoup(\'<div class=""foo bar""></div>\')\n\nIn [3]: soup(attrs={\'class\': \'bar\'})\nOut[3]: [<div class=""foo bar""></div>]\n\nAlso, you don\'t have to type findAll anymore.\n', '\nYou should use lxml. It works with multiple class values separated by spaces (\'class1 class2\').\nDespite its name, lxml is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.\nIan Bicking agrees and prefers lxml over BeautifulSoup.\nThere\'s no reason to use BeautifulSoup anymore, unless you\'re on Google App Engine or something where anything not purely Python isn\'t allowed.\nYou can even use CSS selectors with lxml, so it\'s far easier to use than BeautifulSoup. Try playing around with it in an interactive Python console.\n', '\nIt’s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, “class”, is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument class_:\nLike:\nsoup.find_all(""a"", class_=""class1"")\n\n']",https://stackoverflow.com/questions/1242755/beautiful-soup-cannot-find-a-css-class-if-the-object-has-other-classes-too,screen-scraping
file_get_contents() give me 403 Forbidden,"
I have a partner that has created some content for me to scrape.
I can access the page with my browser, but when trying to user file_get_contents, I get a 403 forbidden.
I've tried using stream_context_create, but that's not helping - it might be because I don't know what should go in there.
1) Is there any way for me to scrape the data?
2) If no, and if partner is not allowed to configure server to allow me access, what can I do then?
The code I've tried using:
$opts = array(
  'http'=>array(
    'user_agent' => 'My company name',
    'method'=>""GET"",
    'header'=> implode(""\r\n"", array(
      'Content-type: text/plain;'
    ))
  )
);

$context = stream_context_create($opts);

//Get header content
$_header = file_get_contents($partner_url,false, $context);

",35k,"
            26
        ","['\nThis is not a problem in your script, its a feature in you partners web server security.\nIt\'s hard to say exactly whats blocking you, most likely its some sort of block against scraping. If your partner has access to his web servers setup it might help pinpoint.\nWhat you could do is to ""fake a web browser"" by setting the user-agent headers so that it imitates a standard web browser.\nI would recommend cURL to do this, and it will be easy to find good documentation for doing this.\n    // create curl resource\n    $ch = curl_init();\n\n    // set url\n    curl_setopt($ch, CURLOPT_URL, ""example.com"");\n\n    //return the transfer as a string\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n    curl_setopt($ch,CURLOPT_USERAGENT,\'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13\');\n\n    // $output contains the output string\n    $output = curl_exec($ch);\n\n    // close curl resource to free up system resources\n    curl_close($ch); \n\n', ""\n//set User Agent first\nini_set('user_agent','Mozilla/4.0 (compatible; MSIE 6.0)'); \n\n"", ""\nAlso if for some reason you're requesting a http resource but that resource lives on your server you can save yourself some trouble if you just include the file as an absolute path.\nLike: /home/sally/statusReport/myhtmlfile.html \ninstead of \nhttps://example.org/myhtmlfile.html\n"", ""\nI have two things in my mind, If you're opening a URI with special characters, such as spaces, you need to encode the URI with urlencode() and A URL can be used as a filename with this function if the fopen wrappers have been enabled.\n""]",https://stackoverflow.com/questions/11680709/file-get-contents-give-me-403-forbidden,screen-scraping
Scrape a dynamic website,"
What is the best method to scrape a dynamic website where most of the content is generated by what appears to be ajax requests?  I have previous experience with a Mechanize, BeautifulSoup, and python combo, but I am up for something new.
--Edit--
For more detail: I'm trying to scrape the CNN primary database.  There is a wealth of information there, but there doesn't appear to be an api.
",8k,"
            12
        ","['\nThe best solution that I found was to use Firebug to monitor XmlHttpRequests, and then to use a script to resend them.\n', ""\nThis is a difficult problem because you either have to reverse engineer the JavaScript on a per-site basis, or implement a JavaScript engine and run the scripts (which has its own difficulties and pitfalls).\nIt's a heavy weight solution, but I've seen people doing this with GreaseMonkey scripts - allow Firefox to render everything and run the JavaScript, and then scrape the elements. You can even initiate user actions on the page if needed.\n"", '\nSelenium IDE, a tool for testing, is something I\'ve used for a lot of screen-scraping. There are a few things it doesn\'t handle well (Javascript window.alert() and popup windows in general), but it does its work on a page by actually triggering the click events and typing into the text boxes. Because the IDE portion runs in Firefox, you don\'t have to do all of the management of sessions, etc. as Firefox takes care of it. The IDE records and plays tests back.\nIt also exports C#, PHP, Java, etc. code to build compiled tests/scrapers that are executed on the Selenium server. I\'ve done that for more than a few of my Selenium scripts, which makes things like storing the scraped data in a database much easier.\nScripts are fairly simple to write and alter, being made up of things like (""clickAndWait"",""submitButton""). Worth a look given what you\'re describing.\n', '\nAdam Davis\'s advice is solid.\nI would additionally suggest that you try to ""reverse-engineer"" what the JavaScript is doing, and instead of trying to scrape the page, you issue the HTTP requests that the JavaScript is issuing and interpret the results yourself (most likely in JSON format, nice and easy to parse).  This strategy could be anything from trivial to a total nightmare, depending on the complexity of the JavaScript.\nThe best possibility, of course, would be to convince the website\'s maintainers to implement a developer-friendly API.  All the cool kids are doing it these days 8-)  Of course, they might not  want their data scraped in an automated fashion... in which case you can expect a cat-and-mouse game of making their page increasingly difficult to scrape :-(\n', ""\nThere is a bit of a learning curve, but tools like Pamie (Python) or Watir (Ruby) will let you latch into the IE web browser and get at the elements. This turns out to be easier than Mechanize and other HTTP level tools since you don't have to emulate the browser, you just ask the browser for the html elements. And it's going to be way easier than reverse engineering the Javascript/Ajax calls. If needed you can also use tools like beatiful soup in conjunction with Pamie.\n"", '\nProbably the easiest way is to use IE webbrowser control in C# (or any other language). You have access to all the stuff inside browser out of the box + you dont need to care about cookies, SSL and so on.\n', '\ni found the IE Webbrowser control have all kinds of quirks and workarounds that would justify some high quality software to take care of all those inconsistencies, layered around the shvwdoc.dll api and mshtml and provide a framework. \n', ""\nThis seems like it's a pretty common problem.  I wonder why someone hasn't anyone developed a programmatic browser?  I'm envisioning a Firefox you can call from the command line with a URL as an argument and it will load the page, run all of the initial page load JS events and save the resulting file.\nI mean Firefox, and other browsers already do this, why can't we simply strip off the UI stuff?  \n""]",https://stackoverflow.com/questions/206855/scrape-a-dynamic-website,screen-scraping
Protection from screen scraping [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



Following on from my question on the Legalities of screen scraping, even if it's illegal people will still try, so:
What technical mechanisms can be employed to prevent or at least disincentivise screen scraping?
Oh and just for grins and to make life difficult, it may well be nice to retain access for search engines. I may well be playing devil's advocate here but there is a serious underlying point.
",24k,"
            31
        ","['\nYou can’t prevent it.\n', ""\nI've written a blog post about this here:  http://blog.screen-scraper.com/2009/08/17/further-thoughts-on-hindering-screen-scraping/\nTo paraphrase:\nIf you post information on the internet someone can get it, it's just a matter of how many resources they want to invest.  Some means to make the required resources higher are:\nTuring tests\nThe most common implementation of the Turning Test is the old CAPTCHA that tries to ensure a human reads the text in an image, and feeds it into a form.\nWe have found a large number of sites that implement a very weak CAPTCHA that takes only a few minutes to get around. On the other hand, there are some very good implementations of Turing Tests that we would opt not to deal with given the choice, but a sophisticated OCR can sometimes overcome those, or many bulletin board spammers have some clever tricks to get past these.\nData as images\nSometimes you know which parts of your data are valuable. In that case it becomes reasonable to replace such text with an image. As with the Turing Test, there is OCR software that can read it, and there’s no reason we can’t save the image and have someone read it later.\nOften times, however, listing data as an image without a text alternate is in violation of the Americans with Disabilities Act (ADA), and can be overcome with a couple of phone calls to a company’s legal department.\nCode obfuscation\nUsing something like a JavaScript function to show data on the page though it’s not anywhere in the HTML source is a good trick. Other examples include putting prolific, extraneous comments through the page or having an interactive page that orders things in an unpredictable way (and the example I think of used CSS to make the display the same no matter the arrangement of the code.)\nCSS Sprites\nRecently we’ve encountered some instances where a page has one images containing numbers and letters, and used CSS to display only the characters they desired.  This is in effect a combination of the previous 2 methods.  First we have to get that master-image and read what characters are there, then we’d need to read the CSS in the site and determine to what character each tag was pointing.\nWhile this is very clever, I suspect this too would run afoul the ADA, though I’ve not tested that yet.\nLimit search results\nMost of the data we want to get at is behind some sort of form. Some are easy, and submitting a blank form will yield all of the results. Some need an asterisk or percent put in the form. The hardest ones are those that will give you only so many results per query. Sometimes we just make a loop that will submit the letters of the alphabet to the form, but if that’s too general, we must make a loop to submit all combination of 2 or 3 letters–that’s 17,576 page requests.\nIP Filtering\nOn occasion, a diligent webmaster will notice a large number of page requests coming from a particular IP address, and block requests from that domain.  There are a number of methods to pass requests through alternate domains, however, so this method isn’t generally very effective.\nSite Tinkering\nScraping always keys off of certain things in the HTML.  Some sites have the resources to constantly tweak their HTML so that any scrapes are constantly out of date.  Therefore it becomes cost ineffective to continually update the scrape for the constantly changing conditions.\n"", '\nSo, one approach would be to obfuscate the code (rot13, or something), and then have some javascript in the page that do something like document.write(unobfuscate(obfuscated_page)). But this totally blows away search engines (probably!).\nOf course this doesn’t actually stop someone who wants to steal your data either, but it does make it harder.\nOnce the client has the data it is pretty much game over, so you need to look at something on the server side.\nGiven that search engines are basically screen scrapers things are difficult. You need to look at what the difference between the good screen scrapers and the bad screen scrapers are. And of course, you have just the normal human users as well. So this comes down to a problem of how can you on the server effectively classify as request as coming from a human, a good screen scraper, or a bad screen scraper.\nSo, the place to start would be looking at your log-files and seeing if there is some pattern that allows you to effectively classify requests, and then on determining the pattern see if there is some way that a bad screen scraper, upon knowing this classification, could cloak itself to appear like a human or good screen scraper.\nSome ideas:\n\nYou may be able to determine the good screen scrapers by IP address(es)..\nYou could potentially determine scraper vs. human by number of concurrent connections, total number of connections per time-period, access pattern, etc.\n\nObviously these aren’t ideal or fool-proof. Another tactic is to determine what measures can you take that are unobtrusive to humans, but (may be) annoying for scrapers. An example might be slowing down the number of requests. (Depends on the time criticality of the request. If they are scraping in real-time, this would effect their end users).\nThe other aspect is to look at serving these users better. Clearly they are scraping because they want the data. If you provide them an easy way in which to directly obtain the data in a useful format then that will be easier for them to do instead of screen scraping. If there is an easy way then access to the data can be regulated. E.g: give requesters a unique key, and then limit the number of requests per key to avoid overload on the server, or charge per 1000 requests, etc.\nOf course there are still people who will want to rip you off, and then there are probably other ways to disincentivise, bu they probably start being non-technical, and require legal avenues to be persued.\n', ""\nIt's pretty hard to prevent screen scraping but if you really, really wanted to you could\nchange your HTML frequently or change the HTML tag names frequently. Most screen scrapers work by using string comparisons with tag names, or regular expressions searching for particular strings etc. If you are changing the underlying HTML it will make them need to change their software.\n"", '\nIt would be very difficult to prevent.  The problem is that Web pages are meant to be parsed by a program (your browser), so they are exceptionally easy to scrape.  The best you can do is be vigilant, and if you find that your site is being scraped, block the IP of the offending program.\n', ""\nDon't prevent it, detect it and retaliate those who try.\nFor example, leave your site open to download but disseminate some links that no sane user would follow. If someone follows that link, is clicking too fast for a human or other suspicious behaviour, react promptly to stop the user from trying. If there is a login system, block the user and contact him regarding unacceptable behaviour. That should make sure they don't try again. If there is no login system, instead of actual pages, return a big warning with fake links to the same warning.\nThis really applies for things like Safari Bookshelf where a user copy-pasting a piece of code or a chapter to mail a colleague is fine while a full download of book is not acceptable. I'm quite sure that they detect when some tries to download their books, block the account and show the culprit that he might get in REAL trouble should he try that again.\nTo make a non-IT analogy, if airport security only made it hard to bring weapons on board of planes, terrorists would try many ways to sneak one past security. But the fact that just trying will get you in deep trouble make it so that nobody is going to try and find the ways to sneak one. The risk of getting caught and punished is too high. Just do the same. If possible.\n"", ""\nSearch engines ARE screen scrapers by definition.  So most things you do to make it harder to screen scrape will also make it harder to index your content.\nWell behaved robots will honour your robots.txt file.\nYou could also block the IP of known offenders or add obfuscating HTML tags into your content when it's not sent to a known good robot.  It's a losing battle though.  I recommend the litigation route for known offenders.\nYou could also hide identifying data in the content to make it easier to track down offenders. Encyclopaedias have been known to to add Fictitious entries to help detect and prosecute copyright infringers.\n"", ""\nPrevent? -- impossible, but you can make it harder.\nDisincentivise? -- possible, but you won't like the answer: provide bulk data exports for interested parties. \nOn the long run, all your competitors will have the same data if you publish it, so you need other means of diversifying your website (e.g. update it more frequently, make it faster or easier to use). Nowdays even Google is using scraped information like user reviews, what do you think you can do about it? Sue them and get booted from their index?\n"", ""\nThe best return on investment is probably to add random newlines and multiple spaces, since most screen scrapers work from the HTML as text rather than as a XML (since most pages don't parse as valid XML).\nThe browser ignores whitespace, so your user's don't notice that \n  Price : 1\n  Price :    2\n  Price\\n:\\n3\n\nare different.  (this comes from my experience scraping government sites with AWK).\nNext step is adding  tags around random elements to mess up the DOM.\n"", '\nOne way is to create an function that takes text and position and then Serverside generate x, y pos for every character in the text, generate divs in random order containing the characters. Generate a javascript that then posision every div on right place on screen. Looks good on screen but in code behind there is no real order to fetch the text if you dont go throuh the trouble to scrape via your javascript (that can be changed dynamically every request)\nToo much work and have possibly many quirks, it depends on how much text and how complicate UI you have on the site and other things.\n', '\nVery few I think given the intention of any site is to publish (i.e. to make public) information. \n\nYou can hide your data behind logins of course, but that\'s a very situational solution. \nI\'ve seen apps which would only serve up content where the request headers indicated a web browser (rather than say anonymous or ""jakarta"") but that\'s easy to spoof and you\'ll lose some genuine humans.\nThen there\'s the possibility that you accept some scrapage but make life insurmountably hard for them by not serving content if requests are coming from the same IP at too high a rate. This suffers from not being full coverage but more importantly there is the ""AOL problem"" that an IP can cover many many unique human users.\n\nBoth of the last two techniques also depend heavily on having traffic intercepting technology which is an inevitable performance and/or financial outlay.\n', ""\nGiven that most sites want a good search engine ranking, and search engines are scraper bots, there's not much you can do that won't harm your SEO. \nYou could make an entirely ajax loaded site or flash based site, which would make it harder for bots, or hide everything behind a login, which would make it harder still, but either of these approaches is going to hurt your search rankings and possibly annoy your users, and if someone really wants it, they'll find a way.  \nThe only guaranteed way of having content that can't be scraped is to not publish it on the web. The nature of the web is such that when you put it out there, it's out there.\n"", '\nIf its not much information you want to protect you can convert it to a picture on the fly. Then they must use OCR wich makes it easier to scrape another site instead of yours..\n', ""\nYou could check the user agent of clients coming to your site. Some third party screen scraping programs have their own user agent so you could block that. Good screen scrapers however spoof their user agent so you won't be able to detect it. Be careful if you do try to block anyone because you don't want to block a legitimate user :)\nThe best you can hope for is to block people using screen scrapers that aren't smart enough to change their user agent.\n"", '\nI tried to ""screen scrape"" some PDF files once, only to find that they\'d actually put the characters in the PDF in semi-random order.  I guess the PDF format allows you to specify a location for each block of text, and they\'d used very small blocks (smaller than a word).  I suspect that the PDFs in question weren\'t trying to prevent screen scraping so much as they were doing something weird with their render engine.\nI wonder if you could do something like that.\n', '\nYou could put everything in flash, but in most cases that would annoy many legitimate users, myself included. It can work for some information such as stock prices or graphs.\n', '\nI suspect there is no good way to do this.\nI suppose you could run all your content through a mechanism to convert text to images rendered using a CAPTCHA-style font and layout, but that would break SEO and annoy your users.\n', '\nWell, before you push the content from the server to the client, remove all the \\r\\n, \\n, \\t and replace everything with nothing but a single space. Now you have 1 long line in your html page. Google does this. This will make it hard for others to read your html or JavaScript.\nThen you can create empty tags and randomly insert them here and there. The will have no effect.\nThen you can log all the IPs and how often they hit your site. If you see one that comes in on time everytime, you mark it as robot and block it.\nMake sure you leave the search engines alone if you want them to come in.\n\nHope this helps\n', ""\nWhat about using the iText library to create PDFs out of your database information? As with Flash, it won't make scraping impossible, but might make it a little more difficult.\nNels\n"", ""\nOld question, but- adding interactivity makes screen scraping much more difficult. If the data isn't in the original response- say, you made an AJAX request to populate a div after page load- most scrapers won't see it.\nFor example- I use the mechanize library to do my scraping. Mechanize doesn't execute Javascript- it isn't a modern browser- it just parses HTML, let's me follow links and extract text, etc. Whenever I run into a page that makes heavy use of Javascript, I choke- without a fully scripted browser (that supports the full gamut of Javascript) I'm stuck.\nThis is the same issue that makes automated testing of highly interactive web applications so difficult.\n"", '\nI never thought that preventing print screen would be possible... well what do you know, checkout the new tech - sivizion.com. With their video buffer technology there is no way to do a print screen, cool, really cool, though hard to use ... I think they license the tech also, check it out. (If I am wrong please post here how it can be hacked.)\nFound it here: How do I prevent print screen\n']",https://stackoverflow.com/questions/396817/protection-from-screen-scraping,screen-scraping
CasperJS passing data back to PHP,"
CasperJS is being called by PHP using an exec() command. After CasperJS does its work such as retrieving parts of a webpage, how can the retrieved data be returned back to PHP?
",11k,"
            9
        ","['\nI think the best way to transfer data from CasperJS to another language such as PHP is running CasperJS script as a service. Because CasperJS has been written over PhantomJS, CasperJS can use an embedded web server module of PhantomJS called Mongoose.\nFor information about how works the embedded web server see here\nHere an example about how a CasperJS script can start a web server.\n//define ip and port to web service\nvar ip_server = \'127.0.0.1:8585\';\n\n//includes web server modules\nvar server = require(\'webserver\').create();\n\n//start web server\nvar service = server.listen(ip_server, function(request, response) {\n\n    var links = [];\n    var casper = require(\'casper\').create();\n\n    function getLinks() {\n        var links = document.querySelectorAll(\'h3.r a\');\n        return Array.prototype.map.call(links, function(e) {\n            return e.getAttribute(\'href\')\n        });\n    }\n\n    casper.start(\'http://google.fr/\', function() {\n        // search for \'casperjs\' from google form\n        this.fill(\'form[action=""/search""]\', { q: \'casperjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'casperjs\' search\n        links = this.evaluate(getLinks);\n        // now search for \'phantomjs\' by filling the form again\n        this.fill(\'form[action=""/search""]\', { q: \'phantomjs\' }, true);\n    });\n\n    casper.then(function() {\n        // aggregate results for the \'phantomjs\' search\n        links = links.concat(this.evaluate(getLinks));\n    });\n\n    //\n    casper.run(function() {\n            response.statusCode = 200;\n            //sends results as JSON object\n            response.write(JSON.stringify(links, null, null));\n            response.close();              \n    });\n\n});\nconsole.log(\'Server running at http://\' + ip_server+\'/\');\n\n', ""\nYou can redirect output from stdout to an array.\nOn this page it says you can do:  \nstring exec ( string $command [, array &$output [, int &$return_var ]] )\n\nIt goes on to say: \n\nIf the output argument is present, then the specified array will be filled with every line of output from the command. \n\nSo basically you can do exec('casperjs command here, $array_here);\n""]",https://stackoverflow.com/questions/15852987/casperjs-passing-data-back-to-php,screen-scraping
unable to call firefox from selenium in python on AWS machine,"
I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:
In [1]: from selenium import webdriver

In [2]: br = webdriver.Firefox()
---------------------------------------------------------------------------
WebDriverException                        Traceback (most recent call last)
/home/ubuntu/<ipython-input-2-d6a5d754ea44> in <module>()
----> 1 br = webdriver.Firefox()

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.pyc in __init__(self, firefox_profile, firefox_binary, timeout)
     49         RemoteWebDriver.__init__(self,
     50             command_executor=ExtensionConnection(""127.0.0.1"", self.profile,
---> 51             self.binary, timeout),
     52             desired_capabilities=DesiredCapabilities.FIREFOX)
     53

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/extension_connection.pyc in __init__(self, host, firefox_profile, firefox_binary, timeout)
     45         self.profile.add_extension()
     46
---> 47         self.binary.launch_browser(self.profile)
     48         _URL = ""http://%s:%d/hub"" % (HOST, PORT)
     49         RemoteConnection.__init__(

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in launch_browser(self, profile)
     42
     43         self._start_from_profile_path(self.profile.path)
---> 44         self._wait_until_connectable()
     45
     46     def kill(self):

/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.pyc in _wait_until_connectable(self)
     79                 raise WebDriverException(""The browser appears to have exited ""
     80                       ""before we could connect. The output was: %s"" %
---> 81                       self._get_firefox_output())
     82             if count == 30:
     83                 self.kill()

WebDriverException: Message: 'The browser appears to have exited before we could connect. The output was: Error: no display specified\n'

I did search on the web and found that this problem happened with other people (https://groups.google.com/forum/?fromgroups=#!topic/selenium-users/21sJrOJULZY). But I don't understand the solution, if it is. 
Can anyone help me please? Thanks!
",24k,"
            33
        ","['\n', '\n', '\n', '\n']",https://stackoverflow.com/questions/13039530/unable-to-call-firefox-from-selenium-in-python-on-aws-machine,screen-scraping
scrape websites with infinite scrolling,"
I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.
",31k,"
            31
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stckoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nStep 3 : Print the data if required.\n', ""\nMost sites that have infinite scrolling do (as Lattyware notes) have a proper API as well, and you will likely be better served by using this rather than scraping.\nBut if you must scrape...\nSuch sites are using JavaScript to request additional content from the site when you reach the bottom of the page. All you need to do is figure out the URL of that additional content and you can retrieve it. Figuring out the required URL can be done by inspecting the script, by using the Firefox Web console, or by using a debug proxy.\nFor example, open the Firefox Web Console, turn off all the filter buttons except Net, and load the site you wish to scrape. You'll see all the files as they are loaded. Scroll the page while watching the Web Console and you'll see the URLs being used for the additional requests. Then you can request that URL yourself and see what format the data is in (probably JSON) and get it into your Python script.\n"", '\nFinding the url of the ajax source will be the best option but it can be cumbersome for certain sites. Alternatively you could use a headless browser like QWebKit from PyQt and send keyboard events while reading the data from the DOM tree. QWebKit has a nice and simple api.\n']",https://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling,screen-scraping
Screen Scraping from a web page with a lot of Javascript [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I have been asked to write an app which screen scrapes info from an intranet web page and presents the certain info from it in a nice easy to view format. The web page is a real mess and requires the user to click on half a dozen icons to discover if an ordered item has arrived or has been receipted. As you can imagine users find this irritating to say the least and it would be nice to have an app anyone can use that lists the state of their orders in a single screen.
Yes I know a better solution would be to re-write the web app but that would involve calling in the vendor and would cost us as small fortune.
Anyway while looking into this I discovered the web page I want to scrape is mostly Javascript (although it doesn't use any AJAX techniques). Does anyone know if a library or program exists which I could feed with the Javascript and which would then spit out the DOM for my app to parse ? 
I can pretty much write the app in any language but my preference would be JavaFX just so I could have a play with it.
Thanks for your time.
Ian
",18k,"
            17
        ","[""\nYou may consider using HTMLunit\nIt's a java class library made to automate browsing without having to control a browser, and it integrates the Mozilla Rhino Javascript engine to process javascript on the pages it loads. There's also a JRuby wrapper for that, named Celerity. Its javascript support is not really perfect right now, but if your pages don't use many hacks things should work fine the performance should be way better than controlling a browser. Furthermore, you don't have to worry about cookies being persisted after your scraping is over and all the other nasty things connected to controlling a browser (history, autocomplete, temp files etc).\n"", ""\nSince you say that no AJAX is used, then all the info is present at the HTML source. The javascript just renders it based on user clicks. So you need to reverse engineer the way the application works, parse the html and the javascript code and extract the useful information. It is strictly business of text parsing - you shouldn't deal with running javascript and producing a new DOM. This would be much more difficult to do.\nIf AJAX was used, your job would be easier. You could easily find out how the AJAX services work (probably by receiving JSON and XML) and extract the information.\n"", '\nYou could consider using a greasemonkey JS. greasemonkey is a very powerful Firefox add on that allows you to run your own script alongside that of specific web sites. This allows you to modify how the web site is displayed, add or remove content. You can even use it to  do AJAX style lookups and add dynamic content. \nIf your tool is for in house use, and users are all happy to use Firefox then this could be a winner.\nRegards\n', '\nI suggest IRobotSoft web scraper.  It is a dedicated free software for screen scraping with the best javascript support.  You can create and test a robot with its visual interface.  You can also embed it into your own application using its ActiveX control and hide the browser window. \n', ""\nI'd go with Perl's Win32::IE::Mechanize which lets you automate Internet Explorer. You should be able to click on icons and extract text while letting MSIE do the annoying tasks of processing all the JS.\n"", ""\nI agree with kgiannakakis' answer. I'd be suprised if you couldn't reverse engineer the javascript to identify where the information comes from and then write some simple Python scripts using Urllib2 and the Beautiful Soup library to scrape the same information.\nIf Python and scraping are a new idea, there's some excellent tutorials available on how to get going.\n[Edit] Looks like there's a Python version of mechanize too. Time to re-write some scrapers I developed a while back! :-)\n"", '\nI created a project site2archive that uses phantomJs to render including JS stuff and wget to scrape. phantomJs is based on Webkit, that delivers a similar browsing environment as Safari and Google Chrome.\n']",https://stackoverflow.com/questions/857515/screen-scraping-from-a-web-page-with-a-lot-of-javascript,screen-scraping
Scraping javascript website in R,"
I want to scrape the match time and date from this url:
http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary
By using the chrome dev tools, I can see this appears to be generated using the following code:
<td colspan=""3"" id=""utime"" class=""mstat-date"">01:20 AM, October 29, 2014</td>

But this is not in the source html.
I think this is because its java (correct me if Im wrong). How can I scrape this information using R?
",8k,"
            9
        ","['\nSo, RSelenium is not the only answer (anymore). If you can install the PhantomJS binary (grab phantomjs binaries from here: http://phantomjs.org/) then you can use it to render the HTML and scrape it with rvest (similar to the RSelenium approach but doesn\'t require java):\nlibrary(rvest)\n\n# render HTML from the site with phantomjs\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n## [1] ""10:20 AM, October 28, 2014""\n\n', '\nYou could also use docker as the web driver (in place of selenium)\nYou will still need to install phantomjs, and docker too. Then run:\nlibrary(RSelenium)\n\nurl <- ""http://www.scoreboard.com/game/rosol-l-goffin-d-2014/8drhX07d/#game-summary""\n\nsystem(\'docker run -d -p 4445:4444 selenium/standalone-chrome\') \nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""chrome"")\nremDr$open()\nremDr$navigate(url)\n\nwriteLines(sprintf(""var page = require(\'webpage\').create();\npage.open(\'%s\', function () {\n    console.log(page.content); //page source\n    phantom.exit();\n});"", url), con=""scrape.js"")\n\nsystem(""phantomjs scrape.js > scrape.html"", intern = T)\n\n# extract the content you need\npg <- read_html(""scrape.html"")\npg %>% html_nodes(""#utime"") %>% html_text()\n\n# [1] ""10:20 AM, October 28, 2014""\n\n']",https://stackoverflow.com/questions/26631511/scraping-javascript-website-in-r,screen-scraping
Perform screen-scape of Webbrowser control in thread,"
I am using the technique shown in 

WebBrowser Control in a new thread

Trying to get a screen-scrape of a webpage I have been able to get the following code to successfully work when the WebBrowser control is placed on a WinForm. However it fails by providing an arbitrary image of the desktop when run inside a thread.
Thread browserThread = new Thread(() =>
{
    WebBrowser br = new WebBrowser();
    br.DocumentCompleted += webBrowser1_DocumentCompleted;
    br.ProgressChanged += webBrowser1_ProgressChanged;
    br.ScriptErrorsSuppressed = true;
    br.Navigate(url);
    Application.Run();
});
browserThread.SetApartmentState(ApartmentState.STA);
browserThread.Start();

private Image TakeSnapShot(WebBrowser browser)
{
    int width;
    int height;

    width = browser.ClientRectangle.Width;
    height = browser.ClientRectangle.Height;

    Bitmap image = new Bitmap(width, height);

    using (Graphics graphics = Graphics.FromImage(image))
    {
        Point p = new Point(0, 0);
        Point upperLeftSource = browser.PointToScreen(p);
        Point upperLeftDestination = new Point(0, 0);

        Size blockRegionSize = browser.ClientRectangle.Size;
        blockRegionSize.Width = blockRegionSize.Width - 15;
        blockRegionSize.Height = blockRegionSize.Height - 15;
        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);
    }

    return image;
}

This obviously happens because of the method Graphics.CopyFromScreen() but I am unaware of any other approach. Is there a way to resolve this issue that anyone could suggest? or is my only option to create a form, add the control, make it visible and then screen-scrape? For obvious reasons I'm hoping to avoid such an approach.
",4k,"
            3
        ","['\nYou can write \nprivate Image TakeSnapShot(WebBrowser browser)\n{\n     browser.Width = browser.Document.Body.ScrollRectangle.Width;\n     browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n     Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n     browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n     return bitmap;\n}\n\nA full working code\nvar image = await WebUtils.GetPageAsImageAsync(""http://www.stackoverflow.com"");\nimage.Save(fname , System.Drawing.Imaging.ImageFormat.Bmp);\n\n\npublic class WebUtils\n{\n    public static Task<Image> GetPageAsImageAsync(string url)\n    {\n        var tcs = new TaskCompletionSource<Image>();\n\n        var thread = new Thread(() =>\n        {\n            WebBrowser browser = new WebBrowser();\n            browser.Size = new Size(1280, 768);\n\n            WebBrowserDocumentCompletedEventHandler documentCompleted = null;\n            documentCompleted = async (o, s) =>\n            {\n                browser.DocumentCompleted -= documentCompleted;\n                await Task.Delay(2000); //Run JS a few seconds more\n\n                Bitmap bitmap = TakeSnapshot(browser);\n\n                tcs.TrySetResult(bitmap);\n                browser.Dispose();\n                Application.ExitThread();\n            };\n\n            browser.ScriptErrorsSuppressed = true;\n            browser.DocumentCompleted += documentCompleted;\n            browser.Navigate(url);\n            Application.Run();\n        });\n\n        thread.SetApartmentState(ApartmentState.STA);\n        thread.Start();\n\n        return tcs.Task;\n    }\n\n    private static Bitmap TakeSnapshot(WebBrowser browser)\n    {\n         browser.Width = browser.Document.Body.ScrollRectangle.Width;\n         browser.Height= browser.Document.Body.ScrollRectangle.Height;\n\n         Bitmap bitmap = new Bitmap(browser.Width - System.Windows.Forms.SystemInformation.VerticalScrollBarWidth, browser.Height);\n\n         browser.DrawToBitmap(bitmap, new Rectangle(0, 0, bitmap.Width, bitmap.Height));\n\n         return bitmap;\n    }\n}\n\n', '\nusing (Graphics graphics = Graphics.FromImage(image))\n    {\n        Point p = new Point(0, 0);\n        Point upperLeftSource = browser.PointToScreen(p);\n        Point upperLeftDestination = new Point(0, 0);\n\n        Size blockRegionSize = browser.ClientRectangle.Size;\n        blockRegionSize.Width = blockRegionSize.Width - 15;\n        blockRegionSize.Height = blockRegionSize.Height - 15;\n        graphics.CopyFromScreen(upperLeftSource, upperLeftDestination, blockRegionSize);\n    }\n\nDo you really need using statement ? \nYou also return image but how is copied image assigned to it? \n']",https://stackoverflow.com/questions/18675606/perform-screen-scape-of-webbrowser-control-in-thread,screen-scraping
Looping over urls to do the same thing,"
I am tring to scrape a few sites. Here is my code:
for (var i = 0; i < urls.length; i++) {
    url = urls[i];
    console.log(""Start scraping: "" + url);

    page.open(url, function () {
        waitFor(function() {
            return page.evaluate(function() {
                return document.getElementById(""progressWrapper"").childNodes.length == 1;
            });

        }, function() {
            var price = page.evaluate(function() {
                // do something
                return price;
            });

            console.log(price);
            result = url + "" ; "" + price;
            output = output + ""\r\n"" + result;
        });
    });

}
fs.write('test.txt', output);
phantom.exit();

I want to scrape all sites in the array urls, extract some information and then write this information to a text file.
But there seems to be a problem with the for loop. When scraping only one site without using a loop, all works as I want. But with the loop, first nothing happens, then the line 
console.log(""Start scraping: "" + url);

is shown, but one time too much.
If url = {a,b,c}, then phantomjs does:
Start scraping: a 
Start scraping: b 
Start scraping: c 
Start scraping:

It seems that page.open isn't called at all.
I am newbie to JS so I am sorry for this stupid question.
",4k,"
            1
        ","['\nPhantomJS is asynchronous. By calling page.open() multiple times using a loop, you essentially rush the execution of the callback. You\'re overwriting the current request before it is finished with a new request which is then again overwritten. You need to execute them one after the other, for example like this:\npage.open(url, function () {\n    waitFor(function() {\n       // something\n    }, function() {\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                // and so on\n            });\n        });\n    });\n});\n\nBut this is tedious. There are utilities that can help you with writing nicer code like async.js. You can install it in the directory of the phantomjs script through npm.\nvar async = require(""async""); // install async through npm\nvar tests = urls.map(function(url){\n    return function(callback){\n        page.open(url, function () {\n            waitFor(function() {\n               // something\n            }, function() {\n                callback();\n            });\n        });\n    };\n});\nasync.series(tests, function finish(){\n    fs.write(\'test.txt\', output);\n    phantom.exit();\n});\n\nIf you don\'t want any dependencies, then it is also easy to define your own recursive function (from here):\nvar urls = [/*....*/];\n\nfunction handle_page(url){\n    page.open(url, function(){\n        waitFor(function() {\n           // something\n        }, function() {\n            next_page();\n        });\n    });\n}\n\nfunction next_page(){\n    var url = urls.shift();\n    if(!urls){\n        phantom.exit(0);\n    }\n    handle_page(url);\n}\n\nnext_page();\n\n']",https://stackoverflow.com/questions/26681464/looping-over-urls-to-do-the-same-thing,screen-scraping
Scraping contents of multi web pages of a website using BeautifulSoup and Selenium,"
The website I want to scrap is :
http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061
I want to get the last page number of the above the link for proceeding, which is 499 while taking the screenshot.

My code :
   from bs4 import BeautifulSoup 
   from urllib.request import urlopen as uReq
   from selenium import webdriver;import time
   from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC
   from selenium.webdriver.common.desired_capabilities import         DesiredCapabilities

   firefox_capabilities = DesiredCapabilities.FIREFOX
   firefox_capabilities['marionette'] = True
   firefox_capabilities['binary'] = '/etc/firefox'

   driver = webdriver.Firefox(capabilities=firefox_capabilities)
   url = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""

   driver.get(url)
   wait = WebDriverWait(driver, 10)
   soup=BeautifulSoup(driver.page_source,""lxml"")
   containers = soup.findAll(""ul"",{""class"":""pages table""})
   containers[0] = soup.findAll(""li"")
   li_len = len(containers[0])
   for item in soup.find(""ul"",{""class"":""pages table""}) : 
   li_text = item.select(""li"")[li_len].text
   print(""li_text : {}\n"".format(li_text))
   driver.quit()

I need help to figure out the error in my code for getting the last page number. Also, I would be grateful if someone give the alternate solution for the same and suggest ways to achieve my intention.
",1k,"
            0
        ","['\nIf you want to get the last page number of the above the link for proceeding, which is 499 you can use either Selenium or Beautifulsoup as follows :\n\nSelenium :\nfrom selenium import webdriver\n\ndriver = webdriver.Firefox(executable_path=r\'C:\\Utility\\BrowserDrivers\\geckodriver.exe\')\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\ndriver.get(url)\nelement = driver.find_element_by_xpath(""//div[@class=\'row pagination\']//p/span[contains(.,\'Reviews on Reliance Jio\')]"")\ndriver.execute_script(""return arguments[0].scrollIntoView(true);"", element)\nprint(driver.find_element_by_xpath(""//ul[@class=\'pagination table\']/li/ul[@class=\'pages table\']//li[last()]/a"").get_attribute(""innerHTML""))\ndriver.quit()\n\nConsole Output :\n499\n\n\nBeautifulsoup :\nimport bs4\nfrom bs4 import BeautifulSoup as soup\nfrom urllib.request import urlopen as uReq\n\nurl = ""http://www.mouthshut.com/mobile-operators/Reliance-Jio-reviews-925812061""\nuClient = uReq(url)\npage_html = uClient.read()\nuClient.close()\npage_soup = soup(page_html, ""html.parser"")\ncontainer = page_soup.find(""ul"",{""class"":""pages table""})\nall_li = container.findAll(""li"")\nlast_div = None\nfor last_div in all_li:pass\nif last_div:\n    content = last_div.getText()\n    print(content)\n\nConsole Output :\n499\n\n']",https://stackoverflow.com/questions/47869382/scraping-contents-of-multi-web-pages-of-a-website-using-beautifulsoup-and-seleni,screen-scraping
Scrape web pages in real time with Node.js,"
What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available.
Let's assume that this script should just provide the results in JSON format, and we can process them either directly in the browser or in another web application.
A few starting points:
Using node.js and jquery to scrape websites
Anybody have any ideas?
",46k,"
            66
        ","['\nNode.io seems to take the cake :-)\n', '\nAll aforementioned solutions presume running the scraper locally. This means you will be severely limited in performance (due to running them in sequence or in a limited set of threads). A better approach, imho, is to rely on an existing, albeit commercial, scraping grid.\nHere is an example:\nvar bobik = new Bobik(""YOUR_AUTH_TOKEN"");\nbobik.scrape({\n  urls: [\'amazon.com\', \'zynga.com\', \'http://finance.google.com/\', \'http://shopping.yahoo.com\'],\n  queries:  [""//th"", ""//img/@src"", ""return document.title"", ""return $(\'script\').length"", ""#logo"", "".logo""]\n}, function (scraped_data) {\n  if (!scraped_data) {\n    console.log(""Data is unavailable"");\n    return;\n  }\n  var scraped_urls = Object.keys(scraped_data);\n  for (var url in scraped_urls)\n    console.log(""Results from "" + url + "": "" + scraped_data[scraped_urls[url]]);\n});\n\nHere, scraping is performed remotely and a callback is issued to your code only when results are ready (there is also an option to collect results as they become available).\nYou can download Bobik client proxy SDK at https://github.com/emirkin/bobik_javascript_sdk\n', ""\nI've been doing research myself, and https://npmjs.org/package/wscraper boasts itself as a\n\na web scraper agent based on cheerio.js a fast, flexible, and lean\n  implementation of core jQuery; built on top of request.js; inspired by\n  http-agent.js\n\nVery low usage (according to npmjs.org) but worth a look for any interested parties.\n"", ""\nYou don't always need to jQuery. If you play with the DOM returned from jsdom for example you can easily take what you need yourself (also considering you dont have to worry about xbrowser issues.) See: https://gist.github.com/1335009 that's not taking away from node.io at all, just saying you might be able to do it yourself depending...\n"", ""\nThe new way using ES7/promises\nUsually when you're scraping you want to use some method to\n\nGet the resource on the webserver (html document usually)\nRead that resource and work with it as\n\n\nA DOM/tree structure and make it navigable\nparse it as token-document with something like SAS.\n\n\nBoth tree, and token-parsing have advantages, but tree is usually substantially simpler. We'll do that. Check out request-promise, here is how it works:\nconst rp = require('request-promise');\nconst cheerio = require('cheerio'); // Basically jQuery for node.js \n\nconst options = {\n    uri: 'http://www.google.com',\n    transform: function (body) {\n        return cheerio.load(body);\n    }\n};\n\nrp(options)\n    .then(function ($) {\n        // Process html like you would with jQuery... \n    })\n    .catch(function (err) {\n        // Crawling failed or Cheerio \n\nThis is using cheerio which is essentially a lightweight server-side jQuery-esque library (that doesn't need a window object, or jsdom).\nBecause you're using promises, you can also write this in an asychronous function. It'll look synchronous, but it'll be asynchronous with ES7:\nasync function parseDocument() {\n    let $;\n    try {\n      $ = await rp(options);\n    } catch (err) { console.error(err); }\n\n    console.log( $('title').text() ); // prints just the text in the <title>\n}\n\n"", '\ncheck out https://github.com/rc0x03/node-promise-parser\nFast: uses libxml C bindings\nLightweight: no dependencies like jQuery, cheerio, or jsdom\nClean: promise based interface- no more nested callbacks\nFlexible: supports both CSS and XPath selectors\n\n', ""\nI see most answers the right path with cheerio and so forth, however once you get to the point where you need to parse and execute JavaScript (ala SPA's and more), then I'd check out https://github.com/joelgriffith/navalia (I'm the author). Navalia is built to support scraping in a headless-browser context, and it's pretty quick. Thanks!\n"", '\nIt is my easy to use but badly spelled general purpose scraper https://github.com/harish2704/html-scraper written for Node.JS\nIt can extract information based on predefined schemas.\nA schema defnition includes a css selector and a data extraction function.\nIt currently using cheerio for dom parsing..\n']",https://stackoverflow.com/questions/5211486/scrape-web-pages-in-real-time-with-node-js,screen-scraping
How can I scrape an HTML table to CSV?,"
The Problem
I use a tool at work that lets me do queries and get back HTML tables of info. I do not have any kind of back-end access to it.
A lot of this info would be much more useful if I could put it into a spreadsheet for sorting, averaging, etc. How can I screen-scrape this data to a CSV file?
My First Idea
Since I know jQuery, I thought I might use it to strip out the table formatting onscreen, insert commas and line breaks, and just copy the whole mess into notepad and save as a CSV. Any better ideas?
The Solution
Yes, folks, it really was as easy as copying and pasting. Don't I feel silly.
Specifically, when I pasted into the spreadsheet, I had to select ""Paste Special"" and choose the format ""text."" Otherwise it tried to paste everything into a single cell, even if I highlighted the whole spreadsheet.
",83k,"
            44
        ","[""\n\nSelect the HTML table in your tools's UI and copy it into the clipboard (if that's possible\nPaste it into Excel.\nSave as CSV file\n\nHowever, this is a manual solution not an automated one.\n"", '\nusing python: \nfor example imagine you want to scrape forex quotes in csv form from some site like:fxquotes\nthen...\nfrom BeautifulSoup import BeautifulSoup\nimport urllib,string,csv,sys,os\nfrom string import replace\n\ndate_s = \'&date1=01/01/08\'\ndate_f = \'&date=11/10/08\'\nfx_url = \'http://www.oanda.com/convert/fxhistory?date_fmt=us\'\nfx_url_end = \'&lang=en&margin_fixed=0&format=CSV&redirected=1\'\ncur1,cur2 = \'USD\',\'AUD\'\nfx_url = fx_url + date_f + date_s + \'&exch=\' + cur1 +\'&exch2=\' + cur1\nfx_url = fx_url +\'&expr=\' + cur2 +  \'&expr2=\' + cur2 + fx_url_end\ndata = urllib.urlopen(fx_url).read()\nsoup = BeautifulSoup(data)\ndata = str(soup.findAll(\'pre\', limit=1))\ndata = replace(data,\'[<pre>\',\'\')\ndata = replace(data,\'</pre>]\',\'\')\nfile_location = \'/Users/location_edit_this\'\nfile_name = file_location + \'usd_aus.csv\'\nfile = open(file_name,""w"")\nfile.write(data)\nfile.close()\n\n\nedit: to get values from a table:\nexample from: palewire\nfrom mechanize import Browser\nfrom BeautifulSoup import BeautifulSoup\n\nmech = Browser()\n\nurl = ""http://www.palewire.com/scrape/albums/2007.html""\npage = mech.open(url)\n\nhtml = page.read()\nsoup = BeautifulSoup(html)\n\ntable = soup.find(""table"", border=1)\n\nfor row in table.findAll(\'tr\')[1:]:\n    col = row.findAll(\'td\')\n\n    rank = col[0].string\n    artist = col[1].string\n    album = col[2].string\n    cover_link = col[3].img[\'src\']\n\n    record = (rank, artist, album, cover_link)\n    print ""|"".join(record)\n\n', '\nThis is my python version using the (currently) latest version of BeautifulSoup which can be obtained using, e.g.,\n$ sudo easy_install beautifulsoup4\n\nThe script reads HTML from the standard input, and outputs the text found in all tables in proper CSV format.\n#!/usr/bin/python\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\nsoup = BeautifulSoup(sys.stdin.read())\noutput = csv.writer(sys.stdout)\n\nfor table in soup.find_all(\'table\'):\n    for row in table.find_all(\'tr\'):\n        col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n        output.writerow(col)\n    output.writerow([])\n\n', '\nEven easier (because it saves it for you for next time) ...\nIn Excel\nData/Import External Data/New Web Query\nwill take you to a url prompt. Enter your url, and it will delimit available tables on the page to import. Voila.\n', '\nTwo ways come to mind (especially for those of us that don\'t have Excel):\n\nGoogle Spreadsheets has an excellent importHTML function: \n\n=importHTML(""http://example.com/page/with/table"", ""table"", index\nIndex starts at 1\nI recommend a copy and paste values shortly after import\nFile -> Download as -> CSV\n\nPython\'s superb Pandas library has handy read_html and to_csv functions\n\nHere\'s a basic Python3 script that prompts for the URL, which table at that URL, and a filename for the CSV.\n\n\n', '\nQuick and dirty:\nCopy out of browser into Excel, save as CSV.\nBetter solution (for long term use):\nWrite a bit of code in the language of your choice that will pull the html contents down, and scrape out the bits that you want.  You could probably throw in all of the data operations (sorting, averaging, etc) on top of the data retrieval.  That way, you just have to run your code and you get the actual report that you want.\nIt all depends on how often you will be performing this particular task.\n', '\nExcel can open a http page.\nEg:\n\nClick File, Open\nUnder filename, paste the URL  ie: How can I scrape an HTML table to CSV?\nClick ok\n\nExcel does its best to convert the html to a table.\nIts not the most elegant solution, but does work!\n', '\nBasic Python implementation using BeautifulSoup, also considering both rowspan and colspan:\nfrom BeautifulSoup import BeautifulSoup\n\ndef table2csv(html_txt):\n   csvs = []\n   soup = BeautifulSoup(html_txt)\n   tables = soup.findAll(\'table\')\n\n   for table in tables:\n       csv = \'\'\n       rows = table.findAll(\'tr\')\n       row_spans = []\n       do_ident = False\n\n       for tr in rows:\n           cols = tr.findAll([\'th\',\'td\'])\n\n           for cell in cols:\n               colspan = int(cell.get(\'colspan\',1))\n               rowspan = int(cell.get(\'rowspan\',1))\n\n               if do_ident:\n                   do_ident = False\n                   csv += \',\'*(len(row_spans))\n\n               if rowspan > 1: row_spans.append(rowspan)\n\n               csv += \'""{text}""\'.format(text=cell.text) + \',\'*(colspan)\n\n           if row_spans:\n               for i in xrange(len(row_spans)-1,-1,-1):\n                   row_spans[i] -= 1\n                   if row_spans[i] < 1: row_spans.pop()\n\n           do_ident = True if row_spans else False\n\n           csv += \'\\n\'\n\n       csvs.append(csv)\n       #print csv\n\n   return \'\\n\\n\'.join(csvs)\n\n', '\nHere is a tested example that combines grequest and soup to download large quantities of pages from a structured website:\n#!/usr/bin/python\n\nfrom bs4 import BeautifulSoup\nimport sys\nimport re\nimport csv\nimport grequests\nimport time\n\ndef cell_text(cell):\n    return "" "".join(cell.stripped_strings)\n\ndef parse_table(body_html):\n    soup = BeautifulSoup(body_html)\n    for table in soup.find_all(\'table\'):\n        for row in table.find_all(\'tr\'):\n            col = map(cell_text, row.find_all(re.compile(\'t[dh]\')))\n            print(col)\n\ndef process_a_page(response, *args, **kwargs): \n    parse_table(response.content)\n\ndef download_a_chunk(k):\n    chunk_size = 10 #number of html pages\n    x = ""http://www.blahblah....com/inclusiones.php?p=""\n    x2 = ""&name=...""\n    URLS = [x+str(i)+x2 for i in range(k*chunk_size, k*(chunk_size+1)) ]\n    reqs = [grequests.get(url, hooks={\'response\': process_a_page}) for url in URLS]\n    resp = grequests.map(reqs, size=10)\n\n# download slowly so the server does not block you\nfor k in range(0,500):\n    print(""downloading chunk "",str(k))\n    download_a_chunk(k)\n    time.sleep(11)\n\n', ""\nHave you tried opening it with excel?\nIf you save a spreadsheet in excel as html you'll see the format excel uses.\nFrom a web app I wrote I spit out this html format so the user can export to excel.\n"", ""\nIf you're screen scraping and the table you're trying to convert has a given ID, you could always do a regex parse of the html along with some scripting to generate a CSV.\n""]",https://stackoverflow.com/questions/259091/how-can-i-scrape-an-html-table-to-csv,screen-scraping
How to download any(!) webpage with correct charset in python?,"
Problem
When screen-scraping a webpage using python one has to know the character encoding of the page. If you get the character encoding wrong than your output will be messed up.
People usually use some rudimentary technique to detect the encoding. They either use the charset from the header or the charset defined in the meta tag or they use an encoding detector (which does not care about meta tags or headers).
By using only one these techniques, sometimes you will not get the same result as you would in a browser.
Browsers do it this way:

Meta tags always takes precedence (or xml definition)
Encoding defined in the header is used when there is no charset defined in a meta tag
If the encoding is not defined at all, than it is time for encoding detection.

(Well... at least that is the way I believe most browsers do it. Documentation is really scarce.)
What I'm looking for is a library that can decide the character set of a page the way a browser would. I'm sure I'm not the first who needs a proper solution to this problem.
Solution (I have not tried it yet...)
According to Beautiful Soup's documentation.
Beautiful Soup tries the following encodings, in order of priority, to turn your document into Unicode:

An encoding you pass in as the
fromEncoding argument to the soup
constructor.
An encoding discovered  in the document itself: for instance,   in an XML declaration or (for HTML   documents) an http-equiv META tag. If   Beautiful Soup finds this kind of   encoding within the document, it   parses the document again from the   beginning and gives the new encoding   a try. The only exception is if you   explicitly specified an encoding, and   that encoding actually worked: then   it will ignore any encoding it finds   in the document.
An encoding sniffed   by looking at the first few bytes of   the file. If an encoding is detected
at this stage, it will be one of the
UTF-* encodings, EBCDIC, or ASCII.
An
encoding sniffed by the chardet
library, if you have it installed.
UTF-8
Windows-1252

",16k,"
            35
        ","[""\nWhen you download a file with urllib or urllib2, you can find out whether a charset header was transmitted:\nfp = urllib2.urlopen(request)\ncharset = fp.headers.getparam('charset')\n\nYou can use BeautifulSoup to locate a meta element in the HTML:\nsoup = BeatifulSoup.BeautifulSoup(data)\nmeta = soup.findAll('meta', {'http-equiv':lambda v:v.lower()=='content-type'})\n\nIf neither is available, browsers typically fall back to user configuration, combined with auto-detection. As rajax proposes, you could use the chardet module. If you have user configuration available telling you that the page should be Chinese (say), you may be able to do better.\n"", '\nUse the Universal Encoding Detector:\n>>> import chardet\n>>> chardet.detect(urlread(""http://google.cn/""))\n{\'encoding\': \'GB2312\', \'confidence\': 0.99}\n\nThe other option would be to just use wget:\n  import os\n  h = os.popen(\'wget -q -O foo1.txt http://foo.html\')\n  h.close()\n  s = open(\'foo1.txt\').read()\n\n', ""\nIt seems like you need a hybrid of the answers presented:\n\nFetch the page using urllib\nFind <meta> tags using beautiful soup or other method\nIf no meta tags exist, check the headers returned by urllib\nIf that still doesn't give you an answer, use the universal encoding detector.\n\nI honestly don't believe you're going to find anything better than that.  \nIn fact if you read further into the FAQ you linked to in the comments on the other answer, that's what the author of detector library advocates.\nIf you believe the FAQ, this is what the browsers do (as requested in your original question) as the detector is a port of the firefox sniffing code.\n"", '\nI would use html5lib for this.\n', ""\nScrapy downloads a page and detects a correct encoding for it, unlike requests.get(url).text or urlopen. To do so it tries to follow browser-like rules - this is the best one can do, because website owners have incentive to make their websites work in a browser. Scrapy needs to take HTTP headers, <meta> tags, BOM marks and differences in encoding names in account. \nContent-based guessing (chardet, UnicodeDammit) on its own is not a correct solution, as it may fail; it should be only used as a last resort when headers or <meta> or BOM marks are not available or provide no information.\nYou don't have to use Scrapy to get its encoding detection functions; they are released (among with some other stuff) in a separate library called w3lib: https://github.com/scrapy/w3lib. \nTo get page encoding and unicode body use w3lib.encoding.html_to_unicode function, with a content-based guessing fallback:\nimport chardet\nfrom w3lib.encoding import html_to_unicode\n\ndef _guess_encoding(data):\n    return chardet.detect(data).get('encoding')\n\ndetected_encoding, html_content_unicode = html_to_unicode(\n    content_type_header,\n    html_content_bytes,\n    default_encoding='utf8', \n    auto_detect_fun=_guess_encoding,\n)\n\n"", '\ninstead of trying to get a page then figuring out the charset the browser would use, why not just use a browser to fetch the page and check what charset it uses.. \nfrom win32com.client import DispatchWithEvents\nimport threading\n\n\nstopEvent=threading.Event()\n\nclass EventHandler(object):\n    def OnDownloadBegin(self):\n        pass\n\ndef waitUntilReady(ie):\n    """"""\n    copypasted from\n    http://mail.python.org/pipermail/python-win32/2004-June/002040.html\n    """"""\n    if ie.ReadyState!=4:\n        while 1:\n            print ""waiting""\n            pythoncom.PumpWaitingMessages()\n            stopEvent.wait(.2)\n            if stopEvent.isSet() or ie.ReadyState==4:\n                stopEvent.clear()\n                break;\n\nie = DispatchWithEvents(""InternetExplorer.Application"", EventHandler)\nie.Visible = 0\nie.Navigate(\'http://kskky.info\')\nwaitUntilReady(ie)\nd = ie.Document\nprint d.CharSet\n\n', '\nBeautifulSoup dose this with UnicodeDammit : Unicode, Dammit\n']",https://stackoverflow.com/questions/1495627/how-to-download-any-webpage-with-correct-charset-in-python,screen-scraping
"Nokogiri, open-uri, and Unicode Characters","
I'm using Nokogiri and open-uri to grab the contents of the title tag on a webpage, but am having trouble with accented characters.  What's the best way to deal with these?  Here's what I'm doing:
require 'open-uri'
require 'nokogiri'

doc = Nokogiri::HTML(open(link))
title = doc.at_css(""title"")

At this point, the title looks like this:

Rag\303\271

Instead of:

Ragù

How can I have nokogiri return the proper character (e.g. ù in this case)?
Here's an example URL:
http://www.epicurious.com/recipes/food/views/Tagliatelle-with-Duck-Ragu-242037
",23k,"
            27
        ","['\nSummary: When feeding UTF-8 to Nokogiri through open-uri, use open(...).read and pass the resulting string to Nokogiri.\nAnalysis:\nIf I fetch the page using curl, the headers properly show Content-Type: text/html; charset=UTF-8 and the file content includes valid UTF-8, e.g. ""Genealogía de Jesucristo"". But even with a magic comment on the Ruby file and setting the doc encoding, it\'s no good:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\ndoc = Nokogiri::HTML(open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\'))\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1]\nputs h52.text, h52.text.encoding\n#=> GenealogÃ a de Jesucristo\n#=> UTF-8\n\nWe can see that this is not the fault of open-uri:\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ngene = html.read[/Gene\\S+/]\nputs gene, gene.encoding\n#=> Genealogía\n#=> UTF-8\n\nThis is a Nokogiri issue when dealing with open-uri, it seems. This can be worked around by passing the HTML as a raw string to Nokogiri:\n# encoding: UTF-8\nrequire \'nokogiri\'\nrequire \'open-uri\'\n\nhtml = open(\'http://www.biblegateway.com/passage/?search=Mateo1-2&version=NVI\')\ndoc = Nokogiri::HTML(html.read)\ndoc.encoding = \'utf-8\'\nh52 = doc.css(\'h5\')[1].text\nputs h52, h52.encoding, h52 == ""Genealogía de Jesucristo""\n#=> Genealogía de Jesucristo\n#=> UTF-8\n#=> true\n\n', ""\nI was having the same problem and the Iconv approach wasn't working. Nokogiri::HTML is an alias to Nokogiri::HTML.parse(thing, url, encoding, options).\nSo, you just need to do:\ndoc = Nokogiri::HTML(open(link).read, nil, 'utf-8')\nand it'll convert the page encoding properly to utf-8. You'll see Ragù instead of Rag\\303\\271.\n"", '\nWhen you say ""looks like this,"" are you viewing this value IRB? It\'s going to escape non-ASCII range characters with C-style escaping of the byte sequences that represent the characters.\nIf you print them with puts, you\'ll get them back as you expect, presuming your shell console is using the same encoding as the string in question (Apparently UTF-8 in this case, based on the two bytes returned for that character). If you are storing the values in a text file, printing to a handle should also result in UTF-8 sequences.\nIf you need to translate between UTF-8 and other encodings, the specifics depend on whether you\'re in Ruby 1.9 or 1.8.6.\nFor 1.9: http://blog.grayproductions.net/articles/ruby_19s_string\nfor 1.8, you probably need to look at Iconv.\nAlso, if you need to interact with COM components in Windows, you\'ll need to tell ruby to use the correct encoding with something like the following:\nrequire \'win32ole\'\n\nWIN32OLE.codepage = WIN32OLE::CP_UTF8\n\nIf you\'re interacting with mysql, you\'ll need to set the collation on the table to one that supports the encoding that you\'re working with. In general, it\'s best to set the collation to UTF-8, even if some of your content is coming back in other encodings; you\'ll just need to convert as necessary.\nNokogiri has some features for dealing with different encodings (probably through Iconv), but I\'m a little out of practice with that, so I\'ll leave explanation of that to someone else.\n', '\nTry setting the encoding option of Nokogiri, like so:\nrequire \'open-uri\'\nrequire \'nokogiri\'\ndoc = Nokogiri::HTML(open(link))\ndoc.encoding = \'utf-8\'\ntitle = doc.at_css(""title"")\n\n', '\nYou need to convert the response from the website being scraped (here epicurious.com) into utf-8 encoding.\nas per the html content from the page being scraped, its ""ISO-8859-1"" for now. So, you need to do something like this:\nrequire \'iconv\'\ndoc = Nokogiri::HTML(Iconv.conv(\'utf-8//IGNORE\', \'ISO-8859-1\', open(link).read))\n\nRead more about it here: http://www.quarkruby.com/2009/9/22/rails-utf-8-and-html-screen-scraping\n', '\nChanging Nokogiri::HTML(...) to Nokogiri::HTML5(...) fixed issues I was having with parsing certain special character, specifically em-dashes.\n(The accented characters in your link came through fine in both, so don\'t know if this would help you with that.)\nEXAMPLE:\nurl = \'https://www.youtube.com/watch?v=4r6gr7uytQA\'\n\ndoc = Nokogiri::HTML(open(url))\ndoc.title\n=> ""Josh Waitzkin â\\u0080\\u0094 How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\ndoc = Nokogiri::HTML5(open(url))\ndoc.title\n=> ""Josh Waitzkin — How to Cram 2 Months of Learning into 1 Day | The Tim Ferriss Show - YouTube""\n\n', '\nJust to add a cross-reference, this SO page gives some related information:\nHow to make Nokogiri transparently return un/encoded Html entities untouched?\n', ""\nTip: you could also use the Scrapifier gem to get metadata, as the page title, from URIs in a very simple way. The data are all encoded in UTF-8.\nCheck it out: https://github.com/tiagopog/scrapifier\nHope it's useful for you.\n""]",https://stackoverflow.com/questions/2572396/nokogiri-open-uri-and-unicode-characters,screen-scraping
Using Python and Mechanize to submit form data and authenticate,"
I want to submit login to the website Reddit.com, navigate to a particular area of the page, and submit a comment.  I don't see what's wrong with this code, but it is not working in that no change is reflected on the Reddit site.
import mechanize
import cookielib


def main():

#Browser
br = mechanize.Browser()


# Cookie Jar
cj = cookielib.LWPCookieJar()
br.set_cookiejar(cj)

# Browser options
br.set_handle_equiv(True)
br.set_handle_gzip(True)
br.set_handle_redirect(True)
br.set_handle_referer(True)
br.set_handle_robots(False)

# Follows refresh 0 but not hangs on refresh > 0
br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)

#Opens the site to be navigated
r= br.open('http://www.reddit.com')
html = r.read()

# Select the second (index one) form
br.select_form(nr=1)

# User credentials
br.form['user'] = 'DUMMYUSERNAME'
br.form['passwd'] = 'DUMMYPASSWORD'

# Login
br.submit()

#Open up comment page
r= br.open('http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/')
html = r.read()

#Text box is the 8th form on the page (which, I believe, is the text area)
br.select_form(nr=7)

#Change 'text' value to a testing string
br.form['text']= ""this is an automated test""

#Submit the information  
br.submit()

What's wrong with this?
",26k,"
            15
        ","['\nI would definitely suggest trying to use the API if possible, but this works for me (not for your example post, which has been deleted, but for any active one):\n#!/usr/bin/env python\n\nimport mechanize\nimport cookielib\nimport urllib\nimport logging\nimport sys\n\ndef main():\n\n    br = mechanize.Browser()\n    cj = cookielib.LWPCookieJar()\n    br.set_cookiejar(cj)\n\n    br.set_handle_equiv(True)\n    br.set_handle_gzip(True)\n    br.set_handle_redirect(True)\n    br.set_handle_referer(True)\n    br.set_handle_robots(False)\n\n    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n\n    r= br.open(\'http://www.reddit.com\')\n\n    # Select the second (index one) form\n    br.select_form(nr=1)\n\n    # User credentials\n    br.form[\'user\'] = \'user\'\n    br.form[\'passwd\'] = \'passwd\'\n\n    # Login\n    br.submit()\n\n    # Open up comment page\n    posting = \'http://www.reddit.com/r/PoopSandwiches/comments/f47f8/testing/\'\n    rval = \'PoopSandwiches\'\n    # you can get the rval in other ways, but this will work for testing\n\n    r = br.open(posting)\n\n    # You need the \'uh\' value from the first form\n    br.select_form(nr=0)\n    uh = br.form[\'uh\']\n\n    br.select_form(nr=7)\n    thing_id = br.form[\'thing_id\']\n    id = \'#\' + br.form.attrs[\'id\']\n    # The id that gets posted is the form id with a \'#\' prepended.\n\n    data = {\'uh\':uh, \'thing_id\':thing_id, \'id\':id, \'renderstyle\':\'html\', \'r\':rval, \'text\':""Your text here!""}\n    new_data_dict = dict((k, urllib.quote(v).replace(\'%20\', \'+\')) for k, v in data.iteritems())\n\n    # not sure if the replace needs to happen, I did it anyway\n    new_data = \'thing_id=%(thing_id)s&text=%(text)s&id=%(id)s&r=%(r)s&uh=%(uh)s&renderstyle=%(renderstyle)s\' %(new_data_dict)\n\n    # not sure which of these headers are really needed, but it works with all\n    # of them, so why not just include them.\n    req = mechanize.Request(\'http://www.reddit.com/api/comment\', new_data)\n    req.add_header(\'Referer\', posting)\n    req.add_header(\'Accept\', \' application/json, text/javascript, */*\')\n    req.add_header(\'Content-Type\', \'application/x-www-form-urlencoded; charset=UTF-8\')\n    req.add_header(\'X-Requested-With\', \'XMLHttpRequest\')\n    cj.add_cookie_header(req)\n    res = mechanize.urlopen(req)\n\nmain()\n\nIt would be interesting to turn javascript off and see how the reddit comments are handled then.  Right now there is a bunch of magic that happens in an onsubmit function called when making your post.  This is where the uh and id value get added.\n']",https://stackoverflow.com/questions/4720470/using-python-and-mechanize-to-submit-form-data-and-authenticate,screen-scraping
View Generated Source (After AJAX/JavaScript) in C#,"
Is there a way to view the generated source of a web page (the code after all AJAX calls and JavaScript DOM manipulations have taken place) from a C# application without opening up a browser from the code?
Viewing the initial page using a WebRequest or WebClient object works ok, but if the page makes extensive use of JavaScript to alter the DOM on page load, then these don't provide an accurate picture of the page.
I have tried using Selenium and Watin UI testing frameworks and they work perfectly, supplying the generated source as it appears after all JavaScript manipulations are completed.  Unfortunately, they do this by opening up an actual web browser, which is very slow.  I've implemented a selenium server which offloads this work to another machine, but there is still a substantial delay.
Is there a .Net library that will load and parse a page (like a browser) and spit out the generated code?  Clearly, Google and Yahoo aren't opening up browsers for every page they want to spider (of course they may have more resources than me...).  
Is there such a library or am I out of luck unless I'm willing to dissect the source code of an open source browser?
SOLUTION
Well, thank you everyone for you're help.  I have a working solution that is about 10X faster then Selenium. Woo!
Thanks to this old article from beansoftware I was able to use the System.Windows.Forms.WebBrowser control to download the page and parse it, then give em the generated source.  Even though the control is in Windows.Forms, you can still run it from Asp.Net (which is what I'm doing), just remember to add System.Window.Forms to your project references.
There are two notable things about the code.  First, the WebBrowser control is called in a new thread.  This is because it must run on a single threaded apartment.
Second, the GeneratedSource variable is set in two places.  This is not due to an intelligent design decision :)  I'm still working on it and will update this answer when I'm done.  wb_DocumentCompleted() is called multiple times.  First when the initial HTML is downloaded, then again when the first round of JavaScript completes.  Unfortunately, the site I'm scraping has 3 different loading stages.  1) Load initial HTML 2) Do first round of JavaScript DOM manipulation 3) pause for half a second then do a second round of JS DOM manipulation.
For some reason, the second round isn't cause by the wb_DocumentCompleted() function, but it is always caught when wb.ReadyState == Complete.  So why not remove it from wb_DocumentCompleted()? I'm still not sure why it isn't caught there and that's where the beadsoftware article recommended putting it.  I'm going to keep looking into it.  I just wanted to publish this code so anyone who's interested can use it.  Enjoy!
using System.Threading;
using System.Windows.Forms;

public class WebProcessor
{
    private string GeneratedSource{ get; set; }
    private string URL { get; set; }

    public string GetGeneratedHTML(string url)
    {
        URL = url;

        Thread t = new Thread(new ThreadStart(WebBrowserThread));
        t.SetApartmentState(ApartmentState.STA);
        t.Start();
        t.Join();

        return GeneratedSource;
    }

    private void WebBrowserThread()
    {
        WebBrowser wb = new WebBrowser();
        wb.Navigate(URL);

        wb.DocumentCompleted += 
            new WebBrowserDocumentCompletedEventHandler(
                wb_DocumentCompleted);

        while (wb.ReadyState != WebBrowserReadyState.Complete)
            Application.DoEvents();

        //Added this line, because the final HTML takes a while to show up
        GeneratedSource= wb.Document.Body.InnerHtml;

        wb.Dispose();
    }

    private void wb_DocumentCompleted(object sender, 
        WebBrowserDocumentCompletedEventArgs e)
    {
        WebBrowser wb = (WebBrowser)sender;
        GeneratedSource= wb.Document.Body.InnerHtml;
    }
}

",14k,"
            27
        ","['\nit is possibly using an instance of a browser (in you case: the ie control). you can easily use in your app and open a page. the control will then load it and process any javascript. once this is done you can access the controls dom object and get the ""interpreted"" code.\n', '\nBest way is using PhantomJs. That\'s Great. (sample of that is Article).\nMy solution is look like this:\nvar page = require(\'webpage\').create();\n\npage.open(""https://sample.com"", function(){\n    page.evaluate(function(){\n        var i = 0,\n        oJson = jsonData,\n        sKey;\n        localStorage.clear();\n\n        for (; sKey = Object.keys(oJson)[i]; i++) {\n            localStorage.setItem(sKey,oJson[sKey])\n        }\n    });\n\n    page.open(""https://sample.com"", function(){\n        setTimeout(function(){\n         page.render(""screenshoot.png"") \n            // Where you want to save it    \n           console.log(page.content); //page source\n            // You can access its content using jQuery\n            var fbcomments = page.evaluate(function(){\n                return $(""body"").contents().find("".content"") \n            }) \n            phantom.exit();\n        },10000)\n    });     \n});\n\n', ""\nTheoretically yes, but, at present, no.\nI don't think there is currently a product or OSS project that does this.  Such a product would need to have it's own javascript interpreter and be able to accurately emulate the run-time environment and quirks of every browser it supports.\nGiven that you need something that accurately emulates the server + browser environment in order to produce the final page code, in the long run, I think that using a browser instance is the best way to accurately generate the page in its final state. This is especially true, when you consider that, after the page load completes, the page sources can still change over time in the browser from AJAX/javascript.\n""]",https://stackoverflow.com/questions/1307800/view-generated-source-after-ajax-javascript-in-c-sharp,screen-scraping
How to use the WebClient.DownloadDataAsync() method in this context?,"
My plan is to have a user write down a movie title in my program and my program will pull the appropiate information asynchronously so the UI doesn't freeze up.
Here's the code:
public class IMDB
    {
        WebClient WebClientX = new WebClient();
        byte[] Buffer = null;


        public string[] SearchForMovie(string SearchParameter)
        {
            //Format the search parameter so it forms a valid IMDB *SEARCH* url.
            //From within the search website we're going to pull the actual movie
            //link.
            string sitesearchURL = FindURL(SearchParameter);

            //Have a method download asynchronously the ENTIRE source code of the
            //IMDB *search* website.
            Buffer = WebClientX.DownloadDataAsync(sitesearchURL);


            //Pass the IMDB source code to method findInformation().

            //string [] lol = findInformation();

            //????

            //Profit.

            string[] lol = null;
            return lol;
        }

My actual problem lies in the WebClientX.DownloadDataAsync() method. I can't use a string URL for it. How can I use that built in function to download the bytes of the site (for later use I will convert this to string, I know how to do this) and without freezing up my GUI?
Perhaps a clear cut example of the DownloadDataAsync so I can learn how to use it?
Thanks SO, you're always such a great resource.
",52k,"
            21
        ","[""\nThere is a newer DownloadDataTaskAsync method that allows you to await the result. It is simpler to read and easier to wire up by far. I'd use that...\nvar client = new WebClient();\n\nvar data = await client.DownloadDataTaskAsync(new Uri(imageUrl));\n\nawait outstream.WriteAsync(data, 0, data.Length);\n\n"", '\nYou need to handle the DownloadDataCompleted event:\nstatic void Main()\n{\n    string url = ""http://google.com"";\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += DownloadDataCompleted;\n    client.DownloadDataAsync(new Uri(url));\n    Console.ReadLine();\n}\n\nstatic void DownloadDataCompleted(object sender,\n    DownloadDataCompletedEventArgs e)\n{\n    byte[] raw = e.Result;\n    Console.WriteLine(raw.Length + "" bytes received"");\n}\n\nThe args contains other bits of information relating to error conditions etc - check those too.\nAlso note that you\'ll be coming into DownloadDataCompleted on a different thread; if you are in a UI (winform, wpf, etc) you\'ll need to get to the UI thread before updating the UI. From winforms, use this.Invoke. For WPF, look at the Dispatcher.\n', '\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n       delegate(object sender, DownloadDataCompletedEventArgs e)\n       {\n            data = e.Result;\n       };\n    Console.WriteLine(""starting..."");\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    while (client.IsBusy)\n    {\n         Console.WriteLine(""\\twaiting..."");\n         Thread.Sleep(100);\n    }\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n', '\nIf anyone using above in web application or websites please set Async = ""true"" in the page directive declaration in aspx file. \n', '\nThreadPool.QueueUserWorkItem(state => WebClientX.DownloadDataAsync(sitesearchURL));\n\nhttp://workblog.pilin.name/2009/02/system.html\n', '\n//using ManualResetEvent class\nstatic ManualResetEvent evnts = new ManualResetEvent(false);\nstatic void Main(string[] args)\n{\n    byte[] data = null;\n    WebClient client = new WebClient();\n    client.DownloadDataCompleted += \n        delegate(object sender, DownloadDataCompletedEventArgs e)\n        {\n             data = e.Result;\n             evnts.Set();\n        };\n    Console.WriteLine(""starting..."");\n    evnts.Reset();\n    client.DownloadDataAsync(new Uri(""http://stackoverflow.com/questions/""));\n    evnts.WaitOne(); // wait to download complete\n\n    Console.WriteLine(""done. {0} bytes received;"", data.Length);\n}\n\n']",https://stackoverflow.com/questions/1585985/how-to-use-the-webclient-downloaddataasync-method-in-this-context,screen-scraping
Text Extraction from HTML Java,"
I'm working on a program that downloads HTML pages and then selects some of the information and write it to another file.
I want to extract the information which is intbetween the paragraph tags, but i can only get one line of the paragraph. My code is as follows;
FileReader fileReader = new FileReader(file);
BufferedReader buffRd = new BufferedReader(fileReader);
BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));
String s;

while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        try {
            out.write(s);
        } catch (IOException e) {
        }
    }
}

i was trying to add another while loop, which would tell the program to keep writing to file until the line contains the </p> tag, by saying;
while ((s = br.readLine()) !=null) {
    if(s.contains(""<p>"")) {
        while(!s.contains(""</p>"") {
            try {
                out.write(s);
            } catch (IOException e) {
            }
        }
    }
}

But this doesn't work. Could someone please help.
",52k,"
            19
        ","['\njsoup\nAnother html parser I really liked using was jsoup. You could get all the <p> elements in 2 lines of code.\nDocument doc = Jsoup.connect(""http://en.wikipedia.org/"").get();\nElements ps = doc.select(""p"");\n\nThen write it out to a file in one more line\nout.write(ps.text());  //it will append all of the p elements together in one long string\n\nor if you want them on separate lines you can iterate through the elements and write them out separately. \n', '\njericho is one of several posible html parsers that could make this task both easy and safe.\n', '\nJTidy can represent an HTML document (even a malformed one) as a document model, making the process of extracting the contents of a <p> tag a rather more elegant process than manually thunking through the raw text.\n', '\nTry (if you don\'t want to use a HTML parser library):\n\n        FileReader fileReader = new FileReader(file);\n        BufferedReader buffRd = new BufferedReader(fileReader);\n        BufferedWriter out = new BufferedWriter(new FileWriter(newFile.txt));\n        String s;\n        int writeTo = 0;\n        while ((s = br.readLine()) !=null) \n        {\n                if(s.contains(""<p>""))\n                {\n                        writeTo = 1;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                if(s.contains(""</p>""))\n                {\n                        writeTo = 0;\n\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n                else if(writeTo==1)\n                {\n                        try \n                        {\n                            out.write(s);\n                    } \n                        catch (IOException e) \n                        {\n\n                    }\n                }\n}\n\n', ""\nI've had success using TagSoup & XPath to parse HTML.\nhttp://home.ccil.org/~cowan/XML/tagsoup/\n"", '\nUse a ParserCallback. Its a simple class thats included with the JDK. It notifies you every time a new tag is found and then you can extract the text of the tag. Simple example:\nimport java.io.*;\nimport java.net.*;\nimport javax.swing.text.*;\nimport javax.swing.text.html.*;\nimport javax.swing.text.html.parser.*;\n\npublic class ParserCallbackTest extends HTMLEditorKit.ParserCallback\n{\n    private int tabLevel = 1;\n    private int line = 1;\n\n    public void handleComment(char[] data, int pos)\n    {\n        displayData(new String(data));\n    }\n\n    public void handleEndOfLineString(String eol)\n    {\n        System.out.println( line++ );\n    }\n\n    public void handleEndTag(HTML.Tag tag, int pos)\n    {\n        tabLevel--;\n        displayData(""/"" + tag);\n    }\n\n    public void handleError(String errorMsg, int pos)\n    {\n        displayData(pos + "":"" + errorMsg);\n    }\n\n    public void handleMutableTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData(""mutable:"" + tag + "": "" + pos + "": "" + a);\n    }\n\n    public void handleSimpleTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + ""::"" + a );\n//      tabLevel++;\n    }\n\n    public void handleStartTag(HTML.Tag tag, MutableAttributeSet a, int pos)\n    {\n        displayData( tag + "":"" + a );\n        tabLevel++;\n    }\n\n    public void handleText(char[] data, int pos)\n    {\n        displayData( new String(data) );\n    }\n\n    private void displayData(String text)\n    {\n        for (int i = 0; i < tabLevel; i++)\n            System.out.print(""\\t"");\n\n        System.out.println(text);\n    }\n\n    public static void main(String[] args)\n    throws IOException\n    {\n        ParserCallbackTest parser = new ParserCallbackTest();\n\n        // args[0] is the file to parse\n\n        Reader reader = new FileReader(args[0]);\n//      URLConnection conn = new URL(args[0]).openConnection();\n//      Reader reader = new InputStreamReader(conn.getInputStream());\n\n        try\n        {\n            new ParserDelegator().parse(reader, parser, true);\n        }\n        catch (IOException e)\n        {\n            System.out.println(e);\n        }\n    }\n}\n\nSo all you need to do is set a boolean flag when the paragraph tag is found. Then in the handleText() method you extract the text.\n', '\nTry this.\n public static void main( String[] args )\n{\n    String url = ""http://en.wikipedia.org/wiki/Big_data"";\n\n    Document document;\n    try {\n        document = Jsoup.connect(url).get();\n        Elements paragraphs = document.select(""p"");\n\n        Element firstParagraph = paragraphs.first();\n        Element lastParagraph = paragraphs.last();\n        Element p;\n        int i=1;\n        p=firstParagraph;\n        System.out.println(""*  "" +p.text());\n        while (p!=lastParagraph){\n            p=paragraphs.get(i);\n            System.out.println(""*  "" +p.text());\n            i++;\n        } \n} catch (IOException e) {\n    // TODO Auto-generated catch block\n    e.printStackTrace();\n}\n}\n\n', '\nYou may just be using the wrong tool for the job:\nperl -ne ""print if m|<p>| .. m|</p>|"" infile.txt >outfile.txt\n\n']",https://stackoverflow.com/questions/1386107/text-extraction-from-html-java,screen-scraping
Screen Scraping a Javascript based webpage in Python,"
I am working on a screen scraping tool in Python. But, as I look through the source of the webpage, I noticed that most of the data is coming through Javascript. 
Any idea, how to scrape javascript based webpage ? Any tool  in Python ?
Thanks
",7k,"
            4
        ","['\nScraping javascript-based webpages is possible with selenium. In particular, try the Selenium WebDriver.\n', '\nI use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. \nAnd here is a full Python example to execute JavaScript and extract the final HTML.\n', '\nYou can use the QtWebKit module of the PyQt4 library\n']",https://stackoverflow.com/questions/8183682/screen-scraping-a-javascript-based-webpage-in-python,screen-scraping
Click on a javascript link within python?,"
I am navigating a site using python's mechanize module and having trouble clicking on a javascript link for next page.  I did a bit of reading and people suggested I need python-spidermonkey and DOMforms.  I managed to get them installed by I am not sure of the syntax to actually click on the link.
I can identify the code on the page as: 
<a href=""javascript:__doPostBack('ctl00$MainContent$gvSearchResults','Page$2')"">2</a>

Does anyone know how to click on it? or if perhaps there's another tool.
Thanks
",12k,"
            15
        ","['\nI mainly use HtmlUnit under jython for these use cases. Also I published a simple article on the subject: Web Scraping Ajax and Javascript sites.\n', ""\ninstead of struggling with python-spidermonkey try webkit's qt python bindings.\nHere is a full example to execute JavaScript and extract the final HTML.\n"", ""\nhow about calling __doPostBack('ctl00$MainContent$gvSearchResults','Page$'+pageid); (javascript method, via python-spidermonkey)\n""]",https://stackoverflow.com/questions/5207948/click-on-a-javascript-link-within-python,screen-scraping
How can I use Perl to grab text from a web page that is dynamically generated with JavaScript?,"
There is a website I am trying to pull information from in Perl, however the section of the page I need is being generated using javascript so all you see in the source is:
<div id=""results""></div>

I need to somehow pull out the contents of that div and save it to a file using Perl/proxies/whatever. e.g. the information I want to save would be
document.getElementById('results').innerHTML;

I am not sure if this is possible or if anyone had any ideas or a way to do this.
I was using a lynx source dump for other pages but since I cant straight forward screen scrape this page I came here to ask about it!
If anyone is interested, the page is http://downloadcenter.trendmicro.com/index.php?clk=left_nav&clkval=pattern_file&regs=NABU and the info I am trying to get is the row about the ConsumerOPR
",6k,"
            12
        ","[""\nYou'll need to reverse-engineer what the Javascript is doing. Does it fire off an AJAX request to populate the <div>? If so, it should be pretty easy to sniff the request using Firebug and then duplicate it with LWP::UserAgent or WWW::Mechanize to get the information.\nIf the Javascript is just doing pure DOM manipulation, then that means the data must exist somewhere else in the page or the Javascript already. So figure out where it's coming from and grab it.\nFinally, if none of those options are adequate, you may need to just use a real browser to do it. There are a few options for automating browser behavior, like WWW::Mechanize::Firefox or Win32::IE::Mechanize.\n"", '\nBringing the Browser to the Server by John Resig might be useful.\n', ""\nAs the content of your page is generated by some Javascript, you need the ability to :\n\nExecute some Javascript code\n\n\nEven, possibly, some complex JS code, doing Ajax requests and all that ?\n\nAnd do it with an engine that supports the functions/methods that are present in a browser (like DOM manipulations)\n\n\nA solution could be to actually really start a browser to navigate to that page, and, then, parse the page that's loaded by it, to extract the information ?\nI've never used this for grabbing, but the Selenium suite might help, here : using Selenium RC, you can start a real browser, and pilot it -- then, you have functions to get data from it.\nIt's not quite fast, and it's pretty heavy (it has to start a browser !), but it works quite well : you'll be using Firefox, for example, to navigate to your page -- which means a real Javascript engine, that's used every day by a lot of people ;-)\n"", ""\nThis might be what your looking for (in PHP):\n$url = 'http://downloadcenter.trendmicro.com/ajx/pattern_result.php';\n\n$ch = curl_init();\ncurl_setopt ($ch, CURLOPT_SSL_VERIFYPEER, FALSE);\ncurl_setopt ($ch, CURLOPT_URL, $url);\ncurl_setopt ($ch, CURLOPT_POST, 1);\ncurl_setopt ($ch, CURLOPT_POSTFIELDS, 'q=patresult_page&reg=NABU');\ncurl_setopt ($ch, CURLOPT_RETURNTRANSFER, 1);\n$content = curl_exec($ch);\ncurl_close($ch);\n\necho $content;\nexit;\n\nonce you get the content you can use something like: http://code.google.com/p/phpquery/ to parse the results you need or a similar perl equivalent??? \nAnd/or do the parsing yourself.\nFYI: all I did was use firebug to inspect the requests and recreated it with PHP/CURL...\n"", '\nto work with the dynamically created HTML you can use the FireFox Chickenfoot plugin.\nOr if you need something that works from a command line script use bindings to Perl. I have done this with Python before.\n']",https://stackoverflow.com/questions/2655034/how-can-i-use-perl-to-grab-text-from-a-web-page-that-is-dynamically-generated-wi,screen-scraping
How can i grab CData out of BeautifulSoup,"
I have a website that I'm scraping that has a similar structure the following. I'd like to be able to grab the info out of the CData block. 
I'm using BeautifulSoup to pull other info off the page, so if the solution can work with that, it would help keep my learning curve down as I'm a python novice.
Specifically, I want to get at the two different types of data hidden in the CData statement. the first which is just text I'm pretty sure I can throw a regex at it and get what I need. For the second type, if i could drop the data that has html elements into it's own beautifulsoup, I can parse that. 
I'm just learning python and beautifulsoup, so I'm struggling to find the magical incantation that will give me just the CData by itself.
<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN""   ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"">
<html xmlns=""http://www.w3.org/1999/xhtml"">  
<head>  
<title>
   Cows and Sheep
  </title>
</head>
<body>
 <div id=""main"">
  <div id=""main-precontents"">
   <div id=""main-contents"" class=""main-contents"">
    <script type=""text/javascript"">
       //<![CDATA[var _ = g_cow;_[7654]={cowname_enus:'cows rule!',leather_quality:99,icon:'cow_level_23'};_[37357]={sheepname_enus:'baa breath',wool_quality:75,icon:'sheep_level_23'};_[39654].cowmeat_enus = '<table><tr><td><b class=""q4"">cows rule!</b><br></br>
       <!--ts-->
       get it now<table width=""100%""><tr><td>NOW</td><th>NOW</th></tr></table><span>244 Cows</span><br></br>67 leather<br></br>68 Brains
       <!--yy-->
       <span class=""q0"">Cow Bonus: +9 Cow Power</span><br></br>Sheep Power 60 / 60<br></br>Sheep 88<br></br>Cow Level 555</td></tr></table>
       <!--?5695:5:40:45-->
       ';
        //]]>
      </script>
     </div>
     </div>
    </div>
 </body>
</html>

",15k,"
            12
        ","['\nOne thing you need to be careful of BeautifulSoup grabbing CData is not to use a lxml parser.\nBy default, the lxml parser will strip CDATA sections from the tree and replace them by their plain text content, Learn more here\n#Trying it with html.parser\n\n\n>>> from bs4 import BeautifulSoup\n>>> import bs4\n>>> s=\'\'\'<?xml version=""1.0"" ?>\n<foo>\n    <bar><![CDATA[\n        aaaaaaaaaaaaa\n    ]]></bar>\n</foo>\'\'\'\n>>> soup = BeautifulSoup(s, ""html.parser"")\n>>> soup.find(text=lambda tag: isinstance(tag, bs4.CData)).string.strip()\n\'aaaaaaaaaaaaa\'\n>>> \n\n', '\nBeautifulSoup sees CData as a special case (subclass) of ""navigable strings"". So for example:\nimport BeautifulSoup\n\ntxt = \'\'\'<foobar>We have\n       <![CDATA[some data here]]>\n       and more.\n       </foobar>\'\'\'\n\nsoup = BeautifulSoup.BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, BeautifulSoup.CData):\n    print \'CData contents: %r\' % cd\n\nIn your case of course you could look in the subtree starting at the div with the \'main-contents\' ID, rather than all over the document tree.\n', '\nYou could try this:\nfrom BeautifulSoup import BeautifulSoup\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ns = soup.findAll(\'script\')\ncdata = s[0].contents[0]\n\nThat should give you the contents of cdata.\nUpdate\nThis may be a little cleaner:\nfrom BeautifulSoup import BeautifulSoup\nimport re\n\n// source.html contains your html above\nf = open(\'source.html\')\nsoup = BeautifulSoup(\'\'.join(f.readlines()))\ncdata = soup.find(text=re.compile(""CDATA""))\n\nJust personal preference, but I like the bottom one a little better.\n', ""\nimport re\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(content)\nfor x in soup.find_all('item'):\n    print re.sub('[\\[CDATA\\]]', '', x.string)\n\n"", ""\nFor anyone using BeautifulSoup4, Alex Martelli's solution works but do this:\nfrom bs4 import BeautifulSoup, CData\n\nsoup = BeautifulSoup(txt)\nfor cd in soup.findAll(text=True):\n  if isinstance(cd, Cdata):\n    print 'CData contents: %r' % cd\n\n""]",https://stackoverflow.com/questions/2032172/how-can-i-grab-cdata-out-of-beautifulsoup,screen-scraping
php - Fastest way to check presence of text in many domains (above 1000),"
I have a php script running and using cURL to retrieve the content of webpages on which I would like to check for the presence of some text.
Right now it looks like this:
for( $i = 0; $i < $num_target; $i++ ) {
    $ch = curl_init();
    $timeout = 10;
    curl_setopt ($ch, CURLOPT_URL,$target[$i]);
    curl_setopt ($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt ($ch, CURLOPT_FORBID_REUSE, true);
    curl_setopt ($ch, CURLOPT_CONNECTTIMEOUT, $timeout);
    $url = curl_exec ($ch);
    curl_close($ch);

    if (preg_match($text,$url,$match)) {
        $match[$i] = $match;
        echo ""text"" . $text . "" found in URL: "" . $url . "": "" . $match .;

        } else {
        $match[$i] = $match;
        echo ""text"" . $text . "" not found in URL: "" . $url . "": no match"";
        }
}

I was wondering if I could use a special cURL setup that makes it faster ( I looked in the php manual chose the options that seemed the best to me but I may have neglected some that could increase the speed and performance of the script).
I was then wondering if using cgi, Perl or python (or another solution) could be faster than php.
Thank you in advance for any help / advice / suggestion.
",953,"
            -2
        ","['\nYou can use curl_multi_init .... which Allows the processing of multiple cURL handles in parallel.\nExample \n$url = array();\n$url[] = \'http://www.huffingtonpost.com\';\n$url[] = \'http://www.yahoo.com\';\n$url[] = \'http://www.google.com\';\n$url[] = \'http://technet.microsoft.com/en-us/\';\n\n$start = microtime(true);\necho ""<pre>"";\nprint_r(checkLinks($url, ""Azure""));\necho ""<h1>"", microtime(true) - $start, ""</h1>"";\n\nOutput\nArray\n(\n    [0] => http://technet.microsoft.com/en-us/\n)\n\n1.2735739707947 <-- Faster\n\nFunction Used\nfunction checkLinks($nodes, $text) {\n    $mh = curl_multi_init();\n    $curl_array = array();\n    foreach ( $nodes as $i => $url ) {\n        $curl_array[$i] = curl_init($url);\n        curl_setopt($curl_array[$i], CURLOPT_RETURNTRANSFER, true);\n        curl_setopt($curl_array[$i], CURLOPT_USERAGENT, \'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.2) Gecko/20090729 Firefox/3.5.2 (.NET CLR 3.5.30729)\');\n        curl_setopt($curl_array[$i], CURLOPT_CONNECTTIMEOUT, 5);\n        curl_setopt($curl_array[$i], CURLOPT_TIMEOUT, 15);\n        curl_multi_add_handle($mh, $curl_array[$i]);\n    }\n    $running = NULL;\n    do {\n        usleep(10000);\n        curl_multi_exec($mh, $running);\n    } while ( $running > 0 );\n    $res = array();\n    foreach ( $nodes as $i => $url ) {\n        $curlErrorCode = curl_errno($curl_array[$i]);\n        if ($curlErrorCode === 0) {\n            $info = curl_getinfo($curl_array[$i]);\n            if ($info[\'http_code\'] == 200) {\n                if (stripos(curl_multi_getcontent($curl_array[$i]), $text) !== false) {\n                    $res[] = $info[\'url\'];\n                }\n            }\n        }\n        curl_multi_remove_handle($mh, $curl_array[$i]);\n        curl_close($curl_array[$i]);\n    }\n    curl_multi_close($mh);\n    return $res;\n}\n\n']",https://stackoverflow.com/questions/12891689/php-fastest-way-to-check-presence-of-text-in-many-domains-above-1000,screen-scraping
What's the best way of scraping data from a website? [closed],"






Closed. This question is opinion-based. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 years ago.







                        Improve this question
                    



I need to extract contents from a website, but the application doesn’t provide any application programming interface or another mechanism to access that data programmatically.
I found a useful third-party tool called Import.io that provides click and go functionality for scraping web pages and building data sets, the only thing is I want to keep my data locally and I don't want to subscribe to any subscription plans.
What kind of technique does this company use for scraping the web pages and building their datasets? I found some web scraping frameworks pjscrape & Scrapy could they provide such a feature
",167k,"
            114
        ","['\nYou will definitely want to start with a good web scraping framework. Later on you may decide that they are too limiting and you can put together your own stack of libraries but without a lot of scraping experience your design will be much worse than pjscrape or scrapy.\nNote: I use the terms crawling and scraping basically interchangeable here. This is a copy of my answer to your Quora question, it\'s pretty long.\nTools\nGet very familiar with either Firebug or Chrome dev tools depending on your preferred browser. This will be absolutely necessary as you browse the site you are pulling data from and map out which urls contain the data you are looking for and what data formats make up the responses.\nYou will need a good working knowledge of HTTP as well as HTML and will probably want to find a decent piece of man in the middle proxy software. You will need to be able to inspect HTTP requests and responses and understand how the cookies and session information and query parameters are being passed around. Fiddler (http://www.telerik.com/fiddler) and Charles Proxy (http://www.charlesproxy.com/) are popular tools. I use mitmproxy (http://mitmproxy.org/) a lot as I\'m more of a keyboard guy than a mouse guy.\nSome kind of console/shell/REPL type environment where you can try out various pieces of code with instant feedback will be invaluable. Reverse engineering tasks like this are a lot of trial and error so you will want a workflow that makes this easy.\nLanguage\nPHP is basically out, it\'s not well suited for this task and the library/framework support is poor in this area. Python (Scrapy is a great starting point) and Clojure/Clojurescript (incredibly powerful and productive but a big learning curve) are great languages for this problem. Since you would rather not learn a new language and you already know Javascript I would definitely suggest sticking with JS. I have not used pjscrape but it looks quite good from a quick read of their docs. It\'s well suited and implements an excellent solution to the problem I describe below.\nA note on Regular expressions:\nDO NOT USE REGULAR EXPRESSIONS TO PARSE HTML.\nA lot of beginners do this because they are already familiar with regexes. It\'s a huge mistake, use xpath or css selectors to navigate html and only use regular expressions to extract data from actual text inside an html node. This might already be obvious to you, it becomes obvious quickly if you try it but a lot of people waste a lot of time going down this road for some reason. Don\'t be scared of xpath or css selectors, they are WAY easier to learn than regexes and they were designed to solve this exact problem.\nJavascript-heavy sites\nIn the old days you just had to make an http request and parse the HTML reponse. Now you will almost certainly have to deal with sites that are a mix of standard HTML HTTP request/responses and asynchronous HTTP calls made by the javascript portion of the target site. This is where your proxy software and the network tab of firebug/devtools comes in very handy. The responses to these might be html or they might be json, in rare cases they will be xml or something else.\nThere are two approaches to this problem:\nThe low level approach:\nYou can figure out what ajax urls the site javascript is calling and what those responses look like and make those same requests yourself. So you might pull the html from http://example.com/foobar and extract one piece of data and then have to pull the json response from http://example.com/api/baz?foo=b... to get the other piece of data. You\'ll need to be aware of passing the correct cookies or session parameters. It\'s very rare, but occasionally some required parameters for an ajax call will be the result of some crazy calculation done in the site\'s javascript, reverse engineering this can be annoying.\nThe embedded browser approach:\nWhy do you need to work out what data is in html and what data comes in from an ajax call? Managing all that session and cookie data? You don\'t have to when you browse a site, the browser and the site javascript do that. That\'s the whole point.\nIf you just load the page into a headless browser engine like phantomjs it will load the page, run the javascript and tell you when all the ajax calls have completed. You can inject your own javascript if necessary to trigger the appropriate clicks or whatever is necessary to trigger the site javascript to load the appropriate data.\nYou now have two options, get it to spit out the finished html and parse it or inject some javascript into the page that does your parsing and data formatting and spits the data out (probably in json format). You can freely mix these two options as well.\nWhich approach is best?\nThat depends, you will need to be familiar and comfortable with the low level approach for sure. The embedded browser approach works for anything, it will be much easier to implement and will make some of the trickiest problems in scraping disappear. It\'s also quite a complex piece of machinery that you will need to understand. It\'s not just HTTP requests and responses, it\'s requests, embedded browser rendering, site javascript, injected javascript, your own code and 2-way interaction with the embedded browser process.\nThe embedded browser is also much slower at scale because of the rendering overhead but that will almost certainly not matter unless you are scraping a lot of different domains. Your need to rate limit your requests will make the rendering time completely negligible in the case of a single domain.\nRate Limiting/Bot behaviour\nYou need to be very aware of this. You need to make requests to your target domains at a reasonable rate. You need to write a well behaved bot when crawling websites, and that means respecting robots.txt and not hammering the server with requests. Mistakes or negligence here is very unethical since this can be considered a denial of service attack. The acceptable rate varies depending on who you ask, 1req/s is the max that the Google crawler runs at but you are not Google and you probably aren\'t as welcome as Google. Keep it as slow as reasonable. I would suggest 2-5 seconds between each page request.\nIdentify your requests with a user agent string that identifies your bot and have a webpage for your bot explaining it\'s purpose. This url goes in the agent string.\nYou will be easy to block if the site wants to block you. A smart engineer on their end can easily identify bots and a few minutes of work on their end can cause weeks of work changing your scraping code on your end or just make it impossible. If the relationship is antagonistic then a smart engineer at the target site can completely stymie a genius engineer writing a crawler. Scraping code is inherently fragile and this is easily exploited. Something that would provoke this response is almost certainly unethical anyway, so write a well behaved bot and don\'t worry about this.\nTesting\nNot a unit/integration test person? Too bad. You will now have to become one. Sites change frequently and you will be changing your code frequently. This is a large part of the challenge.\nThere are a lot of moving parts involved in scraping a modern website, good test practices will help a lot. Many of the bugs you will encounter while writing this type of code will be the type that just return corrupted data silently. Without good tests to check for regressions you will find out that you\'ve been saving useless corrupted data to your database for a while without noticing. This project will make you very familiar with data validation (find some good libraries to use) and testing. There are not many other problems that combine requiring comprehensive tests and being very difficult to test.\nThe second part of your tests involve caching and change detection. While writing your code you don\'t want to be hammering the server for the same page over and over again for no reason. While running your unit tests you want to know if your tests are failing because you broke your code or because the website has been redesigned. Run your unit tests against a cached copy of the urls involved. A caching proxy is very useful here but tricky to configure and use properly.\nYou also do want to know if the site has changed. If they redesigned the site and your crawler is broken your unit tests will still pass because they are running against a cached copy! You will need either another, smaller set of integration tests that are run infrequently against the live site or good logging and error detection in your crawling code that logs the exact issues, alerts you to the problem and stops crawling. Now you can update your cache, run your unit tests and see what you need to change.\nLegal Issues\nThe law here can be slightly dangerous if you do stupid things. If the law gets involved you are dealing with people who regularly refer to wget and curl as ""hacking tools"". You don\'t want this.\nThe ethical reality of the situation is that there is no difference between using browser software to request a url and look at some data and using your own software to request a url and look at some data. Google is the largest scraping company in the world and they are loved for it. Identifying your bots name in the user agent and being open about the goals and intentions of your web crawler will help here as the law understands what Google is. If you are doing anything shady, like creating fake user accounts or accessing areas of the site that you shouldn\'t (either ""blocked"" by robots.txt or because of some kind of authorization exploit) then be aware that you are doing something unethical and the law\'s ignorance of technology will be extraordinarily dangerous here. It\'s a ridiculous situation but it\'s a real one.\nIt\'s literally possible to try and build a new search engine on the up and up as an upstanding citizen, make a mistake or have a bug in your software and be seen as a hacker. Not something you want considering the current political reality.\nWho am I to write this giant wall of text anyway?\nI\'ve written a lot of web crawling related code in my life. I\'ve been doing web related software development for more than a decade as a consultant, employee and startup founder. The early days were writing perl crawlers/scrapers and php websites. When we were embedding hidden iframes loading csv data into webpages to do ajax before Jesse James Garrett named it ajax, before XMLHTTPRequest was an idea. Before jQuery, before json. I\'m in my mid-30\'s, that\'s apparently considered ancient for this business.\nI\'ve written large scale crawling/scraping systems twice, once for a large team at a media company (in Perl) and recently for a small team as the CTO of a search engine startup (in Python/Javascript). I currently work as a consultant, mostly coding in Clojure/Clojurescript (a wonderful expert language in general and has libraries that make crawler/scraper problems a delight)\nI\'ve written successful anti-crawling software systems as well. It\'s remarkably easy to write nigh-unscrapable sites if you want to or to identify and sabotage bots you don\'t like.\nI like writing crawlers, scrapers and parsers more than any other type of software. It\'s challenging, fun and can be used to create amazing things.\n', '\nYes you can do it yourself. It is just a matter of grabbing the sources of the page and parsing them the way you want. \nThere are various possibilities. A good combo is using python-requests (built on top of urllib2, it is urllib.request in Python3) and BeautifulSoup4, which has its methods to select elements and also permits CSS selectors:\nimport requests\nfrom BeautifulSoup4 import BeautifulSoup as bs\nrequest = requests.get(""http://foo.bar"")\nsoup = bs(request.text) \nsome_elements = soup.find_all(""div"", class_=""myCssClass"")\n\nSome will prefer xpath parsing or jquery-like pyquery, lxml or something else.\nWhen the data you want is produced by some JavaScript, the above won\'t work. You either need python-ghost or Selenium. I prefer the latter combined with PhantomJS, much lighter and simpler to install, and easy to use:\nfrom selenium import webdriver\nclient = webdriver.PhantomJS()\nclient.get(""http://foo"")\nsoup = bs(client.page_source)\n\nI would advice to start your own solution. You\'ll understand Scrapy\'s benefits doing so.\nps: take a look at scrapely: https://github.com/scrapy/scrapely\npps: take a look at Portia, to start extracting information visually, without programming knowledge: https://github.com/scrapinghub/portia \n']",https://stackoverflow.com/questions/22168883/whats-the-best-way-of-scraping-data-from-a-website,screen-scraping
"Headless, scriptable Firefox/Webkit on linux? [closed]","






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 3 years ago.







                        Improve this question
                    



I'm looking to automate some web interactions, namely periodic download of files from a secure website. This basically involves entering my username/password and navigating to the appropriate URL.
I tried simple scripting in Python, followed by more sophisticated scripting, only to discover this particular website is using some obnoxious javascript and flash based mechanism for login, rendering my methods useless. 
I then tried HTMLUnit, but that doesn't seem to want to work either. I suspect use of Flash is the issue.
I don't really want to think about it any more, so I'm leaning towards scripting an actual browser to log in and grab the file I need. 
Requirements are:

Run on linux server (ie. no X running). If I really need to have X I can make that happen, but I won't be happy.
Be reliable. I want to start this thing and never think about it again.
Be scriptable. Nothing too sophisticated, but I should be able to tell the browser the various steps to take and pages to visit.

Are there any good toolkits for a headless, X-less scriptable browser? Have you tried something like this and if so do you have any words of wisdom?
",22k,"
            46
        ","['\nWhat about phantomjs?  \n', '\nI did related task with IE embedded browser (although it was gui application with hidden browser component panel). Actually you can take any layout engine and cut output logic. Navigation is should be done via firing script-like events.\nYou can use Crowbar. It is headless version of firefox (Gecko engine). It turns browser into RESTful server that can accept requests (""fetch  url""). So it parse html, represent it as DOM, wait defined delay for all script performed. \nIt works on linux. I suppose you can easily extend it for your goal using JS and rich XULrunner abilities.\n', '\nHave you tried Selenium? It will allow you to record a usage scenario, using an extension for Firefox, which can later be played back using a number of different methods.\nEdit: I just realized this was a very late response. :)\n', '\nHave a look at WebKitDriver. The project includes headless implementation of WebKit.\n', ""\nI don't know how to do flash interactions (and am also interested), but for html/javascript you can use Chickenfoot. \nAnd to get a headless + scriptable browser working on Linux you can use the Qt webkit library. Here is an example use.\n"", ""\nTo accomplish this, I just write Chrome extensions that post to CouchDBs (example and its Futon). Add the Couch to the permissions in the manifest to allow cross-domain XHRs.\n(I arrived at this thread in search of a headless alternative to what I've been doing; having found this thread, I'm going to try Crowbar at some point.)\nAlso, considering the bizarre characteristics of this website, I can't help wondering whether you can exploit some security hole to get around the Flash and Javascript.\n""]",https://stackoverflow.com/questions/2073481/headless-scriptable-firefox-webkit-on-linux,screen-scraping
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            41
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]",https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent,screen-scraping
How to fetch HTML in Java,"
Without the use of any external library, what is the simplest way to fetch a website's HTML content into a String?
",72k,"
            33
        ","['\nI\'m currently using this:\nString content = null;\nURLConnection connection = null;\ntry {\n  connection =  new URL(""http://www.google.com"").openConnection();\n  Scanner scanner = new Scanner(connection.getInputStream());\n  scanner.useDelimiter(""\\\\Z"");\n  content = scanner.next();\n  scanner.close();\n}catch ( Exception ex ) {\n    ex.printStackTrace();\n}\nSystem.out.println(content);\n\nBut not sure if there\'s a better way.\n', '\nThis has worked well for me:\nURL url = new URL(theURL);\nInputStream is = url.openStream();\nint ptr = 0;\nStringBuffer buffer = new StringBuffer();\nwhile ((ptr = is.read()) != -1) {\n    buffer.append((char)ptr);\n}\n\nNot sure at to whether the other solution(s) provided are any more efficient or not.\n', ""\nI just left this post in your other thread, though what you have above might work as well.  I don't think either would be any easier than the other.  The Apache packages can be accessed by just using import org.apache.commons.HttpClient at the top of your code.\nEdit: Forgot the link ;)\n"", '\nWhilst not vanilla-Java, I\'ll offer up a simpler solution. Use Groovy ;-)\nString siteContent = new URL(""http://www.google.com"").text\n\n', '\nIts not library but a tool named curl generally installed in most of the servers or you can easily install in ubuntu by \nsudo apt install curl\n\nThen fetch any html page and store it to your local file like an example \ncurl https://www.facebook.com/ > fb.html\n\nYou will get the home page html.You can run it in your browser as well.\n']",https://stackoverflow.com/questions/31462/how-to-fetch-html-in-java,screen-scraping
Scraping Real Time Visitors from Google Analytics,"
I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this information is to open a new tab for each site.
Google doesn't have a real-time API, so I'm wondering if it is possible to scrape this data. Eduardo Cereto found out that Google transfers the real-time data over the realtime/bind network request. Anyone more savvy have an idea of how I should start? Here's what I'm thinking:

Figure out how to authenticate programmatically

Inspect all of the realtime/bind requests to see how they change. Does each request have a unique key? Where does that come from? Below is my breakdown of the request:
https://www.google.com/analytics/realtime/bind?VER=8
&key= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&ds= [What is this? Where does it come from? 21 character lowercase alphanumeric, stays the same each request]
&pageId=rt-standard%2Frt-overview
&q=t%3A0%7C%3A1%3A0%3A%2Ct%3A11%7C%3A1%3A5%3A%2Cot%3A0%3A0%3A4%2Cot%3A0%3A0%3A3%2Ct%3A7%7C%3A1%3A10%3A6%3D%3DREFERRAL%3B%2Ct%3A10%7C%3A1%3A10%3A%2Ct%3A18%7C%3A1%3A10%3A%2Ct%3A4%7C5%7C2%7C%3A1%3A10%3A2!%3Dzz%3B%2C&f
The q variable URI decodes to this (what the?):
t:0|:1:0:,t:11|:1:5:,ot:0:0:4,ot:0:0:3,t:7|:1:10:6==REFERRAL;,t:10|:1:10:,t:18|:1:10:,t:4|5|2|:1:10:2!=zz;,&f
&RID=rpc
&SID= [What is this? Where does it come from? 16 character uppercase alphanumeric, stays the same each request]
&CI=0
&AID= [What is this? Where does it come from? integer, starts at 1, increments weirdly to 150 and then 298]
&TYPE=xmlhttp
&zx= [What is this? Where does it come from? 12 character lowercase alphanumeric, changes each request]
&t=1

Inspect all of the realtime/bind responses to see how they change. How does the data come in? It looks like some altered JSON. How many times do I need to connect to get the data? Where is the active visitors on site number in there? Here is a dump of sample data:


19
[[151,[""noop""]
]
]
388
[[152,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[49,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2,0],""name"":""Total""}]}}]]]
]
388
[[153,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[52,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[2,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3,2],""name"":""Total""}]}}]]]
]
388
[[154,[""rt"",[{""ot:0:0:4"":{""timeUnit"":""MINUTES"",""overTimeData"":[{""values"":[53,53,52,40,42,55,49,41,51,52,47,42,62,82,76,71,81,66,81,86,71,66,65,65,55,51,53,73,71,81],""name"":""Total""}]},""ot:0:0:3"":{""timeUnit"":""SECONDS"",""overTimeData"":[{""values"":[0,3,1,1,1,1,1,0,1,0,1,1,1,0,2,0,2,2,1,0,0,0,0,0,2,1,1,2,1,2,0,5,1,0,2,1,1,1,2,0,2,1,0,5,1,1,2,0,0,0,0,0,0,0,0,0,1,1,0,3],""name"":""Total""}]}}]]]
]

Let me know if you can help with any of the items above!

",18k,"
            27
        ","['\nTo get the same, Google has  launched new Real Time API. With this API you can easily retrieve real time online visitors as well as several Google Analytics with following dimensions and metrics. https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/\nThis is quite similar to Google Analytics API. To start development on this, \nhttps://developers.google.com/analytics/devguides/reporting/realtime/v3/devguide \n', '\nWith Google Chrome I can see the data on the Network Panel.\nThe request endpoint is https://www.google.com/analytics/realtime/bind\nSeems like the connection stays open for 2.5 minutes, and during this time it just keeps getting more and more data. \nAfter about 2.5 minutes the connection is closed and a new one is open.\nOn the Network panel you can only see the data for the connections that are terminated. So leave it open for 5 minutes or so and you can start to see the data.\nI hope that can give you a place to start.\n', '\nHaving google in the loop seems pretty redundant. Suggest you use a common element delivered on demand from the dashboard server and include this item by absolute URL on all pages to be monitored for a given site. The script outputting the item can read the IP of the browser asking and these can all be logged into a database and filtered for uniqueness giving a real time head count.\n<?php\n$user_ip = $_SERVER[""REMOTE_ADDR""];\n/// Some MySQL to insert $user_ip to the database table for website XXX  goes here\n\n\n$file = \'tracking_image.gif\';\n$type = \'image/gif\';\nheader(\'Content-Type:\'.$type);\nheader(\'Content-Length: \' . filesize($file));\nreadfile($file);\n?>\n\nAmmendum:\nA database can also add a timestamp to every row of data it stores. This can be used to further filter results and provide the number of visitors in the last hour or minute. \nClient side Javascript with AJAX for fine tuning or overkill\nThe onblur and onfocus javascript commands  can be used to tell if the the page is visible, pass the data back to the dashboard server via Ajax. http://www.thefutureoftheweb.com/demo/2007-05-16-detect-browser-window-focus/\nWhen a visitor closes a page this can also be detected by the javascript onunload function in the body tag and Ajax can be used to send data back to the server one last time before the browser finally closes the page.\nAs you may also wish to collect some information about the visitor like Google analytics does this page https://panopticlick.eff.org/ has a lot of javascript that can be examined and adapted.\n', '\nI needed/wanted realtime data for personal use so I reverse-engineered their system a little bit.\nInstead of binding to /bind I get data from /getData (no pun intended).\nAt /getData the minimum request is apparently: https://www.google.com/analytics/realtime/realtime/getData?pageId&key={{propertyID}}&q=t:0|:1\nHere\'s a short explanation of the possible query parameters and syntax, please remember that these are all guesses and I don\'t know all of them:\nQuery Syntax: pageId&key=propertyID&q=dataType:dimensions|:page|:limit:filters\nValues: \npageID: Required but seems to only be used for internal analytics.\n\npropertyID: a{{accountID}}w{{webPropertyID}}p{{profileID}}, as specified at the Documentation link below. You can also find this in the URL of all analytics pages in the UI.\n\n\ndataType:\n    t: Current data\n    ot: Overtime/Past\n    c: Unknown, returns only a ""count"" value\n\n\ndimensions (| separated or alone), most values are only applicable for t:\n    1:  Country\n    2:  City\n    3:  Location code?\n    4:  Latitude\n    5:  Longitude\n    6:  Traffic source type (Social, Referral, etc.)\n    7:  Source\n    8:  ?? Returns (not set)\n    9:  Another location code? longer.\n    10: Page URL\n    11: Visitor Type (new/returning)\n    12: ?? Returns (not set)\n    13: ?? Returns (not set)\n    14: Medium\n    15: ?? Returns ""1""\n\npage:\n    At first this seems to work for pagination but after further analysis it looks like it\'s also used to specify which of the 6 pages (Overview, Locations, Traffic Sources, Content, Events and Conversions) to return data for.\n\n    For some reason 0 returns an impossibly high metrictotal\n\nlimit: Result limit per page, maximum of 50\n\nfilters:\n    Syntax is as specified at the Documentation 2 link below except the OR is specified using | instead of a comma.6==CUSTOM;1==United%20States\n\n\nYou can also combine multiple queries in one request by comma separating them (i.e. q=t:1|2|:1|:10,t:6|:1|:10).\nFollowing the above ""documentation"", if you wanted to build a query that requests the page URL and city of the top 10 active visitors with a traffic source type of CUSTOM located in the US you would use this URL: https://www.google.com/analytics/realtime/realtime/getData?key={{propertyID}}&pageId&q=t:10|2|:1|:10:6==CUSTOM;1==United%20States\n\nDocumentation\nDocumentation 2\n\nI hope that my answer is readable and (although it\'s a little late) sufficiently answers your question and helps others in the future.\n']",https://stackoverflow.com/questions/11021554/scraping-real-time-visitors-from-google-analytics,screen-scraping
Is there a simple way in R to extract only the text elements of an HTML page?,"
Is there a simple way in R to extract only the text elements of an HTML page?
I think this is known as 'screen scraping' but I have no experience of it, I just need a simple way of extracting the text you'd normally see in a browser when visiting a url.
",27k,"
            26
        ","['\nI had to do this once upon time myself. \nOne way of doing it is to make use of XPath expressions. You will need these packages installed from the repository at http://www.omegahat.org/\nlibrary(RCurl)\nlibrary(RTidyHTML)\nlibrary(XML)\n\nWe use RCurl to connect to the website of interest. It has lots of options which allow you to access websites that the default functions in base R would have difficulty with I think it\'s fair to say. It is an R-interface to the libcurl library.\nWe use RTidyHTML to clean up malformed HTML web pages so that they are easier to parse. It is an R-interface to the libtidy library.\nWe use XML to parse the HTML code with our XPath expressions. It is an R-interface to the libxml2 library.\nAnyways, here\'s what you do (minimal code, but options are available, see help pages of corresponding functions):\nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r"" \ndoc.raw <- getURL(u)\ndoc <- tidyHTML(doc.raw)\nhtml <- htmlTreeParse(doc, useInternal = TRUE)\ntxt <- xpathApply(html, ""//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]"", xmlValue)\ncat(unlist(txt))\n\nThere may be some problems with this approach, but I can\'t remember what they are off the top of my head (I don\'t think my xpath expression works with all web pages, sometimes it might not filter out script code or it may plain just not work with some other pages at all, best to experiment!)\nP.S. Another way, which works almost perfectly I think at web scraping all text from html is the following (basically getting Internet Explorer to do the conversion for you):\nlibrary(RDCOMClient) \nu <- ""http://stackoverflow.com/questions/tagged?tagnames=r""\nie <- COMCreate(""InternetExplorer.Application"") \nie$Navigate(u)\ntxt <- list()\ntxt[[u]] <- ie[[""document""]][[""body""]][[""innerText""]] \nie$Quit() \nprint(txt) \n\nHOWEVER, I\'ve never liked doing this because not only is it slow, but if you vectorise it and apply a vector of URLs, if internet explorer crashes on a bad page, then R might hang or crash itself (I don\'t think ?try helps that much in this case). Also it\'s prone to allowing pop-ups. I don\'t know, it\'s been a while since I\'ve done this, but thought I should point this out.\n', ""\nThe best solution is package htm2txt.\nlibrary(htm2txt)\nurl <- 'https://en.wikipedia.org/wiki/Alan_Turing'\ntext <- gettxt(url)\n\nFor details, see https://CRAN.R-project.org/package=htm2txt.\n"", '\nWell it´s not exactly a R way of doing it, but it´s as simple as they come: outwit plugin for firefox. The basic version is for free and helps to extract tables and stuff. \nah and if you really wanna do it the hard way in R, this link is for you:\n', ""\nI've had good luck with the readHTMLTable() function of the XML package. It returns a list of all tables on the page.\nlibrary(XML)\nurl <- 'http://en.wikipedia.org/wiki/World_population'\nallTables <- readHTMLTable(url)\n\nThere can be many tables on each page.\nlength(allTables)\n# [1] 17\n\nSo just select the one you want.\ntbl <- allTables[[3]]\n\nThe biggest hassle can be installing the XML package. It's big, and it needs the libxml2 library (and, under Linux, it needs the xml2-config Debian package, too). The second biggest hassle is that HTML tables often contain junk you don't want, besides the data you do want.\n"", ""\nYou can also use the rvest package and first, select all html nodes/tags containing text (e.g. p, h1, h2, h3) and then extract the text from those:\nrequire(rvest)\nurl = 'https://en.wikipedia.org/wiki/Alan_Turing'\nsite = read_html(url)\ntext = html_text(html_nodes(site, 'p,h1,h2,h3')) # comma separate\n\n"", '\nHere is another approach that can be used :\nlibrary(pagedown)\nlibrary(pdftools)\nchrome_print(input = ""http://stackoverflow.com/questions/tagged?tagnames=r"", \n             output = ""C:/.../test.pdf"")\ntext <- pdf_text(""C:/.../test.pdf"")\n\nIt is also possible to use RSelenium :\nlibrary(RSelenium)\nshell(\'docker run -d -p 4445:4444 selenium/standalone-firefox\')\nremDr <- remoteDriver(remoteServerAddr = ""localhost"", port = 4445L, browserName = ""firefox"")\nremDr$open()\nremDr$navigate(""http://stackoverflow.com/questions/tagged?tagnames=r"")\nremDr$getPageSource()[[1]]\n\n']",https://stackoverflow.com/questions/3195522/is-there-a-simple-way-in-r-to-extract-only-the-text-elements-of-an-html-page,screen-scraping
Programmatic Python Browser with JavaScript,"
I want to screen-scrape a web-site that uses JavaScript. 
There is mechanize, the programmatic web browser for Python. However, it (understandably) doesn't interpret javascript. Is there any programmatic browser for Python which does? If not, is there any JavaScript implementation in Python that I could use to attempt to create one?
",18k,"
            14
        ","['\nYou might be better off using a tool like Selenium to automate the scraping using a web browser, so the JS executes and the page renders just like it would for a real user.\n', '\nThe PyV8 package nicely wraps Google\'s V8 Javascript engine for Python.  It\'s particularly nice because not only can you call from Python to Javascript code, but you can call back from Javascript to Python code.  This makes it quite straightforward to implement the usual browser-supplied objects (that is, everything in the Javascript global namespace: ""window"", ""document"", and so on), which you\'d need to do if you were going to make a Javascript-capable Python browser emulator thing, possibly by hooking this up with mechanize.\n', ""\nMy favorite is PyPhantomJS. It's written using Python and PyQt4. It's completely headless and you can control it completely from JavaScript.\nHowever, if you are looking to actually see the page, you can use QWebView from PyQt4 as well.\n"", '\nThere is also spynner "" a stateful programmatic web browser module for Python with Javascript/AJAX support based on the QtWebkit framework"" : http://code.google.com/p/spynner/\n', '\nYou could also try defining Chickenfoot page triggers on the pages in question, executing whatever operations you want on the page and saving the results of the operation to a local file, and calling Firefox from the command line inside your program, followed by reading the file.\n', '\ni recommend that you take a look at some of the options available to you at http://wiki.python.org/moin/WebBrowserProgramming - surprisingly this is coming up as a common question (i\'ve found three on stackoverflow today, by searching for the words ""python browser"" on google).  if you do the same you\'ll find the other answers i gave.\n', '\nyou may try zope browser\nhttp://pypi.python.org/pypi?:action=display&name=zope.testbrowser\n', ""\nPlaywright or pyppeteer are both reasonably good, and use headless Chromium to render pages and interpret JavaScript.\nI'd pick Playwright out of the two, simply because it's backed by a larger entity, and supports Chromium/Firefox/WebKit out of the box.\n""]",https://stackoverflow.com/questions/1916711/programmatic-python-browser-with-javascript,screen-scraping
Run multiple scrapy spiders at once using scrapyd,"
I'm using scrapy for a project where I want to scrape a number of sites - possibly hundreds - and I have to write a specific spider for each site. I can schedule one spider in a project deployed to scrapyd using:
curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2

But how do I schedule all spiders in a project at once?
All help much appreciated!
",8k,"
            12
        ","['\nMy solution for running 200+ spiders at once has been to create a custom command for the project.  See http://doc.scrapy.org/en/latest/topics/commands.html#custom-project-commands for more information about implementing custom commands.\nYOURPROJECTNAME/commands/allcrawl.py :\nfrom scrapy.command import ScrapyCommand\nimport urllib\nimport urllib2\nfrom scrapy import log\n\nclass AllCrawlCommand(ScrapyCommand):\n\n    requires_project = True\n    default_settings = {\'LOG_ENABLED\': False}\n\n    def short_desc(self):\n        return ""Schedule a run for all available spiders""\n\n    def run(self, args, opts):\n        url = \'http://localhost:6800/schedule.json\'\n        for s in self.crawler.spiders.list():\n            values = {\'project\' : \'YOUR_PROJECT_NAME\', \'spider\' : s}\n            data = urllib.urlencode(values)\n            req = urllib2.Request(url, data)\n            response = urllib2.urlopen(req)\n            log.msg(response)\n\nMake sure to include the following in your settings.py\nCOMMANDS_MODULE = \'YOURPROJECTNAME.commands\'\n\nThen from the command line (in your project directory) you can simply type\nscrapy allcrawl\n\n', ""\nSorry, I know this is an old topic, but I've started learning scrapy recently and stumbled here, and I don't have enough rep yet to post a comment, so posting an answer.\nFrom the common scrapy practices you'll see that if you need to run multiple spiders at once, you'll have to start multiple scrapyd service instances and then distribute your Spider runs among those.\n""]",https://stackoverflow.com/questions/10801093/run-multiple-scrapy-spiders-at-once-using-scrapyd,screen-scraping
Python Scraping JavaScript using Selenium and Beautiful Soup,"
I'm trying to scrape a JavaScript enables page using BS and Selenium. 
I have the following code so far. It still doesn't somehow detect the JavaScript (and returns a null value). In this case I'm trying to scrape the Facebook comments in the bottom. (Inspect element shows the class as postText)
Thanks for the help!
from selenium import webdriver  
from selenium.common.exceptions import NoSuchElementException  
from selenium.webdriver.common.keys import Keys  
import BeautifulSoup

browser = webdriver.Firefox()  
browser.get('http://techcrunch.com/2012/05/15/facebook-lightbox/')  
html_source = browser.page_source  
browser.quit()

soup = BeautifulSoup.BeautifulSoup(html_source)  
comments = soup(""div"", {""class"":""postText""})  
print comments

",16k,"
            11
        ","['\nThere are some mistakes in your code that are fixed below. However, the class ""postText"" must exist elsewhere, since it is not defined in the original source code.\nMy revised version of your code was tested and is working on multiple websites.\nfrom selenium import webdriver  \nfrom selenium.common.exceptions import NoSuchElementException  \nfrom selenium.webdriver.common.keys import Keys  \nfrom bs4 import BeautifulSoup\n\nbrowser = webdriver.Firefox()  \nbrowser.get(\'http://techcrunch.com/2012/05/15/facebook-lightbox/\')  \nhtml_source = browser.page_source  \nbrowser.quit()\n\nsoup = BeautifulSoup(html_source,\'html.parser\')  \n#class ""postText"" is not defined in the source code\ncomments = soup.findAll(\'div\',{\'class\':\'postText\'})  \nprint comments\n\n']",https://stackoverflow.com/questions/14529849/python-scraping-javascript-using-selenium-and-beautiful-soup,screen-scraping
OpenGL/D3D: How do I get a screen grab of a game running full screen in Windows?,"
Suppose I have an OpenGL game running full screen (Left 4 Dead 2). I'd like to programmatically get a screen grab of it and then write it to a video file. 
I've tried GDI, D3D, and OpenGL methods (eg glReadPixels) and either receive a blank screen or flickering in the capture stream.
Any ideas?
For what it's worth, a canonical example of something similar to what I'm trying to achieve is Fraps.
",6k,"
            10
        ","['\nThere are a few approaches to this problem. Most of them are icky, and it totally depends on what kind of graphics API you want to target, and which functions the target application uses.\nMost DirectX, GDI+ and OpenGL applications are double or tripple-buffered, so they all call:\nvoid SwapBuffers(HDC hdc)\n\nat some point. They also generate WM_PAINT messages in their message queue whenever the window should be drawn. This gives you two options.\n\nYou can install a global hook or thread-local hook into the target process and capture WM_PAINT messages. This allows you to copy the contents from the device context just before the painting happens. The process can be found by enumerating all the processes on the system and look for a known window name, or a known module handle.\n\nYou can inject code into the target process\'s local copy of SwapBuffers. On Linux this would be easy to do via the LD_PRELOAD environmental variable, or by calling ld-linux.so.2 explicitly, but there is no equivalient on Windows. Luckily there is a framework from Microsoft Research which can do this for you called Detours. You can find this here: link.\n\n\nThe demoscene group Farbrausch made a demo-capturing tool named kkapture which makes use of the Detours library. Their tool targets applications that require no user input however, so they basically run the demos at a fixed framerate by hooking into all the possible time functions, like timeGetTime(), GetTickCount() and QueryPerformanceCounter(). It\'s totally rad. A presentation written by ryg (I think?) regarding kkapture\'s internals can be found here. I think that\'s of interest to you.\nFor more information about Windows hooks, see here and here.\nEDIT:\nThis idea intrigued me, so I used Detours to hook into OpenGL applications and mess with the graphics. Here is Quake 2 with green fog added:  \nSome more information about how Detours works, since I\'ve used it first hand now:\nDetours works on two levels. The actual hooking only works in the same process space as the target process. So Detours has a function for injecting a DLL into a process and force its DLLMain to run too, as well as functions that are supposed to be used in that DLL. When DLLMain is run, the DLL should call DetourAttach() to specify the functions to hook, as well as the ""detour"" function, which is the code you want to override with.\nSo it basically works like this:\n\nYou have a launcher application who\'s only task is to call DetourCreateProcessWithDll(). It works the same way as CreateProcessW, only with a few extra parameters. This injects a DLL into a process and calls its DllMain().\nYou implement a DLL that calls the Detour functions and sets up trampoline functions. That means calling DetourTransactionBegin(), DetourUpdateThread(), DetourAttach() followed by DetourTransactionEnd().\nUse the launcher to inject the DLL you implemented into a process.\n\nThere are some caveats though. When DllMain is run, libraries that are imported later with LoadLibrary() aren\'t visible yet. So you can\'t necessarily set up everything during the DLL attachment event. A workaround is to keep track of all the functions that are overridden so far, and try to initialize the others inside these functions that you can already call. This way you will discover new functions as soon as LoadLibrary have mapped them into the memory space of the process. I\'m not quite sure how well this would work for wglGetProcAddress though. (Perhaps someone else here has ideas regarding this?)\nSome LoadLibrary() calls seem to fail. I tested with Quake 2, and DirectSound and the waveOut API failed to initalize for some reason. I\'m still investigating this.\n', ""\nI found a sourceforge'd project called taksi:\nhttp://taksi.sourceforge.net/\nTaksi does not provide audio capture, though.\n"", ""\nI've written screen grabbers in the past (DirectX7-9 era).  I found good old DirectDraw worked remarkably well and would reliably grab bits of hardware-accelerated/video screen content which other methods (D3D, GDI, OpenGL) seemed to leave blank or scrambled.  It was very fast too.\n""]",https://stackoverflow.com/questions/3486729/opengl-d3d-how-do-i-get-a-screen-grab-of-a-game-running-full-screen-in-windows,screen-scraping
What's the best approach for parsing XML/'screen scraping' in iOS? UIWebview or NSXMLParser?,"
I am creating an iOS app that needs to get some data from a web page. My first though was to use NSXMLParser initWithContentsOfURL: and parse the HTML with the NSXMLParser delegate. However this approach seems like it could quickly become painful (if, for example, the HTML changed I would have to rewrite the parsing code which could be awkward). 
Seeing as I'm loading a web page I took take a look at UIWebView too. It looks like UIWebView may be the way to go. stringByEvaluatingJavaScriptFromString: seems like a very handy way to extract the data and would allow the javascript to be stored in a separate file that would be easy to edit if the HTML changed. However, using UIWebView seems a bit hacky (seeing as UIWebView is a UIView subclass it may block the main thread, and the docs say that the javascript has a limit of 10MB).
Does anyone have any advice regarding parsing XML/HTML before I get stuck in?
UPDATE:
I wrote a blog post about my solution:HTML parsing/screen scraping in iOS
",11k,"
            8
        ","['\nI\'ve done this a few times. The best approach I\'ve found is to use libxml2 which has a mode for HTML. Then you can use XPath to query the document. \nWorking with the libxml2 API is not the most enjoyable. So, I usually bring over the XPathQuery.h/.m files documented on this page:\nhttp://cocoawithlove.com/2008/10/using-libxml2-for-parsing-and-xpath.html\nThen I fetch the data using a NSConnection and query the data with something like this:\nNSArray *tdNodes = PerformHTMLXPathQuery(self.receivedData, @""//td[@class=\'col-name\']/a/span"");\n\nSummary:\n\nAdd libxml2 to your project, here are some quick instructions for XCode4: \nhttp://cmar.me/2011/04/20/adding-libxml2-to-an-xcode-4-project/\nGet the XPathQuery.h/.m\nUse an XPath statement to query the html document.\n\n', ""\nParsing HTML with an XML parser usually does not work anyway because many sites have incorrect HTML, which a web browser will deal with, but a strict XML parser like NSXMLParser will totally fail on.\nFor many scripting languages there are great scraping libraries that are more merciful. Like Python's Beautiful Soup module. Unfortunately I do not know of such modules for Objective-C.\nLoading stuff into a UIWebView might be the simplest way to go here. Note that you do not have to put the UIWebView on screen. You can create a separate UIWindow and add the UIWebView to it, so that you do full off-screen rendering. There was a WWDC2009 video about this I think. As you already mention, it will not be lightweight though.\nDepending on the data that you want and the complexity of the pages that you need to parse, you might also be able to parse it by using regular expressions or even a hand written parser. I have done this many times, and for simple data this works well.\n""]",https://stackoverflow.com/questions/3541615/whats-the-best-approach-for-parsing-xml-screen-scraping-in-ios-uiwebview-or,screen-scraping
Webbrowser behaviour issues,"
I am trying to automate Webbrowser with .NET C#. The issue is that the control or should I say IE browser behaves strange on different computers. For example, I am clickin on link and fillup a Ajax popup form on 1st computer like this, without any error:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.Navigate(""http://www.test.com/"");
    webbrowserIE.DocumentCompleted += fillup_LoadCompleted; 
}

void fillup_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    System.Windows.Forms.HtmlElement ele = web_BrowserIE.Document.GetElementById(""login"");
    if (ele != null)
        ele.InvokeMember(""Click"");

    if (this.web_BrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        web_BrowserIE.Document.GetElementById(""login"").SetAttribute(""value"", myUserName);
        web_BrowserIE.Document.GetElementById(""password"").SetAttribute(""value"", myPassword);

        foreach (System.Windows.Forms.HtmlElement el in web_BrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        web_BrowserIE.DocumentCompleted -= fillup_LoadCompleted;        
    }
}

However, the above code wont work on 2nd pc and the only way to click is like this:
private void btn_Start_Click(object sender, RoutedEventArgs e)
{
    webbrowserIE.DocumentCompleted += click_LoadCompleted;
    webbrowserIE.Navigate(""http://www.test.com/""); 
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{
    if (this.webbrowserIE.ReadyState == System.Windows.Forms.WebBrowserReadyState.Complete)
    {
        System.Windows.Forms.HtmlElement ele = webbrowserIE.Document.GetElementById(""login"");
        if (ele != null)
            ele.InvokeMember(""Click"");

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;
        webbrowserIE.DocumentCompleted += fillup_LoadCompleted;
    }
}

void click_LoadCompleted(object sender, System.Windows.Forms.WebBrowserDocumentCompletedEventArgs e)
{

        webbrowserIE.Document.GetElementById(""login_login"").SetAttribute(""value"", myUserName);
        webbrowserIE.Document.GetElementById(""login_password"").SetAttribute(""value"", myPassword);

        //If you know the ID of the form you would like to submit:
        foreach (System.Windows.Forms.HtmlElement el in webbrowserIE.Document.GetElementsByTagName(""button""))
        {
            if (el.InnerText == ""Login"")
            {
                el.InvokeMember(""click"");
            }
        }

        webbrowserIE.DocumentCompleted -= click_LoadCompleted;      
}

So, in second solution I have to call two Load Completed Chains. Could someone advise on how should I can handle this issue? Also, a proposal for more robust approach would be very helpfull. Thank you in advance 
",3k,"
            1
        ","['\nI could recommend two things:\n\nDon\'t execute your code upon DocumentComplete event, rather do upon DOM window.onload event.\nTo make sure your web page behaves in WebBrowser control the same way as it would in full Internet Explorer browser, consider implementing Feature Control.\n\n[EDITED] There\'s one more suggestion, based on the structure of your code. Apparently, you perform a series of navigation/handle DocumentComplete actions. It might be more natural and easy to use async/await for this. Here\'s an example of doing this, with or without async/await. It illustrates how to handle onload, too:\nasync Task DoNavigationAsync()\n{\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // Navigate to http://example.com?i=2\n    // could do the click() simulation instead\n\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n    this.wb.Navigate(""http://example.com?i=2"");\n    await onloadTcs.Task;\n    // the document has been fully loaded, you can access DOM here\n    MessageBox.Show(this.wb.Document.Url.ToString());\n\n    // no more navigation, de-register DocumentCompleted handler\n    this.wb.DocumentCompleted -= handler;\n}\n\nHere\'s the same code without async/await pattern (for .NET 4.0):\nTask DoNavigationAsync()\n{\n    // save the correct continuation context for Task.ContinueWith\n    var continueContext = TaskScheduler.FromCurrentSynchronizationContext(); \n\n    bool documentComplete = false;\n    TaskCompletionSource<bool> onloadTcs = null;\n\n    WebBrowserDocumentCompletedEventHandler handler = delegate \n    {\n        if (documentComplete)\n            return; // attach to onload only once per each Document\n        documentComplete = true;\n\n        // now subscribe to DOM onload event\n        this.wb.Document.Window.AttachEventHandler(""onload"", delegate\n        {\n            // each navigation has its own TaskCompletionSource\n            if (onloadTcs.Task.IsCompleted)\n                return; // this should not be happening\n\n            // signal the completion of the page loading\n            onloadTcs.SetResult(true);\n        });\n    };\n\n    // register DocumentCompleted handler\n    this.wb.DocumentCompleted += handler;\n\n    // Navigate to http://www.example.com?i=1\n    documentComplete = false;\n    onloadTcs = new TaskCompletionSource<bool>();\n    this.wb.Navigate(""http://www.example.com?i=1"");\n\n    return onloadTcs.Task.ContinueWith(delegate \n    {\n        // the document has been fully loaded, you can access DOM here\n        MessageBox.Show(this.wb.Document.Url.ToString());\n\n        // Navigate to http://example.com?i=2\n        // could do the \'click()\' simulation instead\n\n        documentComplete = false;\n        onloadTcs = new TaskCompletionSource<bool>(); // new task for new navigation\n        this.wb.Navigate(""http://example.com?i=2"");\n\n        onloadTcs.Task.ContinueWith(delegate \n        {\n            // the document has been fully loaded, you can access DOM here\n            MessageBox.Show(this.wb.Document.Url.ToString());\n\n            // no more navigation, de-register DocumentCompleted handler\n            this.wb.DocumentCompleted -= handler;\n        }, continueContext);\n\n    }, continueContext);\n}\n\nNote, it both cases it is still a piece of asynchronous code which returns a Task object. Here\'s an example of how to handle the completion of such task:\nprivate void Form1_Load(object sender, EventArgs e)\n{\n    DoNavigationAsync().ContinueWith(_ => {\n        MessageBox.Show(""Navigation complete!"");\n    }, TaskScheduler.FromCurrentSynchronizationContext());\n}\n\nThe benefit of using TAP pattern here is that DoNavigationAsync is a self-contained, independent method. It can be reused and it doesn\'t interfere with the state of parent object (in this case, the main form).\n']",https://stackoverflow.com/questions/18572635/webbrowser-behaviour-issues,screen-scraping
PHP CSS Selector Library? [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



Is there a PHP class/library that would allow me to query an XHTML document with CSS selectors? I need to scrape some pages for data that is very easily accessible if I could somehow use CSS selectors (jQuery has spoiled me!). Any ideas?
",22k,"
            33
        ","[""\nAfter Googling further (initial results weren't very helpful), it seems there is actually a Zend Framework library for this, along with some others:\n\nDOM-Query\nphpQuery\npQuery\nQueryPath\nSimple HTML DOM Parser\nUltimate Web Scraper Toolkit\nZend-Dom\n\n"", '\nXPath is a fairly standard way to access XML (and XHTML) nodes, and provides much more precision than CSS.\n', '\nAnother one:\nhttp://querypath.org/\n', '\nA great one is a component of symfony 2, CssSelector\\Parser\xadIntroduction. It converts CSS selectors into XPath expressions. Take a look =)\nSource code\n', '\nFor jQuery users most interesting may be port of jQuery to PHP, which is phpQuery. Almost all sections of the library are ported. Additionally it contains WebBrowser plugin, which can be used for Web Scraping whole site\'s path/processes (eg accessing data available after logging in). It simply simulates web browser on the server (events and cookies too). Latest versions has experimental support for XML namespaces and CSS3 ""|"" selector.\n', ""\nI ended up using PHP Query Lite, it's very simple and has all I need.\n"", '\nFor document parsing I use DOM.  This can quite easily solve your problem if you know the tag name (in this example ""div""):\n $doc = new DOMDocument();\n $doc->loadHTML($html);\n\n $elements = $doc->getElementsByTagName(""div"");\n foreach ($elements as $e){\n  if ($e->getAttribute(""class"")!=""someclass"") continue;\n\n  //its a div.classname\n }\n\nNot sure if DOM lets you get all elements of a document at once... you might have to do a tree traversal.\n', ""\nI wrote mine, based on Mootools CSS selector engine http://selectors.svn.exyks.org/. it rely on simplexml extension ability (so, it's read-only)\n""]",https://stackoverflow.com/questions/260605/php-css-selector-library,screen-scraping
Scraping websites with Javascript enabled?,"
I'm trying to scrape and submit information to websites that heavily rely on Javascript to do most of its actions. The website won't even work when i disable Javascript in my browser.
I've searched for some solutions on Google and SO and there was someone who suggested i should reverse engineer the Javascript, but i have no idea how to do that. 
So far i've been using Mechanize and it works on websites that don't require Javascript.
Is there any way to access websites that use Javascript by using urllib2 or something similar? 
I'm also willing to learn Javascript, if that's what it takes.
",25k,"
            18
        ","['\nI wrote a small tutorial on this subject, this might help:\nhttp://koaning.io.s3-website.eu-west-2.amazonaws.com/dynamic-scraping-with-python.html\nBasically what you do is you have the selenium library pretend that it is a firefox browser, the browser will wait until all javascript has loaded before it continues passing you the html string. Once you have this string, you can then parse it with beautifulsoup.\n', ""\nI've had exactly the same problem. It is not simple at all, but I finally found a great solution, using PyQt4.QtWebKit.\nYou will find the explanations on this webpage : http://blog.motane.lu/2009/07/07/downloading-a-pages-content-with-python-and-webkit/\nI've tested it, I currently use it, and that's great !\nIts great advantage is that it can run on a server, only using X, without a graphic environment.\n"", ""\nYou should look into using Ghost, a Python library that wraps the PyQt4 + WebKit hack.\nThis makes g the WebKit client:\nimport ghost\ng = ghost.Ghost()\n\nYou can grab a page with g.open(url) and then g.content will evaluate to the document in its current state.\nGhost has other cool features, like injecting JS and some form filling methods, and you can pass the resulting document to BeautifulSoup and so on: soup = bs4.BeautifulSoup(g.content).\nSo far, Ghost is the only thing I've found that makes this kind of thing easy in Python. The only limitation I've come across is that you can't easily create more than one instance of the client object, ghost.Ghost, but you could work around that.\n"", ""\nCheck out crowbar. I haven't had any experience with it, but I was curious about the answer to your question so I started googling around. I'd like to know if this works out for you.\nhttp://grep.codeconsult.ch/2007/02/24/crowbar-scrape-javascript-generated-pages-via-gecko-and-rest/\n"", ""\nMaybe you could use Selenium Webdriver, which has python bindings I believe. I think it's mainly used as a tool for testing websites, but I guess it should be usable for scraping too. \n"", '\nI would actually suggest using Selenium.  Its mainly designed for testing Web-Applications from a ""user perspective however it is basically a ""FireFox"" driver.  I\'ve actually used it for this purpose ... although I was scraping an dynamic AJAX webpage.  As long as the Javascript form has a recognizable ""Anchor Text"" that Selenium can ""click"" everything should sort itself out.\nHope that helps\n']",https://stackoverflow.com/questions/3362859/scraping-websites-with-javascript-enabled,screen-scraping
How can I read and parse the contents of a webpage in R,"
I'd like to read the contents of a URL (e.q., http://www.haaretz.com/) in R. I am wondering how I can do it
",33k,"
            16
        ","['\nNot really sure how you want to process that page, because it\'s really messy.  As we re-learned in this famous stackoverflow question, it\'s not a good idea to do regex on html, so you will definitely want to parse this with the XML package.  \nHere\'s an example to get you started:\nrequire(RCurl)\nrequire(XML)\nwebpage <- getURL(""http://www.haaretz.com/"")\nwebpage <- readLines(tc <- textConnection(webpage)); close(tc)\npagetree <- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)\n# parse the tree by tables\nx <- xpathSApply(pagetree, ""//*/table"", xmlValue)  \n# do some clean up with regular expressions\nx <- unlist(strsplit(x, ""\\n""))\nx <- gsub(""\\t"","""",x)\nx <- sub(""^[[:space:]]*(.*?)[[:space:]]*$"", ""\\\\1"", x, perl=TRUE)\nx <- x[!(x %in% c("""", ""|""))]\n\nThis results in a character vector of mostly just webpage text (along with some javascript):\n> head(x)\n[1] ""Subscribe to Print Edition""              ""Fri., December 04, 2009 Kislev 17, 5770"" ""Israel Time:Â\xa016:48Â\xa0(EST+7)""           \n[4] ""Â\xa0Â\xa0Make Haaretz your homepage""          ""/*check the search form*/""               ""function chkSearch()"" \n\n', '\nYour best bet may be the XML package -- see for example this previous question.\n', '\nI know you asked for R. But maybe python+beautifullsoup is the way forward here? Then do your analysis with R you have scraped the screen with beautifullsoup?\n']",https://stackoverflow.com/questions/1844829/how-can-i-read-and-parse-the-contents-of-a-webpage-in-r,screen-scraping
scrapy how to set referer url,"
I need to set the referer url, before scraping a site, the site uses refering url based Authentication, so it does not allow me to login if the referer is not valid.
Could someone tell how to do this in Scrapy?
",20k,"
            11
        ","[""\nIf you want to change the referer in your spider's request, you can change DEFAULT_REQUEST_HEADERS in the settings.py file:\nDEFAULT_REQUEST_HEADERS = {\n    'Referer': 'http://www.google.com' \n}\n\n"", '\nYou should do exactly as @warwaruk indicated, below is my example elaboration for a crawl spider:\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy import Request\n\nclass MySpider(CrawlSpider):\n  name = ""myspider""\n  allowed_domains = [""example.com""]\n  start_urls = [\n      \'http://example.com/foo\'\n      \'http://example.com/bar\'\n      \'http://example.com/baz\'\n      ]\n  rules = [(...)]\n\n  def start_requests(self):\n    requests = []\n    for item in self.start_urls:\n      requests.append(Request(url=item, headers={\'Referer\':\'http://www.example.com/\'}))\n    return requests    \n\n  def parse_me(self, response):\n    (...)\n\nThis should generate following logs in your terminal:\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/foo> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/bar> (referer: http://www.example.com/)\n(...)\n[myspider] DEBUG: Crawled (200) <GET http://example.com/baz> (referer: http://www.example.com/)\n(...)\n\nWill work same with BaseSpider. In the end start_requests method is BaseSpider method, from which CrawlSpider inherits from.\nDocumentation explains more options to be set in Request apart from headers, such as: cookies , callback function, priority of the request etc.\n', ""\nJust set Referer url in the Request headers\n\nclass scrapy.http.Request(url[, method='GET', body, headers, ...\nheaders (dict) – the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers).\n\nExample:\nreturn Request(url=your_url,\n               headers={'Referer':'http://your_referer_url'})\n"", '\nOverride BaseSpider.start_requests and create there your custom Request passing it your referer header.\n']",https://stackoverflow.com/questions/13069854/scrapy-how-to-set-referer-url,screen-scraping
Extracting table contents from html with python and BeautifulSoup,"
I want to extract certain information out of an html document. E.g. it contains a table 
(among other tables with other contents) like this:
    <table class=""details"">
            <tr>
                    <th>Advisory:</th>
                    <td>RHBA-2013:0947-1</td>
            </tr>
            <tr>    
                    <th>Type:</th>
                    <td>Bug Fix Advisory</td>
            </tr>
            <tr>
                    <th>Severity:</th>
                    <td>N/A</td>
            </tr>
            <tr>    
                    <th>Issued on:</th>
                    <td>2013-06-13</td>
            </tr>
            <tr>    
                    <th>Last updated on:</th>
                    <td>2013-06-13</td>
            </tr>

            <tr>
                    <th valign=""top"">Affected Products:</th>
                    <td><a href=""#Red Hat Enterprise Linux ELS (v. 4)"">Red Hat Enterprise Linux ELS (v. 4)</a></td>
            </tr>


    </table>

I want to extract Information like the date of ""Issued on:"". It looks like BeautifulSoup4
could do this easyly, but somehow I don't manage to get it right.
My code so far:
    from bs4 import BeautifulSoup
    soup=BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)
    table_tag=soup.table
    if table_tag['class'] == ['details']:
            print table_tag.tr.th.get_text() + "" "" + table_tag.tr.td.get_text()
            a=table_tag.next_sibling
            print  unicode(a)
            print table_tag.contents

This gets me the contents of the first table row, and also a listing of the contents. 
But the next sibling thing is not working right, I guess I am just using it wrong.
Of course I could just parse the contents thingy, but it seems to me that beautiful soup
was designed to prevent us from doing exactly this (if I start parsing myself, I might as
well parse the whole doc ...). If someone could enlighten me on how to acomplish this, I 
would be gratefull. If there is a better way then BeautifulSoup, I would be interested to 
hear about it.
",41k,"
            11
        ","[""\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(unicodestring_containing_the_entire_htlm_doc)\n>>> table = soup.find('table', {'class': 'details'})\n>>> th = table.find('th', text='Issued on:')\n>>> th\n<th>Issued on:</th>\n>>> td = th.findNext('td')\n>>> td\n<td>2013-06-13</td>\n>>> td.text\nu'2013-06-13'\n\n""]",https://stackoverflow.com/questions/17196018/extracting-table-contents-from-html-with-python-and-beautifulsoup,screen-scraping
Scraping javascript-generated data using Python,"
I want to scrape some data of following url using Python.
http://www.hankyung.com/stockplus/main.php?module=stock&mode=stock_analysis_infomation&itemcode=078340
It's about a summary of company information. 
What I want to scrape is not shown on the first page. 
By clicking tab named ""재무제표"", you can access financial statement. And clicking tab named ""현금흐름표', you can access ""Cash Flow"". 
I want to scrape the ""Cash Flow"" data. 
However, Cash flow data is generated by javascript across the url.
The following link is that url which is hidden, http://stock.kisline.com/compinfo/financial/main.action?vhead=N&vfoot=N&vstay=&omit=&vwidth=
Cash flow data is generated by submitting some option value and cookie to this url.
As you perceived, itemcode=078340 in the first link means stock code and there are as many as 1680 stocks that I want gather cash flow data. I want make it a loop structure.
Is there good way to scrape cash flow data?
I tried scrapy but scrapy is difficult to cope with my another scraping code already I'm using.
",11k,"
            7
        ","[""\nThere's also dryscape (a library written by me, so the recommendation is a bit biased, obviously :) which uses a fast Webkit-based in-memory browser to navigate around. It understands Javascript, too, but is a lot more lightweight than Selenium.\n"", '\nIf you need to scape the page content which is updated with AJAX and you are not in the control of this AJAX interface I would use Selenium browser automator for the task:\nhttp://code.google.com/p/selenium/\n\nSelenium has Python bindings\nIt launches a real browser instance so it can do and scrape 100% the same thing as you see with your own eyes\nGet HTML document content after AJAX updates thru Selenium API\nUse lxml + xpath / CSS selectors to parse out the relevant parts out of the document\n\n']",https://stackoverflow.com/questions/10052465/scraping-javascript-generated-data-using-python,screen-scraping
How to replace words with span tag using jsoup?,"
Assume I have the following html:
<html>
<head>
</head>
<body>
    <div id=""wrapper"" >
         <div class=""s2"">I am going <a title=""some title"" href="""">by flying</a>
           <p>mr tt</p>
         </div> 
    </div>
</body>    
</html>

Any words in the text nodes that are equal to or greater than 4 characters for example the word 'going' is replaced with html content (not text) <span>going<span> in the original html without changing anything else.
If I try do something like element.html(replacement), the problem is if lets the current element is <div class=""s2""> it will also wipe off <a title=""some title"" 
",6k,"
            7
        ","['\nIn this case you must traverse your document as suggested by this answer. Here\'s a way of doing it using Jsoup APIs:\n\nNodeTraversor and NodeVisitor allow you to traverse the DOM\nNode.replaceWith(...) allows for replacing a node in the DOM\n\nHere\'s the code:\npublic class JsoupReplacer {\n\n  public static void main(String[] args) {\n    so6527876();\n  }\n\n  public static void so6527876() {\n    String html = \n    ""<html>"" +\n    ""<head>"" +\n    ""</head>"" +\n    ""<body>"" +\n    ""    <div id=\\""wrapper\\"" >"" +\n    ""         <div class=\\""s2\\"">I am going <a title=\\""some title\\"" href=\\""\\"">by flying</a>"" +\n    ""           <p>mr tt</p>"" +\n    ""         </div> "" +\n    ""    </div>"" +\n    ""</body>    "" +\n    ""</html>"";\n    Document doc = Jsoup.parse(html);\n\n    final List<TextNode> nodesToChange = new ArrayList<TextNode>();\n\n    NodeTraversor nd  = new NodeTraversor(new NodeVisitor() {\n\n      @Override\n      public void tail(Node node, int depth) {\n        if (node instanceof TextNode) {\n          TextNode textNode = (TextNode) node;\n          String text = textNode.getWholeText();\n          String[] words = text.trim().split("" "");\n          for (String word : words) {\n            if (word.length() > 4) {\n              nodesToChange.add(textNode);\n              break;\n            }\n          }\n        }\n      }\n\n      @Override\n      public void head(Node node, int depth) {        \n      }\n    });\n\n    nd.traverse(doc.body());\n\n    for (TextNode textNode : nodesToChange) {\n      Node newNode = buildElementForText(textNode);\n      textNode.replaceWith(newNode);\n    }\n\n    System.out.println(""result: "");\n    System.out.println();\n    System.out.println(doc);\n  }\n\n  private static Node buildElementForText(TextNode textNode) {\n    String text = textNode.getWholeText();\n    String[] words = text.trim().split("" "");\n    Set<String> longWords = new HashSet<String>();\n    for (String word : words) {\n      if (word.length() > 4) {\n        longWords.add(word);\n      } \n    }\n    String newText = text;\n    for (String longWord : longWords) {\n      newText = newText.replaceAll(longWord, \n          ""<span>"" + longWord + ""</span>"");\n    }\n    return new DataNode(newText, textNode.baseUri());\n  }\n\n}\n\n', '\nI think you need to traverse the tree.  The result of text() on an Element will be all of the Element\'s text including text within child elements. Hopefully something like the following code will be helpful to you:\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.commons.io.FileUtils;\nimport org.jsoup.Jsoup;\nimport org.jsoup.nodes.Document;\nimport org.jsoup.nodes.Element;\nimport org.jsoup.nodes.Node;\nimport org.jsoup.nodes.TextNode;\n\npublic class ScreenScrape {\n\n    public static void main(String[] args) throws IOException {\n        String content = FileUtils.readFileToString(new File(""test.html""));\n        Document doc = Jsoup.parse(content);\n        Element body = doc.body();\n        //System.out.println(body.toString());\n\n        StringBuilder sb = new StringBuilder();\n        traverse(body, sb);\n\n        System.out.println(sb.toString());\n    }\n\n    private static void traverse(Node n, StringBuilder sb) {\n        if (n instanceof Element) {\n            sb.append(\'<\');\n            sb.append(n.nodeName());            \n            if (n.attributes().size() > 0) {\n                sb.append(n.attributes().toString());\n            }\n            sb.append(\'>\');\n        }\n        if (n instanceof TextNode) {\n            TextNode tn = (TextNode) n;\n            if (!tn.isBlank()) {\n                sb.append(spanifyText(tn.text()));\n            }\n        }\n        for (Node c : n.childNodes()) {\n            traverse(c, sb);\n        }\n        if (n instanceof Element) {\n            sb.append(""</"");\n            sb.append(n.nodeName());\n            sb.append(\'>\');\n        }        \n    }\n\n    private static String spanifyText(String text){\n        StringBuilder sb = new StringBuilder();\n        StringTokenizer st = new StringTokenizer(text);\n        String token;\n        while (st.hasMoreTokens()) {\n             token = st.nextToken();\n             if(token.length() > 3){\n                 sb.append(""<span>"");\n                 sb.append(token);\n                 sb.append(""</span>"");\n             } else {\n                 sb.append(token);\n             }             \n             sb.append(\' \');\n        }\n        return sb.substring(0, sb.length() - 1).toString();\n    }\n\n}\n\n\nUPDATE\nUsing Jonathan\'s new Jsoup List element.textNode() method and combining it with MarcoS\'s suggested NodeTraversor/NodeVisitor technique I came up with (although I am modifying the tree whilst traversing it - probably a bad idea):\nDocument doc = Jsoup.parse(content);\nElement body = doc.body();\nNodeTraversor nd = new NodeTraversor(new NodeVisitor() {\n\n    @Override\n    public void tail(Node node, int depth) {\n        if (node instanceof Element) {\n            boolean foundLongWord;\n            Element elem = (Element) node;\n            Element span;\n            String token;\n            StringTokenizer st;\n            ArrayList<Node> changedNodes;\n            Node currentNode;\n            for (TextNode tn : elem.textNodes()) {\n                foundLongWord = Boolean.FALSE;\n                changedNodes = new ArrayList<Node>();\n                st = new StringTokenizer(tn.text());\n                while (st.hasMoreTokens()) {\n                    token = st.nextToken();\n                    if (token.length() > 3) {\n                        foundLongWord = Boolean.TRUE;\n                        span = new Element(Tag.valueOf(""span""), elem.baseUri());\n                        span.appendText(token);\n                        changedNodes.add(span);\n                    } else {\n                        changedNodes.add(new TextNode(token + "" "", elem.baseUri()));\n                    }\n                }\n                if (foundLongWord) {\n                    currentNode = changedNodes.remove(0);\n                    tn.replaceWith(currentNode);\n                    for (Node n : changedNodes) {\n                        currentNode.after(n);\n                        currentNode = n;\n                    }\n                }\n            }\n        }\n    }\n\n    @Override\n    public void head(Node node, int depth) {\n    }\n});    \nnd.traverse(body);\nSystem.out.println(body.toString());\n\n', '\nI am replacing word hello with hello(span tag)\nDocument doc = Jsoup.parse(content);\n    Element test =  doc.body();\n    Elements elemenets = test.getAllElements();\n    for(int i =0 ;i <elemenets .size();i++){\n        String elementText = elemenets .get(i).text();\n        if(elementText.contains(""hello""))\n            elemenets .get(i).html(l.get(i).text().replaceAll(""hello"",""<span style=\\""color:blue\\"">hello</span>""));\n    }\n\n']",https://stackoverflow.com/questions/6527876/how-to-replace-words-with-span-tag-using-jsoup,screen-scraping
scrapy log handler,"
I seek your help in the following 2 questions - How do I set the handler for the different log levels like in python. Currently, I have 
STATS_ENABLED = True
STATS_DUMP = True 

LOG_FILE = 'crawl.log'

But the debug messages generated by Scrapy are also added into the log files. Those are very long and ideally, I would  like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.
Secondly, in the docs, it says The logging service must be explicitly started through the scrapy.log.start() function. My question is, where do I run this scrapy.log.start()? Is it inside my spider?
",7k,"
            3
        ","['\n\nSecondly, in the docs, it says The logging service must be explicitly\n  started through the scrapy.log.start() function. My question is, where\n  do I run this scrapy.log.start()? Is it inside my spider?\n\nIf you run a spider using scrapy crawl my_spider -- the log is started automatically if STATS_ENABLED = True\nIf you start the crawler process manually, you can do scrapy.log.start() before starting the crawler process.\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.conf import settings\n\n\nsettings.overrides.update({}) # your settings\n\ncrawlerProcess = CrawlerProcess(settings)\ncrawlerProcess.install()\ncrawlerProcess.configure()\n\ncrawlerProcess.crawl(spider) # your spider here\n\nlog.start() # depends on LOG_ENABLED\n\nprint ""Starting crawler.""\ncrawlerProcess.start()\nprint ""Crawler stopped.""\n\nThe little knowledge I have about your first question:\nBecause you have to start the scrapy log manually, this allows you to use your own logger. \nI think you can copy module scrapy/scrapy/log.py in scrapy sources, modify it, import it instead of scrapy.log and run start() - scrapy will use your log. In it there is a line in function start() which says log.startLoggingWithObserver(sflo.emit, setStdout=logstdout). \nMake your own observer (http://docs.python.org/howto/logging-cookbook.html#logging-to-multiple-destinations) and use it there.\n', ""\n\nI would like the DEBUG level messages to left on standard error and INFO messages to be dump to my LOG_FILE.\n\nYou can set LOG_LEVEL = 'INFO' in settings.py, but it will completely disable DEBUG messages.\n"", '\nHmm, \nJust wanted to update that I am able to get the logging file handler to file by using\nfrom twisted.python import log\nimport logging\nlogging.basicConfig(level=logging.INFO, filemode=\'w\', filename=\'log.txt\'"""""")\nobserver = log.PythonLoggingObserver()\nobserver.start()\n\nhowever I am unable to get the log to display the spiders\' name like from twisted in standard error. I posted this question. \n', ""\nscrapy some-scrapy's-args -L 'INFO' -s LOG_FILE=log1.log\n\noutputs will be redirected to a logname file .\n""]",https://stackoverflow.com/questions/8320730/scrapy-log-handler,screen-scraping
Evaluate javascript on a local html file (without browser),"
This is part of a project I am working on for work.
I want to automate a Sharepoint site, specifically to pull data out of a database that I and my coworkers only have front-end access to.
I FINALLY managed to get mechanize (in python) to accomplish this using Python-NTLM, and by patching part of it's source code to fix a reoccurring error.
Now, I am at what I would hope is my final roadblock: Part of the form I need to submit seems to be output of a JavaScript function :| and lo and behold... Mechanize does not support javascript. I don't want to emulate the javascript functionality myself in python because I would ideally like a reusable solution...
So, does anyone know how I could evaluate the javascript on the local html I download from sharepoint? I just want to run the javascript somehow (to complete the loading of the page), but without a browser.
I have already looked into selenium, but it's pretty slow for the amount of work I need to get done... I am currently looking into PyV8 to try and evaluate the javascript myself... but surely there must be an app or library (or anything) that can do this??
",2k,"
            3
        ","['\nWell, in the end I came down to the following possible solutions:\n\nRun Chrome headless and collect the html output (thanks to koenp for the link!)\nRun PhantomJS, a headless browser with a javascript api\nRun HTMLUnit; same thing but for Java\nUse Ghost.py, a python-based headless browser (that I haven\'t seen suggested anyyyywhere for some reason!)\nWrite a DOM-based javascript interpreter based on Pyv8 (Google v8 javascript engine) and add this to my current ""half-solution"" with mechanize.\n\nFor now, I have decided to use either use Ghost.py or my own modification of the PySide/PyQT Webkit (how ghost works) to evaluate the javascript, as apparently they can run quite fast if you optimize them to not download images and disable the GUI.\nHopefully others will find this list useful!\n', ""\nWell you will need something that both understands the DOM and understand Javascript, so that comes down to a headless browser of some sort. Maybe you can take a look at the selenium webdriver, but I guess you already did that. I don't hink there is an easy way of doing this without running the stuff in an actually browser engine.\n""]",https://stackoverflow.com/questions/16375251/evaluate-javascript-on-a-local-html-file-without-browser,screen-scraping
CasperJS click event having AJAX call,"
I am trying to fetch data from a site by simulating events using CasperJS with phantomJS 1.7.0.
I am able to simulate normal click events and select events. But my code fails in following scenario:
When I click on button / anchor etc on remote page, the click on remote page  initiates an AJAX call / JS call(depending on how that page is implemented by programmer.). 
In case of JS call, my code works and I get changed data. But for clicks where is AJAX call is initiated, I do not get updated data.
For debugging, I tried to get the page source of the element container(before and after), but I see no change in code. 
I tried to set wait time from 10 sec to 1 ms range, but that to does not reflect any changes in behavior. 
Below is my piece of code for clicking. I am using an array of CSS Paths, which represents which element(s) to click.
/*Click on array of clickable elements using CSS Paths.*/
fn_click = function(){
casper.each(G_TAGS,function(casper, cssPath, count1) 
                    {
                            casper.then ( function() {
                            casper.click(cssPath);

                            this.echo('DEBUG AFTER CLICKING -START HTML ');
                            //this.echo(this.getHTML(""CONTAINER WHERE DETAILS CHANGE""));
                            this.echo('DEBUG AFTER CLICKING -START HTML');
                            casper.wait(5000, function() 
                                                    {   

                                                        casper.then(fn_getData);
                                                    } 
                                    );
                            });     
                    });
};

UPDATE:
I tried to use remote-debug option from phantomJS, to debug above script. 
It is not working. I am on windows. I will try to run remote debugging on Ubuntu as well. 
Please help me. I would appreciate any help on this. 
UPDATE:
Please have a look at following code as a sample. 
https://gist.github.com/4441570

Content before click and after click are same. 
I am clicking on sorting options provided under tag (votes / activity etc.). 
",5k,"
            2
        ","[""\nI had the same problem today. I found this post, which put me in the direction of jQuery.\nAfter some testing I found out that there was already a jQuery loaded on that webpage. (A pretty old version though)\nLoading another jQuery on top of that broke any js calls made, so also the link that does an Ajax call.\nTo solve this I found http://api.jquery.com/jQuery.noConflict/\nand I added the following to my code:\n    this.evaluate(function () { jq = $.noConflict(true) } ); \n\nAnything that was formerly assigned to $ will be restored that way. And the jQuery that you injected is now available under 'jq'.\nHope this helps you guys.\n""]",https://stackoverflow.com/questions/14098483/casperjs-click-event-having-ajax-call,screen-scraping
C# WebClient - View source question,"
I'm using  a C# WebClient to post login details to a page and read the all the results.
The page I am trying to load includes flash (which, in the browser, translates into HTML). I'm guessing it's flash to avoid being picked up by search engines???
The flash I am interested in is just text (not an image/video) etc and when I ""View Selection Source"" in firefox I do actually see the text, within HTML, that I want to see.
(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)
Currently after I have posted my login details, and loaded the HTML back, I see the page which does NOT show the flash HTML (as if I had viewed source for the whole page).
Thanks in advance,
Jim
PS: I should point out that the POST is actually working, my log in is successful.
",3k,"
            2
        ","['\nFiddler (or similar tool) is invaluable to track down screen-scraping problems like this.  Using a normal browser and with fiddler active, look at all the requests being made as you go through the login and navigation process to get to the data you want.  In between, you will likely see one or more things that your code is doing differently which the server is responding to and hence showing you different HTML than a real client.\nThe list of stuff below (think of it as ""scraping 101"") is what you want to look for.  Most of the stuff below is probably stuff you\'re already doing, but I included everything for completeness. \nIn order to scrape effectively, you may need to deal with one or more of the following:\n\ncookies and/or hidden fields. when you show up at any page on a site, you\'ll typically get a session cookie and/or hidden form field which (in a normal browser) would be propagated back to the server on all subsequent requests. You will likely also get a persistent cookie.  On many sites, if a requests shows up without a proper cookie (or form field for sites using ""cookieless sessions""), the site will redirect the user to a ""no cookies"" UI, a login page, or another undesirable location (from the scraper app\'s perspective).  always make sure you capture the cookies set on the initial request and faithfully send them back to the server on subsequent requests, except if one of those subsequent requests changes a cookie (in which case propagate that new cookie instead).\nauthentication tokens a special case of above is forms-authentication cookies or hidden fields. make sure you\'re capturing the login token (usually a cookie) and sending it back.\nPOST vs. GET this is obvious, but make sure you\'re using the same HTTP method that a real browser does. \nform fields (esp. hidden ones!) I\'m sure you\'re doing this already, but make sure to send all form fields that a real browser does, not just the visible fields. make sure fields are HTML-encoded properly.\nHTTP headers. you already checked this, but it may make sense to check again just to make sure the (non-cookie) headers are identical. I always start with the exact same headers and then start pulling out headers one by one, and only keep the ones that cause the request to fail or return bogus data. this approach simplifies your scraping code. \nredirects. These can either come from the server, or from client script (e.g. ""if user doesn\'t have flash plug-in loaded, redirect to a non-flash page"").  See WebRequest: How to find a postal code using a WebRequest against this ContentType=""application/xhtml+xml, text/xml, text/html; charset=utf-8""? for a crazy example of how redirection can trip up a screen-scraper.  Note that if you\'re using .NET for scraping, you\'ll need to use HttpWebRequest (not WebClient) for redirect-dependent scraping, because by default WebClient doesn\'t provide a way for your code to attach cookies and headers to the second (post-redirect) request.  See the thread above for more details.\nsub-requests (frames, ajax, flash, etc.) - often, page elements (not the main HTTP requests) will end up fetching the data you want to scrape.  you\'ll be able to figure this out by looking which HTTP response contains the text you want, and then working backwards until you find what on the page is actually making the request for that content. A few sites do really crazy things in sub-requests, like requesting compressed or encrypted text via ajax, and then using client-side script to decrypt it.  if this is the case, you\'ll need to do a bit more work like reverse-engineering what the client script is doing.\nordering - this one is obvious: make HTTP requests in the same order that a browser client does. that doesn\'t mean you need to make every request (e.g. images). Typically you only need to make the requests which return text/html content type, unless the data you want is not in the HTML and is in an ajax/flash/etc. request. \n\n', '\n\n(Interestingly when I view the source for the whole page I do not see the text, within HTML, that I want to see. Could this be related?)\n\nThis usually means that the discrepancy is caused by some DOM manipulations via javascript after the page has loaded.  Try turning off javascript and see what it looks like.\n']",https://stackoverflow.com/questions/1471062/c-sharp-webclient-view-source-question,screen-scraping
Scraping YouTube links from a webpage,"
I've been trying to scrape YouTube links from a webpage, but nothing has worked.
This is a picture of what I've been trying to scrape.:

This is the code I tried most recently:
youtube_link = soup.find(""a"", class_=""ytp-title-link yt-uix-sessionlink"")

And this is the link to the website the YouTube link is in: https://www.electronic-festivals.com/event/i-am-hardstyle-germany
",2k,"
            1
        ","['\nMost of the youtube links are within an iframe and javascript also needs to run. Try using selenium. The following extracts any src or href containing youtube. I only enter the key iframe hosting the youtube clip. You could loop all iframes checking.\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef addItems(links, final):\n    for link in links:\n        ref = link.get_attribute(\'src\') if link.get_attribute(\'src\') is not None else link.get_attribute(\'href\')\n        final.append(ref)\n    return final\n\nurl = ""https://www.electronic-festivals.com/event/i-am-hardstyle-germany"" \ndriver = webdriver.Chrome()\ndriver.get(url)\ndriver.switch_to.frame(driver.find_element_by_css_selector(\'.media-youtube-player\'))\nfinal = []\n\ntry:\n    links = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ""[href*=youtube] , [src*=youtube]"")))\n    addItems(links, final)\nexcept:\n    pass\nfinally:\n    driver.switch_to.default_content()\n\nlinks = driver.find_elements_by_css_selector(\'[href*=youtube] , [src*=youtube]\')\naddItems(links, final)\n\nfor link in set(final):\n    print(link)\n\ndriver.quit()\n\n', '\nyou better use tools instead, youtube frontend or code will change, so better use command link tools, or apis or https://onlinetool.app/ext/youtube_dl\n', '\nIf you mean by scraping downloading, try\npip install youtube-dl\n\nin your shell.\n']",https://stackoverflow.com/questions/54973419/scraping-youtube-links-from-a-webpage,screen-scraping
How does a site like kayak.com aggregate content? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



Greetings,
I've been toying with an idea for a new project and was wondering if anyone has any idea on how a service like Kayak.com is able to aggregate data from so many sources so quickly and accurately. More specifically, do you think Kayak.com is interacting with APIs or are they crawling/scraping airline and hotel websites in order to fulfill user requests? I know there isn't one right answer for this sort of thing but I'm curious to know what others think would be a good way to go about this. If it helps, pretend you are going to create kayak.com tomorrow ... where is your data coming from?
",79k,"
            84
        ","['\nI\'m working in travel industry as a software architect / project lead on the precisely kind of project you describe - in our region we work with suppliers directly, but for outgoing we connect to several aggregators.\nTo answer your question... some data you have, some you get in various ways, and some you have to torture and twist until it confesses.\nWhat\'s your angle?\nThe questions you have to ask are... Do you want to sell advertising like Kayak or do you take a cut like Expedia? Are you into search or into selling travel services? Do you target niche (for example, just air travel) or everything (accommodation, airlines, rent-a-car, additional services like transport/sightseeing/conferences etc)? Do you target region (US or part of US) or the world? How deep do you go - do you just show several sites on a single screen, or do you bundle different services together and package them dynamically?\nGetting the data\nIf you\'re going with Kayak business model, you technically don\'t need site\'s permission... but a lot of sites have affiliate programs with IFrames or other simple ways to direct the customer to their site. On the plus side, you don\'t have to deal with payments/complaints and travelers themselves. As for the cons... if you want to compare prices yourself and present the cheapest option to the user, you\'ll have to integrate on a deeper level, and that means APIs and web scraping.\nAs for web scraping... avoid it. It sucks. Really. Just don\'t do it. Trust me on this one. For example, some things like lowcosters you can\'t get without web scraping. Low cost airlines live from value added services. If the user doesn\'t see their website, they don\'t sell extra stuff, and they don\'t earn anything. Therefore, they don\'t have affiliates, they don\'t offer APIs, and they change their site layout almost constantly. However, there are companies which earn a living by web scraping lowcoster\'s sites and wrapping them into nice APIs. If you can afford them, you can give your users cost-comparison of low cost flights and that\'s huge.\nOn the other hand, there are ""normal"" carriers which offer APIs. It\'s not that big of a problem to get to airlines since they\'re all united under IATA; basically, you buy from IATA, and IATA distributes the money to carriers. However, you probably don\'t want to connect directly to carrier network. They have web services and SOAP these days, but believe me when I say that there are SOAP protocols which are just an insanely thin wrappers around a text prompt through which you can interact with a mainframe with an 80es-style protocol (think of a Unix prompt where you\'re billed per command; and it takes about 20 commands to do one search). That\'s why you probably want to connect to somebody a bit more down the food chain, with a better API.\nAirlines are thus on both extremes of Gaussian curve; on one side are individual suppliers, and on the other highly centralized systems where you implement one API and you\'re able to fly anywhere in the world. Accommodation and the rest of travel products are in between. There are several big players which aggregate hotels, and a ton of small suppliers with a lot of aggregators which cover only part of a spectrum. For example, you can rent a lighthouse and it\'s even not that expensive - but you won\'t be able to compare the prices of different lighthouses in one place.\nIf you\'re into Kayak business model, you\'ll probably end up scraping websites. If you\'re into integrating different providers, you\'ll often work with APIs, some of which are pretty good, and most of which are tolerable. I haven\'t worked with RSS but there\'s not a lot of difference between RSS and web scraping. There is also a fourth option not mentioned in Jeff\'s answer... the one where you get your data nightly, for example .CSV files through FTP and similar.\nLife sucks (mini-rant)\nAnd then there\'s complexity. The more value you want to add, the more complexity you\'ll have to handle. Can you search accommodations which allow pets? For a hostel which is located less than 5 km from the town center? Are you combining flights, and are you able to guarantee that the traveler will have enough time to get from one airport to another... can you sell the transport in advance? A famous cellist doesn\'t want to part from his precious 18th century cello; can you sell him another seat for the cello (yep, not making this one up)?\nWant to compare prices? Sure, the room is EUR 30 per night. But you can either get one double for 30 and one single for 20, or you can get one extra bed in a double and get 70% off for third person. But only if it\'s a child under 12 years of age; our extra beds are not for adults. And you don\'t get the price for extra bed in search results - only when you calculate the final price.\nAnd don\'t even get me started on dynamic packaging. Want to sell accommodation + rent-a-car? No problem; integrate with two different providers, and off you go... manually updating list of locations in the city (from rent-a-car provider) to match with hotels (from accommodation provider, who gives you only the city for each hotel). Of course, provided that you\'ve already matched the list of cities from the two, since there is no international standard for city codes.\nUnlike a lot of other industries which have many products, travel industry has many very complex products. Amazon has it easy; selling books and selling potatoes, it\'s the same thing; you can even ship them in the same box. They combine easily and aren\'t assembled from many parts. :)\nP.S. Linking to an interesting recent thread on Hacker News with some insider info regarding flights.\nP.P.S. Recently stumbled on a great albeit rather old blogpost on IATA\'s NDC protocol with overview of how travel industry is connected and a history lesson how this came to be.\n', '\nThey use a software package like ITA Software, which is one of the companies Google is in the process of picking up.\n', ""\nOnly 3 ways I know of to get data from websites.\nRSS Feeds - We use rss feeds a lot at my company to integrate existing site's data with our apps.  It's fast and most sites already have an RSS feed available.  The problem with this is not all sites implement the RSS standard properly so if you're pulling data from many RSS feeds across many sites then make sure you write your code so that you can add exceptions and filters easily. \nAPIs - These are nice if they are designed well and have all the information you need, however that's not always the case, plus if the sites are not using a standard api format then you'll have to support multiple API's.\nWeb Scraping - This method would be the most unreliable as well as the most expensive to maintain.  But if you're left with nothing else it can be done.  \n"", ""\nThis article says that Kayak was asked to stop scrapping a certain airlines page. That leads me to believe that they probably do scraping on sites that they don't have a relationship with (and a data feed that comes with that relationship).\n"", '\nTravelport offer a product called ""Universal API"" which connects to flights and hotels and car rental companies and copes with package deals and all the various complexities to do with taxes and exchange rates:\nhttps://developer.travelport.com/app/developer-network/resource-centre-uapi\nI\'ve just started using it and it seems fine so far.  The queries are a little slow, but then so is every query on every OTA (Online travel agent)\'s site.\n', ""\nThere's two good APIs I've found from flight comparison websites recently\nThere's one from Wego, and one from Skyscanner. Both seem to have a good range and breadth of data from a number of airlines and good documentation too.\nWego pays each time a user clicks from your app to a booking website and Skyscanner pay affiliates 50% of 'revenue' (I assume that means the commission they make from airlines)\n"", ""\nThis is an old post but I thought I'd just add. I'm a data architect who works for a company that feeds these travel sites with content. This company enters into contracts with many hotel brands, individual hotels and other content providers. We aggregate this information then pass it onto the different channels. They then aggregate again in to their system.\nThe Large GDS systems are also content providers.\nAggregation is done by many methods... matching algorithms(in-house) and keys. Being an aggregation service, we need to communicate on the client level.\nHope this helps! cheers!\n""]",https://stackoverflow.com/questions/4607141/how-does-a-site-like-kayak-com-aggregate-content,screen-scraping
"Screen scraping: getting around ""HTTP Error 403: request disallowed by robots.txt""","
Is there a way to get around the following?
httperror_seek_wrapper: HTTP Error 403: request disallowed by robots.txt

Is the only way around this to contact the site-owner (barnesandnoble.com).. i'm building a site that would bring them more sales, not sure why they would deny access at a certain depth.
I'm using mechanize and BeautifulSoup on Python2.6.
hoping for a work-around
",49k,"
            54
        ","['\noh you need to ignore the robots.txt\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\n\n', '\nYou can try lying about your user agent (e.g., by trying to make believe you\'re a human being and not a robot) if you want to get in possible legal trouble with Barnes & Noble.  Why not instead get in touch with their business development department and convince them to authorize you specifically?  They\'re no doubt just trying to avoid getting their site scraped by some classes of robots such as price comparison engines, and if you can convince them that you\'re not one, sign a contract, etc, they may well be willing to make an exception for you.\nA ""technical"" workaround that just breaks their policies as encoded in robots.txt is a high-legal-risk approach that I would never recommend.  BTW, how does their robots.txt read?\n', ""\nThe code to make a correct request:\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\nbr.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\nresp = br.open(url)\nprint resp.info()  # headers\nprint resp.read()  # content\n\n"", ""\nMechanize automatically follows robots.txt, but it can be disabled assuming you have permission, or you have thought the ethics through ..\nSet a flag in your browser: \nbrowser.set_handle_equiv(False) \n\nThis ignores robots.txt.\nAlso, make sure you throttle your requests, so you don't put too much load on their site. (Note, this also makes it less likely that they will detect and ban you).\n"", ""\nThe error you're receiving is not related to the user agent.  mechanize by default checks robots.txt directives automatically when you use it to navigate to a site.  Use the .set_handle_robots(false) method of mechanize.browser to disable this behavior.\n"", ""\nSet your User-Agent header to match some real IE/FF User-Agent.\nHere's my IE8 useragent string:\nMozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; AskTB5.6)\n\n"", '\nWithout debating the ethics of this you could modify the headers to look like the googlebot for example, or is the googlebot blocked as well?\n', '\nAs it seems, you have to do less work to bypass robots.txt, at least says this article. So you might have to remove some code to ignore the filter.\n']",https://stackoverflow.com/questions/2846105/screen-scraping-getting-around-http-error-403-request-disallowed-by-robots-tx,screen-scraping
BeautifulSoup: How do I extract all the <li>s from a list of <ul>s that contains some nested <ul>s?,"
I'm a newbie programmer trying to jump in to Python by building a script that scrapes http://en.wikipedia.org/wiki/2000s_in_film and extracts a list of ""Movie Title (Year)"".
My HTML source looks like:
<h3>Header3 (Start here)</h3>
<ul>
    <li>List items</li>
    <li>Etc...</li>
</ul>
<h3>Header 3</h3>
<ul>
    <li>List items</li>
    <ul>
        <li>Nested list items</li>
        <li>Nested list items</li></ul>
    <li>List items</li>
</ul>
<h2>Header 2 (end here)</h2>

I'd like all the li tags following the first h3 tag and stopping at the next h2 tag, including all nested li tags.
firstH3 = soup.find('h3')

...correctly finds the place I'd like to start.
firstH3 = soup.find('h3') # Start here
uls = []
for nextSibling in firstH3.findNextSiblings():
    if nextSibling.name == 'h2':
        break
    if nextSibling.name == 'ul':
        uls.append(nextSibling)

...gives me a list uls, each with li contents that I need.
Excerpt of the uls list:
<ul>
...
    <li><i><a href=""/wiki/Agent_Cody_Banks"" title=""Agent Cody Banks"">Agent Cody Banks</a></i> (2003)</li>
    <li><i><a href=""/wiki/Agent_Cody_Banks_2:_Destination_London"" title=""Agent Cody Banks 2: Destination London"">Agent Cody Banks 2: Destination London</a></i> (2004)</li>
    <li>Air Bud series:
        <ul>
            <li><i><a href=""/wiki/Air_Bud:_World_Pup"" title=""Air Bud: World Pup"">Air Bud: World Pup</a></i> (2000)</li>
            <li><i><a href=""/wiki/Air_Bud:_Seventh_Inning_Fetch"" title=""Air Bud: Seventh Inning Fetch"">Air Bud: Seventh Inning Fetch</a></i> (2002)</li>
            <li><i><a href=""/wiki/Air_Bud:_Spikes_Back"" title=""Air Bud: Spikes Back"">Air Bud: Spikes Back</a></i> (2003)</li>
            <li><i><a href=""/wiki/Air_Buddies"" title=""Air Buddies"">Air Buddies</a></i> (2006)</li>
        </ul>
    </li>
    <li><i><a href=""/wiki/Akeelah_and_the_Bee"" title=""Akeelah and the Bee"">Akeelah and the Bee</a></i> (2006)</li>
...
</ul>

But I'm unsure of where to go from here.

Update:
Final Code:
lis = []
    for ul in uls:
        for li in ul.findAll('li'):
            if li.find('ul'):
                break
            lis.append(li)

    for li in lis:
        print li.text.encode(""utf-8"")

The if...break throws out the LI's that contain UL's since the nested LI's are now duplicated.
Print output is now:


102 Dalmatians(2000)
10th & Wolf(2006)
11:14(2006)
12:08 East of Bucharest(2006)
13 Going on 30(2004)
1408(2007)
...


",80k,"
            35
        ","[""\n.findAll() works for nested li elements:\nfor ul in uls:\n    for li in ul.findAll('li'):\n        print(li)\n\nOutput:\n<li>List items</li>\n<li>Etc...</li>\n<li>List items</li>\n<li>Nested list items</li>\n<li>Nested list items</li>\n<li>List items</li>\n\n"", ""\nA list comprehension could work, too.\nlis = [li for ul in uls for li in ul.findAll('li')]\n\n"", '\nimport requests\nfrom bs4 import BeautifulSoup\nr = requests.get(""https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_list_test"")\nsoup =   BeautifulSoup(r.content,""lxml"")\nw3schollsList = soup.find_all(\'body\')\nfor w3scholl in w3schollsList:\n    ulList = w3scholl.find_all(\'li\')\n    for li in ulList:\n        print(li)\n\nNote: here is to get the ""li"" inside the div we made\n']",https://stackoverflow.com/questions/4362981/beautifulsoup-how-do-i-extract-all-the-lis-from-a-list-of-uls-that-contains,screen-scraping
"XPath to Parse ""SRC"" from IMG tag?","
Right now I successfully grabbed the full  element from an HTML page with this:
//img[@class='photo-large']

for example it would return this: 
<img src=""http://example.com/img.jpg"" class='photo-large' />

But I only need the SRC url (http://example.com/img.jpg). Any help?
",69k,"
            29
        ","[""\nYou are so close to answering this yourself that I am somewhat reluctant to answer it for you. However, the following XPath should provide what you want (provided the source is XHTML, of course).\n//img[@class='photo-large']/@src\n\nFor further tips, check out W3 Schools. They have excellent tutorials on such things and a great reference too.\n"", '\nUsing Hpricot this works:\ndoc.at(\'//img[@class=""photo-large""]\')[\'src\']\n\nIn case you have more than one image, the following gives an array:\ndoc.search(\'//img[@class=""photo-large""]\').map do |e| e[\'src\'] end\n\nHowever, Nokogiri is many times faster and it “can be used as a drop in replacement” for Hpricot.\nHere the version for Nokogiri, in which this XPath for selecting attributes works:\ndoc.at(\'//img[@class=""photo-large""]/@src\').to_s\n\nor for many images:\ndoc.search(\'//img[@class=""photo-large""]/@src\').to_a\n\n', '\n//img/@src\nyou can just go with this if you want a link of the image.\nexample:\n<img alt="""" class=""avatar width-full rounded-2"" height=""230"" src=""https://avatars3.githubusercontent.com/...;s=460"" width=""230"">\n\n']",https://stackoverflow.com/questions/1179641/xpath-to-parse-src-from-img-tag,screen-scraping
Puppeteer waitForSelector on multiple selectors,"
I have Puppeteer controlling a website with a lookup form that can either return a result or a ""No records found"" message. How can I tell which was returned? 
waitForSelector seems to wait for only one at a time, while waitForNavigation doesn't seem to work because it is returned using Ajax.
I am using a try catch, but it is tricky to get right and slows everything way down.
try {
    await page.waitForSelector(SELECTOR1,{timeout:1000}); 
}
catch(err) { 
    await page.waitForSelector(SELECTOR2);
}

",25k,"
            23
        ","[""\nMaking any of the elements exists\nYou can use querySelectorAll and waitForFunction together to solve this problem. Using all selectors with comma will return all nodes that matches any of the selector.\nawait page.waitForFunction(() => \n  document.querySelectorAll('Selector1, Selector2, Selector3').length\n);\n\nNow this will only return true if there is some element, it won't return which selector matched which elements.\n"", ""\nhow about using Promise.race() like something I did in the below code snippet, and don't forget the { visible: true } option in page.waitForSelector() method.\npublic async enterUsername(username:string) : Promise<void> {\n    const un = await Promise.race([\n        this.page.waitForSelector(selector_1, { timeout: 4000, visible: true })\n        .catch(),\n        this.page.waitForSelector(selector_2, { timeout: 4000, visible: true })\n        .catch(),\n    ]);\n\n    await un.focus();\n    await un.type(username);\n}\n\n"", ""\nAn alternative and simple solution would be to approach this from a more CSS perspective.  waitForSelector seems to follow the CSS selector list rules. So essentially you can select multiple CSS elements by just using a comma.\ntry {    \n    await page.waitForSelector('.selector1, .selector2',{timeout:1000})\n} catch (error) {\n    // handle error\n}\n\n"", '\nUsing Md. Abu Taher\'s suggestion, I ended up with this:\n// One of these SELECTORs should appear, we don\'t know which\nawait page.waitForFunction((sel) => { \n    return document.querySelectorAll(sel).length;\n},{timeout:10000},SELECTOR1 + "", "" + SELECTOR2); \n\n// Now see which one appeared:\ntry {\n    await page.waitForSelector(SELECTOR1,{timeout:10});\n}\ncatch(err) {\n    //check for ""not found"" \n    let ErrMsg = await page.evaluate((sel) => {\n        let element = document.querySelector(sel);\n        return element? element.innerHTML: null;\n    },SELECTOR2);\n    if(ErrMsg){\n        //SELECTOR2 found\n    }else{\n        //Neither found, try adjusting timeouts until you never get this...\n    }\n};\n//SELECTOR1 found\n\n', ""\nI had a similar issue and went for this simple solution:\nhelpers.waitForAnySelector = (page, selectors) => new Promise((resolve, reject) => {\n  let hasFound = false\n  selectors.forEach(selector => {\n    page.waitFor(selector)\n      .then(() => {\n        if (!hasFound) {\n          hasFound = true\n          resolve(selector)\n        }\n      })\n      .catch((error) => {\n        // console.log('Error while looking up selector ' + selector, error.message)\n      })\n  })\n})\n\nAnd then to use it:\nconst selector = await helpers.waitForAnySelector(page, [\n  '#inputSmsCode', \n  '#buttonLogOut'\n])\n\nif (selector === '#inputSmsCode') {\n  // We need to enter the 2FA sms code. \n} else if (selector === '#buttonLogOut') {\n  // We successfully logged in\n}\n\n"", '\nIn puppeteer you can simply use multiple selectors separated by coma like this:\nconst foundElement = await page.waitForSelector(\'.class_1, .class_2\');\n\nThe returned element will be an elementHandle of the first element found in the page.\nNext if you want to know which element was found you can get the class name like so:\nconst className = await page.evaluate(el => el.className, foundElement);\n\nin your case a code similar to this should work:\nconst foundElement = await page.waitForSelector([SELECTOR1,SELECTOR2].join(\',\'));\nconst responseMsg = await page.evaluate(el => el.innerText, foundElement);\nif (responseMsg == ""No records found""){ // Your code here }\n\n', ""\nOne step further using Promise.race() by wrapping it and just check index for further logic:\n// Typescript\nexport async function racePromises(promises: Promise<any>[]): Promise<number> {\n  const indexedPromises: Array<Promise<number>> = promises.map((promise, index) => new Promise<number>((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\n// Javascript\nexport async function racePromises(promises) {\n  const indexedPromises = promises.map((promise, index) => new Promise((resolve) => promise.then(() => resolve(index))));\n  return Promise.race(indexedPromises);\n}\n\nUsage:\nconst navOutcome = await racePromises([\n  page.waitForSelector('SELECTOR1'),\n  page.waitForSelector('SELECTOR2')\n]);\nif (navigationOutcome === 0) {\n  //logic for 'SELECTOR1'\n} else if (navigationOutcome === 1) {\n  //logic for 'SELECTOR2'\n}\n\n\n\n"", ""\nCombining some elements from above into a helper method, I've built a command that allows me to create multiple possible selector outcomes and have the first to resolve be handled.\n\n\n/**\r\n * @typedef {import('puppeteer').ElementHandle} PuppeteerElementHandle\r\n * @typedef {import('puppeteer').Page} PuppeteerPage\r\n */\r\n\r\n/** Description of the function\r\n  @callback OutcomeHandler\r\n  @async\r\n  @param {PuppeteerElementHandle} element matched element\r\n  @returns {Promise<*>} can return anything, will be sent to handlePossibleOutcomes\r\n*/\r\n\r\n/**\r\n * @typedef {Object} PossibleOutcome\r\n * @property {string} selector The selector to trigger this outcome\r\n * @property {OutcomeHandler} handler handler will be called if selector is present\r\n */\r\n\r\n/**\r\n * Waits for a number of selectors (Outcomes) on a Puppeteer page, and calls the handler on first to appear,\r\n * Outcome Handlers should be ordered by preference, as if multiple are present, only the first occuring handler\r\n * will be called.\r\n * @param {PuppeteerPage} page Puppeteer page object\r\n * @param {[PossibleOutcome]} outcomes each possible selector, and the handler you'd like called.\r\n * @returns {Promise<*>} returns the result from outcome handler\r\n */\r\nasync function handlePossibleOutcomes(page, outcomes)\r\n{\r\n  var outcomeSelectors = outcomes.map(outcome => {\r\n    return outcome.selector;\r\n  }).join(', ');\r\n  return page.waitFor(outcomeSelectors)\r\n  .then(_ => {\r\n    let awaitables = [];\r\n    outcomes.forEach(outcome => {\r\n      let await = page.$(outcome.selector)\r\n      .then(element => {\r\n        if (element) {\r\n          return [outcome, element];\r\n        }\r\n        return null;\r\n      });\r\n      awaitables.push(await);\r\n    });\r\n    return Promise.all(awaitables);\r\n  })\r\n  .then(checked => {\r\n    let found = null;\r\n    checked.forEach(check => {\r\n      if(!check) return;\r\n      if(found) return;\r\n      let outcome = check[0];\r\n      let element = check[1];\r\n      let p = outcome.handler(element);\r\n      found = p;\r\n    });\r\n    return found;\r\n  });\r\n}\n\n\nTo use it, you just have to call and provide an array of Possible Outcomes and their selectors / handlers:\n await handlePossibleOutcomes(page, [\n    {\n      selector: '#headerNavUserButton',\n      handler: element => {\n        console.log('Logged in',element);\n        loggedIn = true;\n        return true;\n      }\n    },\n    {\n      selector: '#email-login-password_error',\n      handler: element => {\n        console.log('password error',element);\n        return false;\n      }\n    }\n  ]).then(result => {\n    if (result) {\n      console.log('Logged in!',result);\n    } else {\n      console.log('Failed :(');\n    }\n  })\n\n"", ""\nI just started with Puppeteer, and have encountered the same issue, therefore I wanted to make a custom function which fulfills the same use-case.\nThe function goes as follows:\nasync function waitForMySelectors(selectors, page){\n    for (let i = 0; i < selectors.length; i++) {\n        await page.waitForSelector(selectors[i]);\n    }\n}\n\nThe first parameter in the function recieves an array of selectors, the second parameter is the page that we're inside to preform the waiting process with.\ncalling the function as the example below:\nvar SelectorsArray = ['#username', '#password'];\nawait waitForMySelectors(SelectorsArray, page);\n\nthough I have not preformed any tests on it yet, it seems functional.\n"", '\nIf you want to wait for the first of multiple selectors and get the matched element(s), you can start with waitForFunction:\nconst matches = await page.waitForFunction(() => {\n  const matches = [...document.querySelectorAll(YOUR_SELECTOR)];\n  return matches.length ? matches : null;\n});\n\nwaitForFunction will return an ElementHandle but not an array of them. If you only need native DOM methods, it\'s not necessary to get handles. For example, to get text from this array:\nconst contents = await matches.evaluate(els => els.map(e => e.textContent));\n\nIn other words, matches acts a lot like the array passed to $$eval by Puppeteer.\nOn the other hand, if you do need an array of handles, the following demonstration code makes the conversion and shows the handles being used as normal:\nconst puppeteer = require(""puppeteer""); // ^16.2.0\n\nconst html = `\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nh1 {\n  display: none;\n}\n</style>\n</head>\n<body>\n<script>\nsetTimeout(() => {\n\n  // add initial batch of 3 elements\n  for (let i = 0; i < 3; i++) {\n    const h1 = document.createElement(""button"");\n    h1.textContent = \\`first batch #\\${i + 1}\\`;\n    h1.addEventListener(""click"", () => {\n      h1.textContent = \\`#\\${i + 1} clicked\\`;\n    });\n    document.body.appendChild(h1);\n  }\n\n  // add another element 1 second later to show it won\'t appear in the first batch\n  setTimeout(() => {\n    const h1 = document.createElement(""h1"");\n    h1.textContent = ""this won\'t be found in the first batch"";\n    document.body.appendChild(h1);\n  }, 1000);\n\n}, 3000); // delay before first batch of elements are added\n</script>\n</body>\n</html>\n`;\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  await page.setContent(html);\n\n  const matches = await page.waitForFunction(() => {\n    const matches = [...document.querySelectorAll(""button"")];\n    return matches.length ? matches : null;\n  });\n  const length = await matches.evaluate(e => e.length);\n  const handles = await Promise.all([...Array(length)].map((e, i) =>\n    page.evaluateHandle((m, i) => m[i], matches, i)\n  ));\n  await handles[1].click(); // show that the handles work\n  const contents = await matches.evaluate(els => els.map(e => e.textContent));\n  console.log(contents);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close())\n;\n\nUnfortunately, it\'s a bit verbose, but this can be made into a helper.\nSee also Wait for first visible among multiple elements matching selector if you\'re interested in integrating the {visible: true} option.\n', ""\nPuppeteer methods might throw errors if they are unable to fufill a request. For example, page.waitForSelector(selector[, options]) might fail if the selector doesn't match any nodes during the given timeframe.\nFor certain types of errors Puppeteer uses specific error classes. These classes are available via require('puppeteer/Errors').\nList of supported classes:\nTimeoutError\nAn example of handling a timeout error:\nconst {TimeoutError} = require('puppeteer/Errors');\n\n// ...\n\ntry {\n  await page.waitForSelector('.foo');\n} catch (e) {\n  if (e instanceof TimeoutError) {\n    // Do something if this is a timeout.\n  }\n}\n\n""]",https://stackoverflow.com/questions/49946728/puppeteer-waitforselector-on-multiple-selectors,screen-scraping
"Scrapy, scraping data inside a Javascript","
I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:
How to get the values (text values) of such cases? 
This, is the site I'm trying to screen scrape:
https://www.mcdonalds.com.sg/locate-us/
Attributes I'm trying to get:
Address, Contact, Operating hours.
If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.

Edit
Sry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.
How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.
Here's my code so far
Items.py 
class McDonaldsItem(Item):
name = Field()
address = Field()
postal = Field()
hours = Field()

McDonalds.py
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
import re

from fastfood.items import McDonaldsItem

class McDonaldSpider(BaseSpider):
name = ""mcdonalds""
allowed_domains = [""mcdonalds.com.sg""]
start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]

def parse_json(self, response):

    js = json.loads(response.body)
    pprint.pprint(js)

Sry for long edit, so in short, how do i store the json value into my attribute? for eg
***item['address'] = * how to retrieve ****
P.S, not sure if this helps but, i run these scripts on the cmd line using
scrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )
I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this.
",21k,"
            23
        ","['\n(I posted this to scrapy-users mailing list but by Paul\'s suggestion I\'m posting it here as it complements the answer with the shell command interaction.)\nGenerally, websites that use a third party service to render some data visualization (map, table, etc) have to send the data somehow, and in most cases this data is accessible from the browser.\nFor this case, an inspection (i.e. exploring the requests made by the browser) shows that the data is loaded from a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\nSo, basically you have there all the data you want in a nice json format ready for consuming. \nScrapy provides the shell command which is very convenient to thinker with the website before writing the spider:\n$ scrapy shell https://www.mcdonalds.com.sg/locate-us/\n2013-09-27 00:44:14-0400 [scrapy] INFO: Scrapy 0.16.5 started (bot: scrapybot)\n...\n\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n\nIn [3]: payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n\nIn [4]: req = FormRequest(url, formdata=payload)\n\nIn [5]: fetch(req)\n2013-09-27 00:45:13-0400 [default] DEBUG: Crawled (200) <POST https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php> (referer: None)\n...\n\nIn [6]: import json\n\nIn [7]: data = json.loads(response.body)\n\nIn [8]: len(data[\'stores\'][\'listing\'])\nOut[8]: 127\n\nIn [9]: data[\'stores\'][\'listing\'][0]\nOut[9]: \n{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n u\'city\': u\'Singapore\',\n u\'id\': 78,\n u\'lat\': u\'1.440409\',\n u\'lon\': u\'103.801489\',\n u\'name\': u""McDonald\'s Admiralty"",\n u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n u\'phone\': u\'68940513\',\n u\'region\': u\'north\',\n u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n u\'zip\': u\'731678\'}\n\nIn short: in your spider you have to return the FormRequest(...) above, then in the callback load the json object from response.body and finally for each store\'s data in the list data[\'stores\'][\'listing\'] create an item with the wanted values.\nSomething like this:\nclass McDonaldSpider(BaseSpider):\n    name = ""mcdonalds""\n    allowed_domains = [""mcdonalds.com.sg""]\n    start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]\n\n    def parse(self, response):\n        # This receives the response from the start url. But we don\'t do anything with it.\n        url = \'https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php\'\n        payload = {\'action\': \'ws_search_store_location\', \'store_name\':\'0\', \'store_area\':\'0\', \'store_type\':\'0\'}\n        return FormRequest(url, formdata=payload, callback=self.parse_stores)\n\n    def parse_stores(self, response):\n        data = json.loads(response.body)\n        for store in data[\'stores\'][\'listing\']:\n            yield McDonaldsItem(name=store[\'name\'], address=store[\'address\'])\n\n', '\nWhen you open https://www.mcdonalds.com.sg/locate-us/ in your browser of choice, open up the ""inspect"" tool (hopefully it has one, e.g. Chrome or Firefox), and look for the ""Network"" tab.\nYou can further filter for ""XHR"" (XMLHttpRequest) events, and you\'ll see a POST request to https://www.mcdonalds.com.sg/wp-admin/admin-ajax.php with this body\naction=ws_search_store_location&store_name=0&store_area=0&store_type=0\n\nThe response to that POST request is a JSON object with all the information you want \nimport json\nimport pprint\n...\nclass MySpider(BaseSpider):\n...\n    def parse_json(self, response):\n\n        js = json.loads(response.body)\n        pprint.pprint(js)\n\nThis would output something like:\n{u\'flagicon\': u\'https://www.mcdonalds.com.sg/wp-content/themes/mcd/images/storeflag.png\',\n u\'stores\': {u\'listing\': [{u\'address\': u\'678A Woodlands Avenue 6<br/>#01-05<br/>Singapore 731678\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 78,\n                           u\'lat\': u\'1.440409\',\n                           u\'lon\': u\'103.801489\',\n                           u\'name\': u""McDonald\'s Admiralty"",\n                           u\'op_hours\': u\'24 hours<br>\\r\\nDessert Kiosk: 0900-0100\',\n                           u\'phone\': u\'68940513\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\', u\'dessert_kiosk\'],\n                           u\'zip\': u\'731678\'},\n                          {u\'address\': u\'383 Bukit Timah Road<br/>#01-09B<br/>Alocassia Apartments<br/>Singapore 259727\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 97,\n                           u\'lat\': u\'1.319752\',\n                           u\'lon\': u\'103.827398\',\n                           u\'name\': u""McDonald\'s Alocassia"",\n                           u\'op_hours\': u\'Daily: 0630-0100\',\n                           u\'phone\': u\'68874961\',\n                           u\'region\': u\'central\',\n                           u\'type\': [u\'24hrs_weekend\',\n                                     u\'drive_thru\',\n                                     u\'mccafe\'],\n                           u\'zip\': u\'259727\'},\n\n                        ...\n                          {u\'address\': u\'60 Yishuan Avenue 4 <br/>#01-11<br/><br/>Singapore 769027\',\n                           u\'city\': u\'Singapore\',\n                           u\'id\': 1036,\n                           u\'lat\': u\'1.423924\',\n                           u\'lon\': u\'103.840628\',\n                           u\'name\': u""McDonald\'s Yishun Safra"",\n                           u\'op_hours\': u\'24 hours\',\n                           u\'phone\': u\'67585632\',\n                           u\'region\': u\'north\',\n                           u\'type\': [u\'24hrs\',\n                                     u\'drive_thru\',\n                                     u\'live_screening\',\n                                     u\'mccafe\',\n                                     u\'bday_party\'],\n                           u\'zip\': u\'769027\'}],\n             u\'region\': u\'all\'}}\n\nI\'ll leave you to extract the fields you want.\nIn the FormRequest() you send with Scrapy you probably need to add a ""X-Requested-With: XMLHttpRequest"" header (your browser sends that if you look at the request headers in the inspect tool)\n']",https://stackoverflow.com/questions/19021541/scrapy-scraping-data-inside-a-javascript,screen-scraping
Scraping in Python - Preventing IP ban,"
I am using Python to scrape pages. Until now I didn't have any complicated issues.
The site that I'm trying to scrape uses a lot of security checks and have some mechanism to prevent scraping. 
Using Requests and lxml I was able to scrape about 100-150 pages before getting banned by IP. Sometimes I even get ban on first request (new IP, not used before, different C block). I have tried with spoofing headers, randomize time between requests, still the same.
I have tried with Selenium and I got much better results. With Selenium I was able to scrape about 600-650 pages before getting banned. Here I have also tried to randomize requests (between 3-5 seconds, and make time.sleep(300) call on every 300th request). Despite that, Im getting banned.
From here I can conclude that site have some mechanism where they ban IP if it requested more than X pages in one open browser session or something like that.
Based on your experience what else should I try?
Will closing and opening browser in Selenium help (for example after every 100th requests close and open browser). I was thinking about trying with proxies but there are about million of pages and it will be very expansive.
",36k,"
            19
        ","['\nIf you would switch to the Scrapy web-scraping framework, you would be able to reuse a number of things that were made to prevent and tackle banning:\n\nthe built-in AutoThrottle extension:\n\n\nThis is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling.\n\n\nrotating user agents with scrapy-fake-useragent middleware:\n\n\nUse a random User-Agent provided by fake-useragent every request\n\n\nrotating IP addresses:\n\nSetting Scrapy proxy middleware to rotate on each request\nscrapy-proxies\n\nyou can also run it via local proxy & TOR:\n\nScrapy: Run Using TOR and Multiple Agents\n\n\n', '\nI had this problem too. I used urllib with tor in python3.\n\ndownload and install tor browser\ntesting tor\n\nopen terminal and type:\ncurl --socks5-hostname localhost:9050 <http://site-that-blocked-you.com>\n\nif you see result it\'s worked.\n\nNow we should test in python. Now run this code\n\nimport socks\nimport socket\nfrom urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\n\n#set socks5 proxy to use tor\n\nsocks.set_default_proxy(socks.SOCKS5, ""localhost"", 9050)\nsocket.socket = socks.socksocket\nreq = Request(\'http://check.torproject.org\', headers={\'User-Agent\': \'Mozilla/5.0\', })\nhtml = urlopen(req).read()\nsoup = BeautifulSoup(html, \'html.parser\')\nprint(soup(\'title\')[0].get_text())\n\nif you see \n\nCongratulations. This browser is configured to use Tor. \n\nit worked in python too and this means you are using tor for web scraping.\n', '\nYou could use proxies.\nYou can buy several hundred IPs for very cheap, and use selenium as you previously have done.\nFurthermore I suggest varying the browser your use and other user-agent parameters.\nYou could iterate over using a single IP address to load only x number of pages and stopping prior to getting banned.\ndef load_proxy(PROXY_HOST,PROXY_PORT):\n        fp = webdriver.FirefoxProfile()\n        fp.set_preference(""network.proxy.type"", 1)\n        fp.set_preference(""network.proxy.http"",PROXY_HOST)\n        fp.set_preference(""network.proxy.http_port"",int(PROXY_PORT))\n        fp.set_preference(""general.useragent.override"",""whater_useragent"")\n        fp.update_preferences()\n        return webdriver.Firefox(firefox_profile=fp)\n\n']",https://stackoverflow.com/questions/35133200/scraping-in-python-preventing-ip-ban,screen-scraping
Maintaining cookies between Mechanize requests,"
I'm trying to use the Ruby version of Mechanize to extract my employer's tickets from a ticket management system that we're moving away from that does not supply an API.
Problem is, it seems Mechanize isn't keeping the cookies between the post call and the get call shown below:
require 'rubygems'
require 'nokogiri'
require 'mechanize'

@agent = Mechanize.new

page = @agent.post('http://<url>.com/user_session', {
                                            'authenticity_token' => '<token>',
                                            'user_session[login]' => '<login>',
                                            'user_session[password]' => '<password>',
                                            'user_session[remember_me]' => '0',
                                            'commit' => 'Login'
})

page = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'
puts page.title

user_session is the URL to which the site's login page POSTs, and I've verified that this indeed logs me in. But the page that returns from the get call is the 'Oops, you're not logged in!' page.
I've verified that clicking links on the page that returns from the post call works, but I can't actually get to where I need to go without JavaScript. And of course I've done this successfully on the browser with the same login.
What am I doing wrong?
",13k,"
            15
        ","[""\nOkay this might help you - first of all what version of mechanize are you using? You need to identify, if this problem is due to the cookies being overwritten/cleaned by mechanize between the requests or if the cookies are wrong/not being set in the first place. You can do that by adding a puts @agent.cookie_jar.jar inbetween the two requests, to see what is stored.\nIf its a overwriting issue, you might be able to solve it by collecting the cookies from the first request, and applying them to the second. There are many ways to do this:\nOne way is to just do a temp_jar = agent.cookie_jar.jar an then just going through each cookie and add it again using the .add method \nHOWEVER - the easiest way is by just installing the latest 2.1 pre release of mechanize (many fixes), because you will then be able to do it very simply.\nTo install the latest do a gem install mechanize --pre and make sure to get rid of the old version of mechanize gem uninstall mechanize 'some_version' after this, you can simply do as follows:\nrequire 'rubygems'\nrequire 'nokogiri'\nrequire 'mechanize'\n\n@agent = Mechanize.new\n\npage = @agent.post('http://<url>.com/user_session', {\n                                        'authenticity_token' => '<token>',\n                                        'user_session[login]' => '<login>',\n                                        'user_session[password]' => '<password>',\n                                        'user_session[remember_me]' => '0',\n                                        'commit' => 'Login'\n})\ntemp_jar = @agent.cookie_jar\n#Do whatever you need an use the cookies again in a new session after that\n@agent = Mechanize.new\n@agent.cookie_jar = temp_jar\n\npage = @agent.get 'http://<url>.com/<organization>/<repo-name>/tickets/1'\nputs page.title\n\nBTW the documentation is here http://mechanize.rubyforge.org/index.html\n"", ""\nMechanize would automatically send cookies obtained from the response in the consecutive request. You can use the same agent without re-new.\nrequire 'mechanize'\n\n@agent = Mechanize.new\n@agent.post(create_sessions_url, params, headers)\n@agent.get(ticket_url)\n\nTested with mechanize 2.7.6.\n""]",https://stackoverflow.com/questions/7046535/maintaining-cookies-between-mechanize-requests,screen-scraping
Screen-scraping a windows application in c# [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 2 years ago.







                        Improve this question
                    



I need to scrape data from a windows application to run a query in another program. Does anyone know of a good starting point for me to do this in .NET?
",29k,"
            13
        ","[""\nYou may want to look into the WM_GETTEXT message. This can be used to read text from other windows -- it's an archaic part of the Windows API, and if you're in C#, you'll need to p/invoke for it. \nCheck out this page for an example of doing this in C#. \nBasically, you first FindControlEx() to get the handle of the window that you want (by caption).\nSecond, you recursively enumerate the controls on that window with EnumChildWindows() to find all of the window's child controls, and all of those children's children until you have a complete map of the target form. \nHere is a selected portion of Theta-ga's excellent explanation from Google Answers:\nTo get the contents of any textbox or listbox control, all we need is it's window handle.  If you have already obtained the window handle then move to part 2 of the explaination.\nPART 1: Obtaining the control handle\n\nTo obtain the handle of a control, we first obtain the handle of it?s parent window. We can do this by using the Win32 FindControlEx() method. This method takes in the window caption (such as 'Calculator') and/or its class name, and return its handle.\nOnce we have the parent window handle, we can call the Win32 EnumChildWindows method. This method takes in a callback method, which it calls with the handle of every child control it finds for the specified parent. For eg., if we call this method with the handle of the Calculator window, it will call the callback method with the handle of the textbox control, and then again with the handles of each of the buttons on the Calculator window, and so on.\nSince we are only interested in the handle of the textbox control, we can check the class of the window in the callback method. The Win32 method GetClassName() can be used for this. This method takes in a window handle and provides us with a string containing the class name. So a textbox belongs to the ?Edit? class, a listbox to the 'ListBox' class and so on. Once you have determined that you have the handle for the right control, you can read its contents.\n\nPART 2: Reading the contents of a control\n\nYou can read in the contents of a control by using the Win32 SendMessage() function, and using it to pass the WM_GETTEXT message to the target control. This will give you the text content of the control. This method will work for a textbox, button, or static control.\nHowever, the above approach will fail if you try to read the contents of a listbox. To get the contents of a listbox, we need to first use SendMessage() with the LB_GETCOUNT message to get the count of list items. Then we need to call SendMessage() with the LB_GETTEXT message for each item in the list.\n\n""]",https://stackoverflow.com/questions/375117/screen-scraping-a-windows-application-in-c-sharp,screen-scraping
Selenium Scroll inside of popup div,"
I am using selenium and trying to scroll inside the popup div on instagram.
I get to a page like 'https://www.instagram.com/kimkardashian/', click followers, and then I can't get the followers list to scroll down.
I tried using hover, click_and_hold, and a few other tricks to select the div but none of them worked.
What would the best way be to get this selected?
This is what I tried so far:
driver.find_elements_by_xpath(""//*[contains(text(), 'followers')]"")[0].click()
element_to_hover_over = driver.find_elements_by_xpath(""//*[contains(text(), 'Follow')]"")[12]
hover = ActionChains(webdriver).move_to_element(element_to_hover_over)
hover.click_and_hold()
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")

",23k,"
            11
        ","['\nThe exact code is as follow. You first have to find the new iframe which contains the name of followers:\nscr1 = driver.find_element_by_xpath(\'/html/body/div[2]/div/div[2]/div/div[2]\')\ndriver.execute_script(""arguments[0].scrollTop = arguments[0].scrollHeight"", scr1)\n\nThis will automatically scroll down the page but you have make a for loop for it until it reaches to the end of page. You can see my Instagram crawler here.\n', '\nYou would need to use jQuery to execute a function on the div.  Here\'s the way I figured to do it.  It was easier to solve it with jQuery and execute a script than handle it with the api.\nheight = 2000\nquery = \'jQuery(""div"").filter((i, div) => jQuery(div).css(""overflow-y"") == ""scroll"")[0].scrollTop = %s\' %height\ndriver.execute_script(query)\n\n']",https://stackoverflow.com/questions/38041974/selenium-scroll-inside-of-popup-div,screen-scraping
Pass the user-agent through webdriver in Selenium,"
I am working on a website scraping project using Selenium in Python. When I open the homepage through a browser, it opens properly.
But, when I try to open the webpage through webdriver() in Selenium, it opens a completely different page.
I think, it is able to detect the user-agent( not sure what it is called) and is able to check the properties of the browser or something.
Is it possible to pass the properties though the webdriver() so that the right homepage is loaded.
Thanks
",13k,"
            10
        ","['\nChanging the user agent in the python version of webdriver is done by altering your browser\'s profile. I have only done this for webdriver.Firefox() by passing a profile parameter. You need to do the following:\nfrom selenium import webdriver\nprofile = webdriver.FirefoxProfile()\nprofile.set_preference(""general.useragent.override"",""your_user_agent_string"")\ndriver=webdriver.Firefox(profile)\n\nEvery time you wish to change the user agent you will need to restart your web browser (i.e. call driver=webdriver.Firefox(profile) again)\nIf you are unsure to what your user agent string is do a search for ""what is my user agent"" on a browser that displays the page properly and just copy and paste that one.\nHope that sorts it.\n', '\nAssuming the user-agent is the problem, in Java you can modify it like this:\nFirefoxProfile profile = new FirefoxProfile();\nprofile.addAdditionalPreference(""general.useragent.override"", ""some UA string"");\nWebDriver driver = new FirefoxDriver(profile);\n\nSee documentation here.\n']",https://stackoverflow.com/questions/8286127/pass-the-user-agent-through-webdriver-in-selenium,screen-scraping
Save all image files from a website,"
I'm creating a small app for myself where I run a Ruby script and save all of the images off of my blog.
I can't figure out how to save the image files after I've identified them. Any help would be much appreciated.
require 'rubygems'
require 'nokogiri'
require 'open-uri'

url = '[my blog url]'
doc = Nokogiri::HTML(open(url))

doc.css(""img"").each do |item|
  #something
end

",11k,"
            8
        ","['\nURL = \'[my blog url]\'\n\nrequire \'nokogiri\' # gem install nokogiri\nrequire \'open-uri\' # already part of your ruby install\n\nNokogiri::HTML(open(URL)).xpath(""//img/@src"").each do |src|\n  uri = URI.join( URL, src ).to_s # make absolute uri\n  File.open(File.basename(uri),\'wb\'){ |f| f.write(open(uri).read) }\nend\n\nUsing the code to convert to absolute paths from here: How can I get the absolute URL when extracting links using Nokogiri?\n', ""\nassuming the src attribute is an absolute url, maybe something like:\nif item['src'] =~ /([^\\/]+)$/\n    File.open($1, 'wb') {|f| f.write(open(item['src']).read)}\nend\n\n"", ""\nTip: there's a simple way to get images from a page's head/body using the Scrapifier gem. The cool thing is that you can also define which type of image you want it to be returned (jpg, png, gif). \nGive it a try: https://github.com/tiagopog/scrapifier\nHope you enjoy.\n"", ""\nsystem %x{ wget #{item['src']} }\n\nEdit: This is assuming you're on a unix system with wget :)\nEdit 2: Updated code for grabbing the img src from nokogiri.\n""]",https://stackoverflow.com/questions/7926675/save-all-image-files-from-a-website,screen-scraping
web scraping to fill out (and retrieve) search forms?,"
I was wondering if it is possible to ""automate"" the task of typing in entries to search forms and extracting matches from the results. For instance, I have a list of journal articles for which I would like to get DOI's (digital object identifier); manually for this I would go to the journal articles search page (e.g., http://pubs.acs.org/search/advanced), type in the authors/title/volume (etc.) and then find the article out of its list of returned results, and pick out the DOI and paste that into my reference list. I use R and Python for data analysis regularly (I was inspired by a post on RCurl) but don't know much about web protocols... is such a thing possible (for instance using something like Python's BeautifulSoup?). Are there any good references for doing anything remotely similar to this task? I'm just as much interested in learning about web scraping and tools for web scraping in general as much as getting this particular task done... Thanks for your time!
",14k,"
            8
        ","['\nBeautiful Soup is great for parsing webpages- that\'s half of what you want to do.  Python, Perl, and Ruby all have a version of Mechanize, and that\'s the other half:\nhttp://wwwsearch.sourceforge.net/mechanize/\nMechanize let\'s you control a browser:\n# Follow a link\nbrowser.follow_link(link_node)\n\n# Submit a form\nbrowser.select_form(name=""search"")\nbrowser[""authors""] = [""author #1"", ""author #2""]\nbrowser[""volume""] = ""any""\nsearch_response = br.submit()\n\nWith Mechanize and Beautiful Soup you have a great start.  One extra tool I\'d consider is Firebug, as used in this quick ruby scraping guide:\nhttp://www.igvita.com/2007/02/04/ruby-screen-scraper-in-60-seconds/\nFirebug can speed your construction of xpaths for parsing documents, saving you some serious time.\nGood luck!\n', '\nPython Code: for search forms.\n# import \nfrom selenium import webdriver\n\nfrom selenium.common.exceptions import TimeoutException\n\nfrom selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0\n\nfrom selenium.webdriver.support import expected_conditions as EC # available since 2.26.0\n\n# Create a new instance of the Firefox driver\ndriver = webdriver.Firefox()\n\n# go to the google home page\ndriver.get(""http://www.google.com"")\n\n# the page is ajaxy so the title is originally this:\nprint driver.title\n\n# find the element that\'s name attribute is q (the google search box)\ninputElement = driver.find_element_by_name(""q"")\n\n# type in the search\ninputElement.send_keys(""cheese!"")\n\n# submit the form (although google automatically searches now without submitting)\ninputElement.submit()\n\ntry:\n    # we have to wait for the page to refresh, the last thing that seems to be updated is the title\n    WebDriverWait(driver, 10).until(EC.title_contains(""cheese!""))\n\n    # You should see ""cheese! - Google Search""\n    print driver.title\n\nfinally:\n    driver.quit()\n\nSource: https://www.seleniumhq.org/docs/03_webdriver.jsp\n', '\nWebRequest req = WebRequest.Create(""http://www.URLacceptingPOSTparams.com"");\n\nreq.Proxy = null;\nreq.Method = ""POST"";\nreq.ContentType = ""application/x-www-form-urlencoded"";\n\n//\n// add POST data\nstring reqString = ""searchtextbox=webclient&searchmode=simple&OtherParam=???"";\nbyte[] reqData = Encoding.UTF8.GetBytes (reqString);\nreq.ContentLength = reqData.Length;\n//\n// send request\nusing (Stream reqStream = req.GetRequestStream())\n  reqStream.Write (reqData, 0, reqData.Length);\n\nstring response;\n//\n// retrieve response\nusing (WebResponse res = req.GetResponse())\nusing (Stream resSteam = res.GetResponseStream())\nusing (StreamReader sr = new StreamReader (resSteam))\n  response = sr.ReadToEnd();\n\n// use a regular expression to break apart response\n// OR you could load the HTML response page as a DOM \n\n(Adapted from Joe Albahri\'s ""C# in a nutshell"")\n', '\nThere are many tools for web scraping. There is a good firefox plugin called iMacros. It works great and needs no programming knowledge at all. The free version can be downloaded from here:\nhttps://addons.mozilla.org/en-US/firefox/addon/imacros-for-firefox/\nThe best thing about iMacros, is that it can get you started in minutes, and it can also be launched from the bash command line, and can also be called from within bash scripts.\nA more advanced step would be selenium webdrive. The reason I chose selenium is that it is documented in a great way suiting beginners. reading just the following page:\nwould get you upand running in no time.\nSelenium supports java, python, php , c so if you are familiar with any of these languages, you would be familiar with all the commands needed. I prefer webdrive variation of selenium, as it opens a browser, so that you can check the fields and outputs. After setting up the script using webdrive, you can easily migrate the script to IDE, thus running headless.\nTo install selenium you can do by typing the command\nsudo easy_install selenium\n\nThis will take care of the dependencies and everything needed for you.\nIn order to run your script interactively, just open a terminal, and type \npython\n\nyou will see the python prompt, >>> and you can type in the commands.\nHere is a sample code which you can paste in the terminal, it will search google for the word cheeses\npackage org.openqa.selenium.example;\n\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.WebElement;\nimport org.openqa.selenium.firefox.FirefoxDriver;\nimport org.openqa.selenium.support.ui.ExpectedCondition;\nimport org.openqa.selenium.support.ui.WebDriverWait;\n\npublic class Selenium2Example  {\n    public static void main(String[] args) {\n        // Create a new instance of the Firefox driver\n        // Notice that the remainder of the code relies on the interface, \n        // not the implementation.\n        WebDriver driver = new FirefoxDriver();\n\n        // And now use this to visit Google\n        driver.get(""http://www.google.com"");\n        // Alternatively the same thing can be done like this\n        // driver.navigate().to(""http://www.google.com"");\n\n        // Find the text input element by its name\n        WebElement element = driver.findElement(By.name(""q""));\n\n        // Enter something to search for\n        element.sendKeys(""Cheese!"");\n\n        // Now submit the form. WebDriver will find the form for us from the element\n        element.submit();\n\n        // Check the title of the page\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        // Google\'s search is rendered dynamically with JavaScript.\n        // Wait for the page to load, timeout after 10 seconds\n        (new WebDriverWait(driver, 10)).until(new ExpectedCondition<Boolean>() {\n            public Boolean apply(WebDriver d) {\n                return d.getTitle().toLowerCase().startsWith(""cheese!"");\n            }\n        });\n\n        // Should see: ""cheese! - Google Search""\n        System.out.println(""Page title is: "" + driver.getTitle());\n\n        //Close the browser\n        driver.quit();\n    }}\n\nI hope that this can give you a head start.\nCheers :)\n']",https://stackoverflow.com/questions/1170120/web-scraping-to-fill-out-and-retrieve-search-forms,screen-scraping
Python web scraping involving HTML tags with attributes,"
I'm trying to make a web scraper that will parse a web-page of publications and extract the authors. The skeletal structure of the web-page is the following:
<html>
<body>
<div id=""container"">
<div id=""contents"">
<table>
<tbody>
<tr>
<td class=""author"">####I want whatever is located here ###</td>
</tr>
</tbody>
</table>
</div>
</div>
</body>
</html>

I've been trying to use BeautifulSoup and lxml thus far to accomplish this task, but I'm not sure how to handle the two div tags and td tag because they have attributes. In addition to this, I'm not sure whether I should rely more on BeautifulSoup or lxml or a combination of both. What should I do?
At the moment, my code looks like what is below:
    import re
    import urllib2,sys
    import lxml
    from lxml import etree
    from lxml.html.soupparser import fromstring
    from lxml.etree import tostring
    from lxml.cssselect import CSSSelector
    from BeautifulSoup import BeautifulSoup, NavigableString

    address='http://www.example.com/'
    html = urllib2.urlopen(address).read()
    soup = BeautifulSoup(html)
    html=soup.prettify()
    html=html.replace('&nbsp', '&#160')
    html=html.replace('&iacute','&#237')
    root=fromstring(html)

I realize that a lot of the import statements may be redundant, but I just copied whatever I currently had in more source file.
EDIT: I suppose that I didn't make this quite clear, but I have multiple  tags in page that I want to scrape. 
",9k,"
            8
        ","[""\nIt's not clear to me from your question why you need to worry about the div tags -- what about doing just:\nsoup = BeautifulSoup(html)\nthetd = soup.find('td', attrs={'class': 'author'})\nprint thetd.string\n\nOn the HTML you give, running this emits exactly:\n####I want whatever is located here ###\n\nwhich appears to be what you want.  Maybe you can specify better exactly what it is you need and this super-simple snippet doesn't do -- multiple td tags all of class author of which you need to consider (all? just some? which ones?), possibly missing any such tag (what do you want to do in that case), and the like. It's hard to infer what exactly are your specs, just from this simple example and overabundant code;-).\nEdit: if, as per the OP's latest comment, there are multiple such td tags, one per author:\nthetds = soup.findAll('td', attrs={'class': 'author'})\nfor thetd in thetds:\n    print thetd.string\n\n...i.e., not much harder at all!-)\n"", ""\nor you could be using pyquery, since BeautifulSoup is not actively maintained anymore, see http://www.crummy.com/software/BeautifulSoup/3.1-problems.html\nfirst, install pyquery with \neasy_install pyquery\n\nthen your script could be as simple as\nfrom pyquery import PyQuery\nd = PyQuery('http://mywebpage/')\nallauthors = [ td.text() for td in d('td.author') ]\n\npyquery uses the css selector syntax familiar from jQuery which I find more intuitive than BeautifulSoup's. It uses lxml underneath, and is much faster than BeautifulSoup. But BeautifulSoup is pure python, and thus works on Google's app engine as well\n"", '\nThe lxml library is now the standard for parsing html in python.  The interface can seem awkward at first, but it is very serviceable for what it does.  \nYou should let the libary handle the xml specialism, such as those escaped &entities;\nimport lxml.html\n\nhtml = """"""<html><body><div id=""container""><div id=""contents""><table><tbody><tr>\n          <td class=""author"">####I want whatever is located here, eh? &iacute; ###</td>\n          </tr></tbody></table></div></div></body></html>""""""\n\nroot = lxml.html.fromstring(html)\ntds = root.cssselect(""div#contents td.author"")\n\nprint tds           # gives [<Element td at 84ee2cc>]\nprint tds[0].text   # what you want, including the \'í\'\n\n', '\nBeautifulSoup is certainly the canonical HTML parser/processor.  But if you have just this kind of snippet you need to match, instead of building a whole hierarchical object representing the HTML, pyparsing makes it easy to define leading and trailing HTML tags as part of creating a larger search expression:\nfrom pyparsing import makeHTMLTags, withAttribute, SkipTo\n\nauthor_td, end_td = makeHTMLTags(""td"")\n\n# only interested in <td>\'s where class=""author""\nauthor_td.setParseAction(withAttribute((""class"",""author"")))\n\nsearch = author_td + SkipTo(end_td)(""body"") + end_td\n\nfor match in search.searchString(html):\n    print match.body\n\nPyparsing\'s makeHTMLTags function does a lot more than just emit ""<tag>"" and ""</tag>"" expressions.  It also handles:\n\ncaseless matching of tags\n""<tag/>"" syntax\nzero or more attribute in the opening tag\nattributes defined in arbitrary order\nattribute names with namespaces\nattribute values in single, double, or no quotes\nintervening whitespace between tag and symbols, or attribute name, \'=\', and value\nattributes are accessible after parsing as named results\n\nThese are the common pitfalls when considering using a regex for HTML scraping.\n']",https://stackoverflow.com/questions/1391657/python-web-scraping-involving-html-tags-with-attributes,screen-scraping
BeautifulSoup get_text does not strip all tags and JavaScript,"
I am trying to use BeautifulSoup to get text from web pages.
Below is a script I've written to do so. It takes two arguments, first is the input HTML or XML file, the second output file.
import sys
from bs4 import BeautifulSoup

def stripTags(s): return BeautifulSoup(s).get_text()

def stripTagsFromFile(inFile, outFile):
    open(outFile, 'w').write(stripTags(open(inFile).read()).encode(""utf-8""))

def main(argv):
    if len(sys.argv) <> 3:
        print 'Usage:\t\t', sys.argv[0], 'input.html output.txt'
        return 1
    stripTagsFromFile(sys.argv[1], sys.argv[2])
    return 0

if __name__ == ""__main__"":
    sys.exit(main(sys.argv))

Unfortunately, for many web pages, for example: http://www.greatjobsinteaching.co.uk/career/134112/Education-Manager-Location
I get something like this (I'm showing only few first lines):
html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""
    Education Manager  Job In London With  Caleeda | Great Jobs In Teaching

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-15255540-21']);
_gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);

Is there anything wrong with my script? I was trying to pass 'xml' as the second argument to BeautifulSoup's constructor, as well as 'html5lib' and 'lxml', but it doesn't help.
Is there an alternative to BeautifulSoup which would work better for this task? All I want is to extract the text which would be rendered in a browser for this web page.
Any help will be much appreciated. 
",15k,"
            7
        ","[""\nnltk's clean_html() is quite good at this!\nAssuming that your already have your html stored in a variable html like\nhtml = urllib.urlopen(address).read()\n\nthen just use\nimport nltk\nclean_text = nltk.clean_html(html)\n\nUPDATE\nSupport for clean_html and clean_url will be dropped for future versions of nltk. Please use BeautifulSoup for now...it's very unfortunate.\nAn example on how to achieve this is on this page:\nBeatifulSoup4 get_text still has javascript\n"", ""\nHere's an approach which is based on the answer here: BeautifulSoup Grab Visible Webpage Text by jbochi. This approach allows for comments embedded in elements containing page text, and does a bit to clean up the output by stripping newlines, consolidating space, etc.\nhtml = urllib.urlopen(address).read()\nsoup = BeautifulSoup.BeautifulSoup(html)\ntexts = soup.findAll(text=True)\n\ndef visible_text(element):\n    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n        return ''\n    result = re.sub('<!--.*-->|\\r|\\n', '', str(element), flags=re.DOTALL)\n    result = re.sub('\\s{2,}|&nbsp;', ' ', result)\n    return result\n\nvisible_elements = [visible_text(elem) for elem in texts]\nvisible_text = ''.join(visible_elements)\nprint(visible_text)\n\n"", ""\nThis was the problem I was having.  no solution seemed to be able to return the text (the text that would actually be rendered in the web broswer).  Other solutions mentioned that BS is not ideal for rendering and that html2text was a good approach.  I tried both html2text and nltk.clean_html and was surprised by the timing results so thought they warranted an answer for posterity.  Of course, the speed delta might highly depend on the contents of the data...\nOne answer here from @Helge was about using nltk of all things.  \nimport nltk\n\n%timeit nltk.clean_html(html)\nwas returning 153 us per loop\n\nIt worked really well to return a string with rendered html.  This nltk module was faster than even html2text, though perhaps html2text is more robust. \nbetterHTML = html.decode(errors='ignore')\n%timeit html2text.html2text(betterHTML)\n%3.09 ms per loop\n\n""]",https://stackoverflow.com/questions/10524387/beautifulsoup-get-text-does-not-strip-all-tags-and-javascript,screen-scraping
How can I screen scrape with Perl?,"
I need to display some values that are stored in a website, for that I need to scrape the website and fetch the content from the table. Any ideas?
",16k,"
            7
        ","['\nIf you are familiar with jQuery you might want to check out pQuery, which makes this very easy:\n## print every <h2> tag in page\nuse pQuery;\n\npQuery(""http://google.com/search?q=pquery"")\n    ->find(""h2"")\n    ->each(sub {\n        my $i = shift;\n        print $i + 1, "") "", pQuery($_)->text, ""\\n"";\n    });\n\nThere\'s also HTML::DOM.\nWhatever you do, though, don\'t use regular expressions for this.\n', '\nI have used HTML Table Extract in the past.\nI personally find it a bit clumsy to use, but maybe I did not understand the object model well.\nI usually use this part of the manual to examine the data:\n use HTML::TableExtract;\n $te = HTML::TableExtract->new();\n $te->parse($html_string);\n\n     # Examine all matching tables\n     foreach $ts ($te->tables) {\n       print ""Table ("", join(\',\', $ts->coords), ""):\\n"";\n       foreach $row ($ts->rows) {\n          print join(\',\', @$row), ""\\n"";\n       }\n     }`\n\n', ""\nAlthough I've generally done this with LWP/LWP::Simple, the current 'preferred' module for any sort of webpage scraping in Perl is WWW::Mechanize.\n"", ""\nIf you're familiar with XPath, you can also use HTML::TreeBuilder::XPath. And if you're not... well you should be ;--)\n"", '\nYou could also use this simple perl module WEB::Scraper, this is simple to understand and make life easy for me. follow this example for more information.   \nhttp://teusje.wordpress.com/2010/05/02/web-scraping-with-perl/\n', '\nFor similar Stackoverflow questions have a look at....\n\nHow can I extract URLs from a web page in Perl\nHow can I extract XML of a website and save in a file using Perl’s LWP?\n\nI do like using pQuery for things like this however Web::Scraper does look interesting.\n', ""\nI don't mean to drag up a dead thread but anyone googling across this thread should also checkout WWW::Scripter - 'For scripting web sites that have scripts'\nhappy remote data aggregating ;)\n"", ""\nTake a look at the magical Web::Scraper, it's THE tool for web scraping.\n"", '\nI use LWP::UserAgent for most of my screen scraping needs. You can also Couple that with HTTP::Cookies if you need Cookies support.\nHere\'s a simple example on how to get source.\nuse LWP;\nuse HTTP::Cookies;\nmy $cookie_jar = HTTP::Cookies->new;\nmy $browser = LWP::UserAgent->new;\n$browser->cookie_jar($cookie_jar);\n\n$resp = $browser->get(""https://www.stackoverflow.com"");\nif($resp->is_success) {\n   # Play with your source here\n   $source = $resp->content;\n   $source =~ s/^.*<table>/<table>/i; # this is just an example \n   print $source;                     # not a solution to your problem.\n}\n\n', '\nCheck out this little example of web scraping with perl:\nlink text\n']",https://stackoverflow.com/questions/713827/how-can-i-screen-scrape-with-perl,screen-scraping
Using Nokogiri to Split Content on BR tags,"
I have a snippet of code im trying to parse with nokogiri that looks like this:
<td class=""j"">
    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>
    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>
</td>

I have access to the source of the td.j using something like this:
data_items = doc.css(""td.j"")
My goal is to split each of those lines up into an array of hashes.  The only logical splitting point i can see is to split on the BRs and then use some regex on the string.  
I was wondering if there's a Better way to do this maybe using nokogiri only?  Even if i could use nokogiri to suck out the 3 line items it would make things easier for me as i could just do some regex parsing on the .content result. 
Not sure how to use Nokogiri to grab lines ending with br though -- should i be using xpaths? any direction is appreciated! thank you
",4k,"
            6
        ","['\nI\'m not sure about the point of using an array of hashes, and without an example I can\'t suggest something. However, for splitting the text on <br> tags, I\'d go about it this way:\nrequire \'nokogiri\'\n\ndoc = Nokogiri::HTML(\'<td class=""j"">\n    <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n    <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n</td>\')\n\ndoc.search(\'br\').each do |n|\n  n.replace(""\\n"")\nend\ndoc.at(\'tr.j\').text.split(""\\n"") # => ["""", ""    Link 1 (info1), Blah 1,"", ""Link 2 (info1), Blah 1,"", ""Link 3 (info2), Blah 1 Foo 2,""]\n\nThis will get you closer to a hash:\nHash[*doc.at(\'td.j\').text.split(""\\n"")[1 .. -1].map{ |t| t.strip.split(\',\')[0 .. 1] }.flatten] # => {""Link 1 (info1)""=>"" Blah 1"", ""Link 2 (info1)""=>"" Blah 1"", ""Link 3 (info2)""=>"" Blah 1 Foo 2""}\n\n', '\nIf your data really is that regular and you don\'t need the attributes from the <a> elements, then you could parse the text form of each table cell without having to worry about the <br> elements at all.\nGiven some HTML like this in html:\n<table>\n    <tbody>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link1.com"">Link 1</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link2.com"">Link 2</a> (info1), Blah 1,<br>\n                <a title=""title text2"" href=""http://link3.com"">Link 3</a> (info2), Blah 1 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link4.com"">Link 4</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link5.com"">Link 5</a> (info1), Blah 2,<br>\n                <a title=""title text2"" href=""http://link6.com"">Link 6</a> (info2), Blah 2 Foo 2,<br>\n            </td>\n        </tr>\n        <tr>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://link7.com"">Link 7</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link8.com"">Link 8</a> (info1), Blah 3,<br>\n                <a title=""title text2"" href=""http://link9.com"">Link 9</a> (info2), Blah 3 Foo 2,<br>\n            </td>\n            <td class=""j"">\n                <a title=""title text1"" href=""http://linkA.com"">Link A</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkB.com"">Link B</a> (info1), Blah 4,<br>\n                <a title=""title text2"" href=""http://linkC.com"">Link C</a> (info2), Blah 4 Foo 2,<br>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\nYou could do this:\nchunks = doc.search(\'.j\').map { |td| td.text.strip.scan(/[^,]+,[^,]+/) }\n\nand have this:\n[\n    [ ""Link 1 (info1), Blah 1"", ""Link 2 (info1), Blah 1"", ""Link 3 (info2), Blah 1 Foo 2"" ],\n    [ ""Link 4 (info1), Blah 2"", ""Link 5 (info1), Blah 2"", ""Link 6 (info2), Blah 2 Foo 2"" ],\n    [ ""Link 7 (info1), Blah 3"", ""Link 8 (info1), Blah 3"", ""Link 9 (info2), Blah 3 Foo 2"" ],\n    [ ""Link A (info1), Blah 4"", ""Link B (info1), Blah 4"", ""Link C (info2), Blah 4 Foo 2"" ]\n]\n\nin chunks. Then you could convert that to whatever hash form you needed.\n']",https://stackoverflow.com/questions/7058922/using-nokogiri-to-split-content-on-br-tags,screen-scraping
Why does scrapy throw an error for me when trying to spider and parse a site?,"
The following code
class SiteSpider(BaseSpider):
    name = ""some_site.com""
    allowed_domains = [""some_site.com""]
    start_urls = [
        ""some_site.com/something/another/PRODUCT-CATEGORY1_10652_-1__85667"",
    ]
    rules = (
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-CATEGORY_(.*)', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(SgmlLinkExtractor(allow=('some_site.com/something/another/PRODUCT-DETAIL(.*)', )), callback=""parse_item""),
    )
    def parse_item(self, response):
.... parse stuff

Throws the following error
Traceback (most recent call last):
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 1174, in mainLoop
    self.runUntilCurrent()
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/base.py"", line 796, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 318, in callback
    self._startRunCallbacks(result)
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 424, in _startRunCallbacks
    self._runCallbacks()
--- <exception caught here> ---
  File ""/usr/lib/python2.6/dist-packages/twisted/internet/defer.py"", line 441, in _runCallbacks
    self.result = callback(self.result, *args, **kw)
  File ""/usr/lib/pymodules/python2.6/scrapy/spider.py"", line 62, in parse
    raise NotImplementedError
exceptions.NotImplementedError: 

When I change the callback to ""parse"" and the function to ""parse"" i don't get any errors, but nothing is scraped. I changed it to ""parse_items"" thinking I might be overriding the parse method by accident. Perhaps I'm setting up the link extractor wrong?
What I want to do is parse each ITEM link on the CATEGORY page. Am I doing this totally wrong?
",9k,"
            6
        ","['\nI needed to change BaseSpider to CrawlSpider. Thanks srapy users!\nhttp://groups.google.com/group/scrapy-users/browse_thread/thread/4adaba51f7bcd0af#\n\nHi Bob,\nPerhaps it might work if you change\n  from BaseSpider to CrawlSpider? The\n  BaseSpider seems not implement Rule,\n  see:\nhttp://doc.scrapy.org/topics/spiders.html?highlight=rule#scrapy.contr...\n-M\n\n', '\nBy default scrapy searches for parse function in the class. Here in your spider, parse function is missing. Instead of parse you have given parse_item. The problem will be solved if parse_item is replace with parse.\nOr you can override the parse method in spider.py with that of parse_item.\n']",https://stackoverflow.com/questions/5264829/why-does-scrapy-throw-an-error-for-me-when-trying-to-spider-and-parse-a-site,screen-scraping
"Issue scraping page with ""Load more"" button with rvest","
I want to obtain the links to the atms listed on this page: https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/
Would I need to do something about the 'load more' button at the bottom of the page?
I have been using the selector tool you can download for chrome to select the CSS path. 
I've written the below code block and it only seems to retrieve the first ten links. 
library(rvest)

base <- ""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/""
base_read <- read_html(base)
atm_urls <- html_nodes(base_read, "".place > a"")
all_urls_final <- html_attr(atm_urls, ""href"" )
print(all_urls_final)

I expected to be able to retrieve all links to the atms listed in the area but my R code has not done so.
Any help would be great. Sorry if this is a really simple question.
",2k,"
            6
        ","['\nYou should give RSelenium a try. I\'m able to get the links with the following code:\n# install.packages(""RSelenium"")\nlibrary(RSelenium)\nlibrary(rvest)\n\n# Download binaries, start driver, and get client object.\nrd <- rsDriver(browser = ""firefox"", port = 4444L)\nffd <- rd$client\n\n# Navigate to page.\nffd$navigate(""https://coinatmradar.com/city/345/bitcoin-atm-birmingham-uk/"")\n\n# Find the load button and assign, then send click event.\nload_btn <- ffd$findElement(using = ""css selector"", "".load-more .btn"")\nload_btn$clickElement()\n\n# Wait for elements to load.\nSys.sleep(2)\n\n# Get HTML data and parse\nhtml_data <- ffd$getPageSource()[[1]]\nhtml_data %>% \n    read_html() %>% \n    html_nodes("".place a:not(.operator-link)"") %>% \n    html_attr(""href"")\n\n#### OUTPUT ####\n\n#  [1] ""/bitcoin_atm/5969/bitcoin-atm-shitcoins-club-birmingham-uk-bitcoin-embassy/""                   \n#  [2] ""/bitcoin_atm/7105/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n#  [3] ""/bitcoin_atm/4759/bitcoin-atm-general-bytes-birmingham-uk-costcutter/""                         \n#  [4] ""/bitcoin_atm/2533/bitcoin-atm-general-bytes-birmingham-uk-londis-# convenience/""                 \n#  [5] ""/bitcoin_atm/5458/bitcoin-atm-general-bytes-coventry-agg-african-restaurant/""                  \n#  [6] ""/bitcoin_atm/711/bitcoin-atm-general-bytes-coventry-bigs-barbers/""                             \n#  [7] ""/bitcoin_atm/5830/bitcoin-atm-general-bytes-telford-bpred-lion-service-station/""               \n#  [8] ""/bitcoin_atm/5466/bitcoin-atm-general-bytes-nottingham-24-express-off-licence/""                \n#  [9] ""/bitcoin_atm/4615/bitcoin-atm-general-bytes-northampton-costcutter/""                           \n# [10] ""/bitcoin_atm/4841/bitcoin-atm-lamassu-worcester-computer-house/""                               \n# [11] ""/bitcoin_atm/3150/bitcoin-atm-bitxatm-leicester-keshs-wines-and-newsagents-braustone/""         \n# [12] ""/bitcoin_atm/2948/bitcoin-atm-bitxatm-coventry-nisa-local/""                                    \n# [13] ""/bitcoin_atm/4742/bitcoin-atm-bitxatm-birmingham-uk-custcutter-coventry-road-hay-mills/""       \n# [14] ""/bitcoin_atm/4741/bitcoin-atm-bitxatm-derby-michaels-drink-store-alvaston/""                    \n# [15] ""/bitcoin_atm/4740/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-crabtree-# hockley/""              \n# [16] ""/bitcoin_atm/4739/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-subway-boldmere/""               \n# [17] ""/bitcoin_atm/4738/bitcoin-atm-bitxatm-birmingham-uk-ashtree-convenience-store/""                \n# [18] ""/bitcoin_atm/4737/bitcoin-atm-bitxatm-birmingham-uk-nisa-local-finnemore-road-bordesley-green/""\n# [19] ""/bitcoin_atm/3160/bitcoin-atm-bitxatm-birmingham-uk-costcutter/"" \n\n', ""\nWhen you click show more the page does an XHR POST request for more results using an offset of 10 (suggesting results come in batches of 10) from current set. You can mimic this so long as you have the followings params in the post body (I suspect only the bottom 3 are essential)\n'direction' : 1\n'sort' : 1\n'offset' : 10\n'pagetype' : 'city'\n'pageid' : 345\n\nAnd the following request header is required (at least in Python implementations) \n'X-Requested-With' : 'XMLHttpRequest'\n\nYou send that correctly and you will get a response containing the additional content. Note: content is wrapped in  ![CDATA[]] as instruction that content should not be interpreted as xml - you will need to account for that by extracting content within for parsing.\nThe total number of atms is returned from original  page you have and with css selector\n.atm-number\n\nYou can split on  &nbsp; and take the upper bound value from the split and convert to int. You then can calculate each offset required to meet that total (being used in a loop as consecutive offset param until total achieved) e.g. 19 results will be 2 requests total with 1 request at offset 10 for additional content.\n""]",https://stackoverflow.com/questions/56118999/issue-scraping-page-with-load-more-button-with-rvest,screen-scraping
PHP equivalent of PyQuery or Nokogiri? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



Basically, I want to do some HTML screen scraping, but figuring out if it is possible in PHP.
In Python, I would use
PyQuery.
In Ruby, I would use Nokogiri.
",4k,"
            6
        ","[""\nIn PHP you can use phpQuery\nP.S. it's kinda ironic, I came to this page looking for phpQuery equivalent in Python :)\n"", '\nIn PHP for screen scraping you can use Snoopy (http://sourceforge.net/projects/snoopy/) or Simple HTML DOM Parser (http://simplehtmldom.sourceforge.net/)\n', ""\nHere's a PHP port of Ruby nokogiri\n"", '\nThe closest thing to Nokogiri in PHP is this one.\n', '\nThere are also the built in PHP DOM libraries.  They include DomDocument with its loadHTML() method for parsing html from a string and DomElement and DomNode for building up HTML via an object. \n']",https://stackoverflow.com/questions/2815726/php-equivalent-of-pyquery-or-nokogiri,screen-scraping
Count number of results for a particular word on Twitter,"
To further a personal project of mine, I have been pondering how to count the number of results for a user specified word on Twitter.  I have used their API extensively, but have not been able to come up with an efficient or even halfway practical way to count the occurrences of a particular word.  The actual results are not critical, just the overall count.  I'll keep scratching my head.  Any ideas or direction pointing would be most appreciated.
e.g. http://search.twitter.com/search?q=tomatoes
",7k,"
            5
        ","[""\nI'm able to go back about a week. I start my search with the parameters that Adam posted and then key off of the smallest id in the set of search results, like so,\nhttp://search.twitter.com/search.atom?lang=en&q=iphone&rpp=100&max_id=\nwhere max_id = the min(id) of the 100 results I just pulled.\n"", '\nnet but I have made recursive function to call search query again and again until I don\'t find word ""page="" in result.\n']",https://stackoverflow.com/questions/580369/count-number-of-results-for-a-particular-word-on-twitter,screen-scraping
Convert a relative URL to an absolute URL with Simple HTML DOM?,"
When I'm scraping content from some pages, the script gives a relative URL. Is it possible to get a absolute URL with Simple HTML DOM?
",5k,"
            4
        ","[""\nI don’t think that the Simple HTML DOM Parser can do that.\nBut you can do that on your own. First you need to distinguish the base URI that is the URI of the document if not declared otherwise (see BASE element). Than get each URI reference and apply the algorithms to resolve a relative URI as described in RFC 3986 (there already are classes you can use for that like the PEAR package Net_URL2).\nSo, using these two classes, you could do something like this:\n$uri = new Net_URL2('http://example.com/foo/bar'); // URI of the resource\n$baseURI = $uri;\nforeach ($html->find('base[href]') as $elem) {\n    $baseURI = $uri->resolve($elem->href);\n}\n\nforeach ($html->find('*[src]') as $elem) {\n    $elem->src = $baseURI->resolve($elem->src)->__toString();\n}\nforeach ($html->find('*[href]') as $elem) {\n    if (strtoupper($elem->tag) === 'BASE') continue;\n    $elem->href = $baseURI->resolve($elem->href)->__toString();\n}\nforeach ($html->find('form[action]') as $elem) {\n    $elem->action = $baseURI->resolve($elem->action)->__toString();\n}\n\nRepeat the substitution for any other attribute containing a URI like background, cite, classid, codebase, data, longdesc, profile and usemap (see index of attributes in HTML 4.01).\n"", '\nIn addition to @Artefacto\'s answer, and if you are outputting the scraped HTML somewhere, you could simply add <base href=""http://example.com""> to the head of the document, which will establish the base URL for all relative URLs in the document as the specified href. Have a look at http://www.w3schools.com/tags/tag_base.asp\n', ""\nEDIT See Gumbo's answer for a formally correct answer. This is a simplified algorithm that will work in the vast majority of cases, but fail on some.\nSure. Do this:\n\nTake the relative URL (a URL that doesn't start with http://, https://, or any other protocol, and also doesn't start with /).\nTake the URL of the page.\nRemove the query string from it (if any). One simple way is to explode around ? and then take the first element of the resulting array (take element with index 0 or use reset).\n\n\nIf the URL of the page ends in /, append it the relative URL and you have the final URL.\nIf the URL doesn't end in /, take dirname of it, and append it the relative URL. You now have the final URL.\n\n\n""]",https://stackoverflow.com/questions/3329499/convert-a-relative-url-to-an-absolute-url-with-simple-html-dom,screen-scraping
Can a cURL based HTTP request imitate a browser based request completely?,"
This is a two part question.
Q1: Can cURL based request 100% imitate a browser based request? 
Q2: If yes, what all options should be set. If not what extra does the browser do that cannot bee imitated by cURL?
I have a website and I see thousands of request being made from a single IP in a very short time. These requests harvest all my data. When looked at the log to identify the agent used, it looks like a request from browser. So was curious to know if its a bot and not a user.
Thanks in advance
",8k,"
            4
        ","['\nThis page has all the answers to your questions. You can imitate the things mostly.\n', ""\nR1 : I suppose, if you set all the correct headers, that, yes, a curl-based request can imitate a browser-based one : after all, both send an HTTP request, which is just a couple of lines of text following a specific convention (namely, the HTTP RFC)\n\nR2 : The best way to answer that question is to take a look at what your browser is sending ; with Firefox, for instance, you can use either Firebug or LiveHTTPHeaders to get that.\nFor instance, to get this page, Firefox sent those request headers :\nGET /questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely HTTP/1.1\nHost: stackoverflow.com\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; fr; rv:1.9.2b4) Gecko/20091124 Firefox/3.6b4\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: fr,fr-fr;q=0.8,en-us;q=0.5,en;q=0.3\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 115\nConnection: keep-alive\nReferer: http://stackoverflow.com/questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely/1926889\nCookie: .......\nCache-Control: max-age=0\n\n(I Just removed a couple of informations -- but you get the idea ;-) )\nUsing curl, you can work with curl_setopt to set the HTTP headers ; here, you'd probably have to use a combination of CURLOPT_HTTPHEADER, CURLOPT_COOKIE, CURLOPT_USERAGENT, ...\n""]",https://stackoverflow.com/questions/1926876/can-a-curl-based-http-request-imitate-a-browser-based-request-completely,screen-scraping
Get instagram followers,"
I want to parse a website's followers count with BeautifulSoup. This is what I have so far:
username_extract = 'lazada_my'

url = 'https://www.instagram.com/'+ username_extract
r = requests.get(url)
soup = BeautifulSoup(r.content,'lxml')
f = soup.find('head', attrs={'class':'count'})

This is the part I want to parse:

Something within my soup.find() function is wrong, but I can't wrap my head around it. When returning f, it is empty. Any idea what I am doing wrong?
",5k,"
            3
        ","['\nI think you can use re module to search the correct count.\nimport requests\nimport re\n\nusername_extract = \'lazada_my\'\n\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nm = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\nprint(m.group(1))\n\n', '\nsoup.find(\'head\', attrs={\'class\':\'count\'}) searches for something that looks like <head class=""count"">, which doesn\'t exist anywhere in the HTML. The data you\'re after is contained in the <script> tag that starts with window._sharedData:\nscript = soup.find(\'script\', text=lambda t: t.startswith(\'window._sharedData\'))\n\nFrom there, you can just strip off the variable assignment and the semicolon to get valid JSON:\n# <script>window._sharedData = ...;</script>\n#                              ^^^\n#                              JSON\n\npage_json = script.text.split(\' = \', 1)[1].rstrip(\';\')\n\nParse it and everything you need is contained in the object:\nimport json\n\ndata = json.loads(page_json)\nfollower_count = data[\'entry_data\'][\'ProfilePage\'][0][\'user\'][\'followed_by\'][\'count\']\n\n', '\nMost of the content is dynamically generated with JS. That\'s the reason you\'re getting empty results.\nBut, the followers count is present in the page source. Only thing is, it is not directly available in the form you want. You can see it here:\n<meta content=""407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)"" name=""description"" />\n\nIf you want to scrape the followers count without regex, you can use this:\n>>> followers = soup.find(\'meta\', {\'name\': \'description\'})[\'content\']\n>>> followers\n\'407.4k Followers, 27 Following, 2,740 Posts - See Instagram photos and videos from Lazada Malaysia (@lazada_my)\'\n>>> followers_count = followers.split(\'Followers\')[0]\n>>> followers_count\n\'407.4k \'\n\n', '\nYou have to look for the scripts, Then look for the \'window._sharedData\' exits in it. If exits then perform the regular expression operation. \nimport re\n\nusername_extract = \'lazada_my\'\nurl = \'https://www.instagram.com/\'+ username_extract\nr = requests.get(url)\nsoup = BeautifulSoup(r.content,\'lxml\')\ns = re.compile(r\'""followed_by"":{""count"":\\d*}\')\nfor i in soup.find_all(\'script\'):\n     if \'window._sharedData\' in str(i):\n         print s.search(str(i.contents)).group()\n\nResult,\n""followed_by"":{""count"":407426}\n\n', '\nThank you all, I ended up using William\'s solution. In case anybody will have future projects, here is my complete code for scraping a bunch of URL\'s for their follower count:\nimport requests\nimport csv \nimport pandas as pd\nimport re\n\ninsta = pd.read_csv(\'Instagram.csv\')\n\nusername = []\n\nbad_urls = [] \n\nfor lines in insta[\'Instagram\'][0:250]:\n    lines = lines.split(""/"")\n    username.append(lines[3])\n\nwith open(\'insta_output.csv\', \'w\') as csvfile:\nt = csv.writer(csvfile, delimiter=\',\')     #   ----> COMMA Seperated\nfor user in username:\n   try:\n       url = \'https://www.instagram.com/\'+ user\n       r = requests.get(url)\n       m = re.search(r\'""followed_by"":\\{""count"":([0-9]+)\\}\', str(r.content))\n       num_followers = m.group(1)\n       t.writerow([user,num_followers])    #  ----> Adding Rows\n   except:\n       bad_urls.append(url)\n\n']",https://stackoverflow.com/questions/49043857/get-instagram-followers,screen-scraping
"BeautifulSoup subpages of list with ""load more"" pagination","
Quite new here, so apologies in advance. I'm looking to get a list of all company descriptions from https://angel.co/companies to play around with. The web-based parsing tools I've tried aren't cutting it, so I'm looking to write a simple python script. Should I start by getting an array of all the company URLs then loop through them? Any resources or direction would be helpful--I've looked around BeautifulSoup's documentation and a few posts/video tutorials, but I'm getting hung up on simulating the json request, among other things (see here: Get all links with BeautifulSoup from a single page website ('Load More' feature))
I see a script that I believe is calling additional listings:
o.on(""company_filter_fetch_page_complete"", function(e) {
    return t.ajax({
        url: ""/companies/startups"",
        data: e,
        dataType: ""json"",
        success: function(t) {
            return t.html ? 
                (E().find("".more"").empty().replaceWith(t.html),
                 c()) : void 0
        }
    })
}),

Thanks!
",4k,"
            3
        ","['\nThe data you want to scrape is dynamically loaded using ajax, you need to do a lot of work to get to the html you actually want:  \nimport requests\nfrom bs4 import BeautifulSoup\n\nheader = {\n    ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n    ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\nwith requests.Session() as s:\n    r = s.get(""https://angel.co/companies"").content\n    csrf = BeautifulSoup(r).select_one(""meta[name=csrf-token]"")[""content""]\n    header[""X-CSRF-Token""] = csrf\n    ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n    _ids = """".join([""ids%5B%5D={}&"".format(i)  for i in ids.pop(""ids"")])\n    rest = ""&"".join([""{}={}"".format(k,v) for k,v in ids.items()])\n    url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n    rsp = s.get(url, headers=header)\n    print(rsp.json())\n\nWe first need to get a valid csrf-token which is what the initial request does, then we need to post to https://angel.co/company_filters/search_data:\n\nwhich gives us:\n{""ids"":[296769,297064,60,63,112,119,130,160,167,179,194,236,281,287,312,390,433,469,496,516],""total"":908164,""page"":1,""sort"":""signal"",""new"":false,""hexdigest"":""3f4980479bd6dca37e485c80d415e848a57c43ae""}\n\nThey are the params needed for our get to https://angel.co/companies/startups i.e our last request:\n\nThat request then gives us more json which holds the html and all the company info:\n{""html"":""<div class=\\"" dc59 frs86 _a _jm\\"" data-_tn=\\""companies/results ...........\n\nThere is way too much to post but that is what you will need to parse.\nSo putting it all together:\nIn [3]: header = {\n   ...:     ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n   ...:     ""X-Requested-With"": ""XMLHttpRequest"",\n   ...: }\n\nIn [4]: with requests.Session() as s:\n   ...:         r = s.get(""https://angel.co/companies"").content\n   ...:         csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n   ...:         header[""X-CSRF-Token""] = csrf\n   ...:         ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal""}, headers=header).json()\n   ...:         _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n   ...:         rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n   ...:         url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n   ...:         rsp = s.get(url, headers=header)\n   ...:         soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n   ...:         for comp in soup.select(""div.base.startup""):\n   ...:                 text = comp.select_one(""div.text"")\n   ...:                 print(text.select_one(""div.name"").text.strip())\n   ...:                 print(text.select_one(""div.pitch"").text.strip())\n   ...:         \nFrontback\nMe, now.\nOutbound\nOptimizely for messages\nAdaptly\nThe Easiest Way to Advertise Across The Social Web.\nDraft\nWords with Friends for Fantasy (w/ real money)\nGraphicly\nan automated ebook publishing and distribution platform\nAppstores\nApp Distribution Platform\neVenues\nOnline Marketplace & Booking Engine for Unique Meeting Spaces\nWePow\nVideo & Mobile Recruitment\nDoubleDutch\nEvent Marketing Automation Software\necomom\nIt\'s all good\nBackType\nAcquired by Twitter\nStipple\nNative advertising for the visual web\nPinterest\nA Universal Social Catalog\nSocialize\nIdentify and reward your most influential users with our drop-in social platform.\nStyleSeat\nLargest and fastest growing marketplace in the $400B beauty and wellness industry\nLawPivot\n99 Designs for legal\nOstrovok\nLeading hotel booking platform for Russian-speakers\nThumb\nLeading mobile social network that helps people get instant opinions\nAppFog\nMaking developing applications on the cloud easier than ever before\nArtsy\nMaking all the world’s art accessible to anyone with an Internet connection.\n\nAs far as the pagination goes, you are limited to 20 pages per day but to get all 20 pages is simply a case of adding page:page_no to our form data to get the new params needed, data={""sort"": ""signal"",""page"":page}, when you click load more you can see what is posted:\n\nSo the final code:\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef parse(soup):\n\n        for comp in soup.select(""div.base.startup""):\n            text = comp.select_one(""div.text"")\n            yield (text.select_one(""div.name"").text.strip()), text.select_one(""div.pitch"").text.strip()\n\ndef connect(page):\n    header = {\n        ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36"",\n        ""X-Requested-With"": ""XMLHttpRequest"",\n    }\n\n    with requests.Session() as s:\n        r = s.get(""https://angel.co/companies"").content\n        csrf = BeautifulSoup(r, ""lxml"").select_one(""meta[name=csrf-token]"")[""content""]\n        header[""X-CSRF-Token""] = csrf\n        ids = s.post(""https://angel.co/company_filters/search_data"", data={""sort"": ""signal"",""page"":page}, headers=header).json()\n        _ids = """".join([""ids%5B%5D={}&"".format(i) for i in ids.pop(""ids"")])\n        rest = ""&"".join([""{}={}"".format(k, v) for k, v in ids.items()])\n        url = ""https://angel.co/companies/startups?{}{}"".format(_ids, rest)\n        rsp = s.get(url, headers=header)\n        soup = BeautifulSoup(rsp.json()[""html""], ""lxml"")\n        for n, p in parse(soup):\n            yield n, p\nfor i in range(1, 21):\n    for name, pitch in connect(i):\n        print(name, pitch)\n\nObviously what you parse is up to you but everything you see in your browser in the results will be available.\n']",https://stackoverflow.com/questions/37799149/beautifulsoup-subpages-of-list-with-load-more-pagination,screen-scraping
Close a scrapy spider when a condition is met and return the output object,"
I have made a spider to get reviews from a page like this here using scrapy. I want product reviews only till a certain date(2nd July 2016 in this case). I want to close my spider as soon as the review date goes earlier than the given date and return the items list.
Spider is working well but my problem is that i am not able to close my spider if the condition is met..if i raise an exception, spider closes without returning anything.
Please suggest the best way to close the spider manually. Here is my code:
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy import Selector
from tars.items import FlipkartProductReviewsItem
import re as r
import unicodedata
from datetime import datetime 

class Freviewspider(CrawlSpider):
    name = ""frs""
    allowed_domains = [""flipkart.com""]
    def __init__(self, *args, **kwargs):
        super(Freviewspider, self).__init__(*args, **kwargs)
        self.start_urls = [kwargs.get('start_url')]


    rules = (
        Rule(LinkExtractor(allow=(), restrict_xpaths=('//a[@class=""nav_bar_next_prev""]')), callback=""parse_start_url"", follow= True),
)


    def parse_start_url(self, response):

        hxs = Selector(response)
        titles = hxs.xpath('//div[@class=""fclear fk-review fk-position-relative line ""]')

        items = []

        for i in titles:

            item = FlipkartProductReviewsItem()

            #x-paths:

            title_xpath = ""div[2]/div[1]/strong/text()""
            review_xpath = ""div[2]/p/span/text()""
            date_xpath = ""div[1]/div[3]/text()""



            #field-values-extraction:

            item[""date""] = (''.join(i.xpath(date_xpath).extract())).replace('\n ', '')
            item[""title""] = (''.join(i.xpath(title_xpath).extract())).replace('\n ', '')

            review_list = i.xpath(review_xpath).extract()
            temp_list = []
            for element in review_list:
                temp_list.append(element.replace('\n ', '').replace('\n', ''))

            item[""review""] = ' '.join(temp_list)

            xxx = datetime.strptime(item[""date""], '%d %b %Y ')
            comp_date = datetime.strptime('02 Jul 2016 ', '%d %b %Y ')
            if xxx>comp_date:
                items.append(item)
            else:
                break

        return(items)

",6k,"
            2
        ",['\nTo force spider to close you can use raise CloseSpider exception as described here in scrapy docs. Just be sure to return/yield your items before you raise the exception.\n'],https://stackoverflow.com/questions/38331428/close-a-scrapy-spider-when-a-condition-is-met-and-return-the-output-object,screen-scraping
How to scrape charts from a website with python?,"
EDIT: 
So I have save the script codes below to a text file but using re to extract the data still doesn't return me anything. My code is: 
file_object = open('source_test_script.txt', mode=""r"")
soup = BeautifulSoup(file_object, ""html.parser"")
pattern = re.compile(r""^var (chart[0-9]+) = new Highcharts.Chart\(({.*?})\);$"", re.MULTILINE | re.DOTALL)
scripts = soup.find(""script"", text=pattern)
profile_text = pattern.search(scripts.text).group(1)
profile = json.loads(profile_text)

print profile[""data""], profile[""categories""]


I would like to extract the chart's data from a website. The following is the source code of the chart. 
  <script type=""text/javascript"">
    jQuery(function() {

    var chart1 = new Highcharts.Chart({

          chart: {
             renderTo: 'chart1',
              defaultSeriesType: 'column',
            borderWidth: 2
          },
          title: {
             text: 'Productions'
          },
          legend: {
            enabled: false
          },
          xAxis: [{
             categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016],

          }],
          yAxis: {
             min: 0,
             title: {
             text: 'Productions'
          }
          },

          series: [{
               name: 'Productions',
               data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]
               }]
       });
    });

    </script>

There are several charts like that from the website, called ""chart1"", ""chart2"", etc. I would like to extract the following data: the categories line and the data line, for each chart: 
categories: [1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016]

data: [1,1,0,1,6,4,9,15,15,19,24,18,53,42,54,53,61,36]

",5k,"
            2
        ","['\nAnother way is to use Highcharts\' JavaScript Library as one would in the console and pull that using Selenium. \nimport time\nfrom selenium import webdriver\n\nwebsite = """"\n\ndriver = webdriver.Firefox()\ndriver.get(website)\ntime.sleep(5)\n\ntemp = driver.execute_script(\'return window.Highcharts.charts[0]\'\n                             \'.series[0].options.data\')\ndata = [item[1] for item in temp]\nprint(data)\n\nDepending on what chart and series you are trying to pull your case might be slightly different.\n', '\nI\'d go a combination of regex and yaml parser.  Quick and dirty below - you may need to tweek the regex but it works with example:\nimport re\nimport sys\nimport yaml\n\nchart_matcher = re.compile(r\'^var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);$\',\n        re.MULTILINE | re.DOTALL)\n\nscript = sys.stdin.read()\n\nm = chart_matcher.findall(script)\n\nfor name, data in m:\n    print name\n    try:\n        chart = yaml.safe_load(data)\n        print ""categories:"", chart[\'xAxis\'][0][\'categories\']\n        print ""data:"", chart[\'series\'][0][\'data\']\n    except Exception, e:\n        print e\n\nRequires the yaml library (pip install PyYAML) and you should use BeautifulSoup to extract the correct <script> tag before passing it to the regex.\nEDIT - full example\nSorry I didn\'t make myself clear.  You use BeautifulSoup to parse the HTML and extract the <script> elements, and then use PyYAML to parse the javascript object declaration.  You can\'t use the built in json library because its not valid JSON but plain javascript object declarations (ie with no functions) are a subset of YAML.\nfrom bs4 import BeautifulSoup\nimport yaml\nimport re\n\nfile_object = open(\'source_test_script.txt\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\n# find every <script> tag in the source using beautifulsoup\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text.replace(\'\\t\', \'\')\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        try:\n            # parse the javascript declaration\n            charts[name] = yaml.safe_load(obj_declaration)\n        except Exception, e:\n            print ""Failed to parse {0}: {1}"".format(name, e)\n\n# extract the data you want\nfor name in charts:\n    print ""## {0} ##"".format(name);\n    print ""categories:"", charts[name][\'xAxis\'][0][\'categories\']\n    print ""data:"", charts[name][\'series\'][0][\'data\']\n    print\n\nOutput:\n## chart1 ##\ncategories: [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\ndata: [22, 1, 0, 1, 6, 4, 9, 15, 15, 19, 24, 18, 53, 42, 54, 53, 61, 36]\n\nNote I had to tweek the regex to make it handle the unicode output and whitespace from BeautifulSoup - in my original example I just piped your source directly to the regex.\nEDIT 2 - no yaml\nGiven that the javascript looks to be partially generated the best you can hope for is to grab the lines - not elegant but will probably work for you.\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nfile_object = open(\'citec.repec.org_p_c_pcl20.html\', mode=""r"")\nsoup = BeautifulSoup(file_object, ""html.parser"")\n\npattern = re.compile(r""var (chart[0-9]+) = new Highcharts.Chart\\(({.*?})\\);"", re.MULTILINE | re.DOTALL | re.UNICODE)\n\ncharts = {}\n\nfor tag in soup.find_all(\'script\'):\n\n    # tabs are special in yaml so remove them first\n    script = tag.text\n\n    values = {}\n\n    # find each object declaration\n    for name, obj_declaration in pattern.findall(script):\n        for line in obj_declaration.split(\'\\n\'):\n            line = line.strip(\'\\t\\n ,;\')\n            for field in (\'data\', \'categories\'):\n                if line.startswith(field + "":""):\n                    data = line[len(field)+1:]\n                    try:\n                        values[field] = json.loads(data)\n                    except:\n                        print ""Failed to parse %r for %s"" % (data, name)\n\n        charts[name] = values\n\nprint charts\n\nNote that it fails for chart7 because that references another variable.\n']",https://stackoverflow.com/questions/39864796/how-to-scrape-charts-from-a-website-with-python,screen-scraping
Screen Scraping of Image Links in PHP,"
I have a website that contains many different pages of products and each page has a certain amount of images in the same format across all pages. I want to be able to screen scrap each page's url so I can retrieve the url of each image from each page. The idea is to make a gallery for each page made up of hotlinked images.
I know this can be done in php, but I am not sure how to scrap the page for multiple links. Any ideas?
",5k,"
            2
        ","[""\nI would recommend using a DOM parser, such as PHP's very own DOMDocument. Example:\n$page = file_get_contents('http://example.com/images.php');\n$doc = new DOMDocument(); \n$doc->loadHTML($page);\n$images = $doc->getElementsByTagName('img'); \nforeach($images as $image) {\n    echo $image->getAttribute('src') . '<br />';\n}\n\n"", '\nYou can use a regular expression (regex) to go through the page source and parse all the IMG tags. \nThis regex will do the job quite nicely: <img[^>]+src=""(.*?)"" \nHow does this work? \n// <img[^>]+src=""(.*?)""\n// \n// Match the characters ""<img"" literally «<img»\n// Match any character that is not a "">"" «[^>]+»\n//    Between one and unlimited times, as many times as possible, giving back as needed (greedy) «+»\n// Match the characters ""src="""" literally «src=""»\n// Match the regular expression below and capture its match into backreference number 1 «(.*?)»\n//    Match any single character that is not a line break character «.*?»\n//       Between zero and unlimited times, as few times as possible, expanding as needed (lazy) «*?»\n// Match the character """""" literally «""»\n\nSample PHP code: \npreg_match_all(\'/<img[^>]+src=""(.*?)""/i\', $subject, $result, PREG_PATTERN_ORDER);\nfor ($i = 0; $i < count($result[0]); $i++) {\n    // image URL is in $result[0][$i];\n}\n\nYou\'ll have to do a bit more work to resolve things like relative URLs.\n', ""\nI really like PHP Simple HTML DOM Parser for things like this. An example of grabbing images is right there on the front page:\n// Create DOM from URL or file\n$html = file_get_html('http://www.google.com/');\n\n// Find all images\nforeach($html->find('img') as $element)\n       echo $element->src . '<br>';\n\n"", '\nYou can you this to scrap pages.\nhttp://simplehtmldom.sourceforge.net/\nbut it requires PHP 5+.\n']",https://stackoverflow.com/questions/3261820/screen-scraping-of-image-links-in-php,screen-scraping
What prevents me from using $.ajax to load another domain's html?,"
My domain:
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'html',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

What prevents me from being able to scrape their-domain? Any work around?
Addendum: thank you all for the suggestions to use a server side script, but I am for the moment interested in solving this problem exclusively using the client.
If I format the request using ""jsonp"" I do at least get a response, but with the following error:""Uncaught SyntaxError: Unexpected token <"". So I am getting a response from their-domain but the parser expects it to be json. (As well it should.) I am hacking through this trying to see if their is a way to trick the client into accepting this response. Please understand that I know this is atypical.
<!DOCTYPE html>  
<html>
<head>
<title>scrape</title>
<script src=""http://code.jquery.com/jquery-1.7.1.min.js""></script>
</head>
<body>
    <script>
        $.ajax({url:'http://their-domain.com/index.html',
        dataType:'jsonp',
            success:function(data){console.log(data);}
        });
    </script>
</body>
</html>

",2k,"
            2
        ","[""\nThere are Four ways to get around Same Origin Policy \n\nProxy - You request it from your server, your server requests it from other domain, your server returns it to the browser\nFlash cross domain policy - other domain must add a crossdomain.xml file to their site\nCross domain HTTP header - other domain must add an Access-Control-Allow-Origin header to their page \nJSONP - It's a json web service that provides a callback function.  Other domain must implement this.\n\nNote: The ONLY way to do it without the other domain's help is #1, routing it through your own server.\n"", '\nthe Same Origin Policy prevents client side scripts from getting data from domains that are not from the originator for the request. You would need a server side script to act as a proxy\n', ""\nIt's the Same Origin Policy, which prevents cross-domain requests.  If you want to scrape html, you are better off writing a server side process to get the content, then use ajax to make a request against your server, which contains the harvested data.  \n"", '\nOne workaround is to make a server-side script (eg. PHP) to get the page, and have $.ajax call that.\n']",https://stackoverflow.com/questions/8944656/what-prevents-me-from-using-ajax-to-load-another-domains-html,screen-scraping
Screen scraping web page after delay,"
I'm trying to scrape a web page using C#, however after the page loads, it executes some JavaScript which loads more elements into the DOM which I need to scrape. A standard scraper simply grabs the html of the page on load and doesn't pick up the DOM changes made via JavaScript. How do I put in some sort of functionality to wait for a second or two and then grab the source?
Here is my current code:
private string ScrapeWebpage(string url, DateTime? updateDate)
{
    HttpWebRequest request = null;
    HttpWebResponse response = null;
    Stream responseStream = null;
    StreamReader reader = null;
    string html = null;
    try
    {
        //create request (which supports http compression)
        request = (HttpWebRequest)WebRequest.Create(url);
        request.Pipelined = true;
        request.Headers.Add(HttpRequestHeader.AcceptEncoding, ""gzip,deflate"");
        if (updateDate != null)
            request.IfModifiedSince = updateDate.Value;
        //get response.
        response = (HttpWebResponse)request.GetResponse();
        responseStream = response.GetResponseStream();
        if (response.ContentEncoding.ToLower().Contains(""gzip""))
            responseStream = new GZipStream(responseStream,
                CompressionMode.Decompress);
        else if (response.ContentEncoding.ToLower().Contains(""deflate""))
            responseStream = new DeflateStream(responseStream,
                CompressionMode.Decompress);
        //read html.
        reader = new StreamReader(responseStream, Encoding.Default);
        html = reader.ReadToEnd();
    }
    catch
    {
        throw;
    }
    finally
    {
        //dispose of objects.
        request = null;
        if (response != null)
        {
            response.Close();
            response = null;
        }
        if (responseStream != null)
        {
            responseStream.Close();
            responseStream.Dispose();
        }
        if (reader != null)
        {
            reader.Close();
            reader.Dispose();
        }
    }
    return html;
}

Here's a sample URL:
http://www.realtor.com/realestateandhomes-search/geneva_ny#listingType-any/pg-4
You'll see when the page first loads it says 134 listings found, then after a second it says 187 properties found.
",4k,"
            2
        ","[""\nTo execute the JavaScript I use webkit to render the page, which is the engine used by Chrome and Safari. Here is an example using its Python bindings.\nWebkit also has .NET bindings but I haven't used them.\n"", ""\nThe approach you have will not work regardless how long you wait, you need a browser to execute the javascript (or something that understands javascript).\nTry this question:\nWhat's a good tool to screen-scrape with Javascript support?\n"", '\nYou would need to execute the javascript yourself to get this functionality. Currently, your code only receives whatever the server replies with at the URL you request. The rest of the listings are ""showing up"" because the browser downloads, parses, and executes the accompanying javascript.\n', '\nThe answer to this similar question says to use a web browser control to read the page in and process it before scraping it. Perhaps with some kind of timer delay to give the javascript some time to execute and return results.\n']",https://stackoverflow.com/questions/5636921/screen-scraping-web-page-after-delay,screen-scraping
"Why is contains(text(), ""string"" ) not working in XPath?","
I have written this expression //*[contains(text(), ""Brand:"" )] for the below HTML code.


<div class=""info-product mt-3"">
  <h3>Informazioni prodotto</h3>


  Brand: <span class=""brand_title font-weight-bold text-uppercase""><a href=""https://mammapack.com/brand/ava"">Ava</a></span><br> SKU: 8002910009960<br> Peso Lordo: 0.471 kg <br> Dimensioni: 44.00 × 145.00 × 153.00 mm<br>

  <p class=""mt-2"">
    AVA BUCATO A MANO E2 GR.380</p>
</div>


The xpath that I have written is not working I want to select Node that contains text Brand:. Can someone tell me my mistake?
",3k,"
            1
        ","['\nYour XPath,\n//*[contains(text(), ""Brand:"")]\n\nin XPath 1.0 will select all elements whose first text node child contains a ""Brand:"" substring.  In XPath 2.0 it is an error to call contains() with a sequence of more than one item as the first argument.\nThis XPath,\n//*[text()[contains(., ""Brand:"")]]\n\nwill select all elements with a text node child whose string value contains a ""Brand:"" substring.\nSee also\n\nXPath 1.0 vs 2.0+ different contains() behavior explanation\nTesting text() nodes vs string values in XPath\n\n']",https://stackoverflow.com/questions/71253563/why-is-containstext-string-not-working-in-xpath,screen-scraping
"Why is this HtmlAgilityPack operation invalid when there are, indeed, matching elements?","
I get ""InvalidOperationException > Message=Sequence contains no matching element"" with the following code:
private void buttonLoadHTML_Click(object sender, EventArgs e)
{
    GetParagraphsListFromHtml(@""C:\PlatypiRUs\fitt.html"");
}

// This code adapted from Kirk Woll's answer at 
   http://stackoverflow.com/questions/4752840/html-agility-pack-c-sharp-paragraph-
   parsing-problem
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);
    foreach (var par in doc.DocumentNode
        .DescendantNodes()
        .Single(x => x.Id == ""body"")
        .DescendantNodes()
        .Where(x => x.Name == ""p""))
        //.Where(x => x.Name == ""h1"" || x.Name == ""h2"" || x.Name == ""h3"" || x.Name 
           == ""hp"" || )) <-- This is what I'd really like to do, but I don't know if   
           this is possible or, if it is, if the syntax is correct
    {
        pars.Add(par.InnerText);
    }
    // test
    foreach (string s in pars)
    {
        MessageBox.Show(s);
    }
    return pars;
}

Why is the code not finding the paragraphs?
I really want to find all the text (h1..3 or higher vals, too), but this is a start.
BTW: The html file I'm testing with does have some paragraph elements.
UPDATE
In response to Amy's implied request, and in the interest of full disclosure/ultimate illumination, here is the entire test html file:
<style>
body {
    background-color: orange;
    font-family: Verdana, sans-serif;
}

h1 {
    color: Blue;   
    font-family: 'Segoe UI', Verdana, sans-serif;
}

h2 {
    color: white;    
    font-family: 'Palatino Linotype', 'Palatino', sans-serif;
}

h3 {
    display: inline-block;
}
</style>

<h1>Found in the Translation</h1>
<h2>Bilingual Editions of Classic Literature</h2>
<div><label>Contact: </label><a href=""mailto:axx3andspace@gmail.com"">Found in the Translation</a></div>

<h2><cite>Around the World in 80 Days</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495308081"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I0DOYRE"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51BCZUX2-dL._SL160_.jpg"" /></a>

<h2><cite>Gulliver's Travels</cite> by Jonathan Swift (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495374688"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I5319ZO"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/517O76OyaWL._SL160_.jpg"" /></a>

<h2><cite>Journey to the Center of the Earth</cite> by Jules Verne (French &amp; English Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495409031"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41hosXOIw8L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I6LG25M"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/41qj8DfrihL._SL160_.jpg"" /></a>

<h2><cite>Treasure Island</cite> by Robert Louis Stevenson (English &amp; Finnish Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495418936"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51veMV3OiOL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00IA5V4KC"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51XNUWbA07L._SL160_.jpg"" /></a>

<h2><cite>Robinson Crusoe</cite> by Daniel Defoe (English &amp; French Side by Side)</h2>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1495448053"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51QQMRPrP9L._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00I9IE8OY"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5128hqiw3DL._SL160_.jpg"" /></a>

<h2><cite>Don Quixote</cite> by Miguel de Cervantes Saavedra (Spanish &amp; English Side by Side)</h2>
<h3>Paperback</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/149474967X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494803445"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/1494841983"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>
<h3>Kindle</h3></br>
<h3>Volume I</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HQMWPQ2"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51HqjOPXLVL._SL160_.jpg"" /></a>
<h3>Volume II</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HYN2QGM"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51NONygEMYL._SL160_.jpg"" /></a>
<h3>Volume III</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00HLX519E"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51G%2BW3ICHkL._SL160_.jpg"" /></a></br>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; German Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2><cite>Alice's Adventures in Wonderland</cite> by Lewis Carroll (English &amp; Italian Side by Side)</h2>
<h3>Coming soon; for now, see:</h3></br/>
<h3>Paperback</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/193659420X"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/5143vIpQ2YL._SL160_.jpg"" /></a>
<h3>Kindle</h3>
<a href=""https://rads.stackoverflow.com/amzn/click/com/B00ESLTIYQ"" rel=""nofollow noreferrer"" target=""_blank""><img height=""160"" width=""107"" src=""http://ecx.images-amazon.com/images/I/51%2BX0Dy7uNL._SL160_.jpg"" /></a>

<h2>Other Sites:</h2>
<p><a href=""http://usamaporama.azurewebsites.net/""  target=""_blank"">USA Map-O-Rama</a></p>
<p><a href=""http://www.awardwinnersonly.com/""  target=""_blank"">Award-winning Movies, Books, and Music</a></p>
<p><a href=""http://www.bigsurgarrapata.com/""  target=""_blank"">Garrapata State Park in Big Sur Throughout the Seasons</a></p>

UPDATE 2
This works (although it is with ""live"" web pages, and not html files saved to disk):
public List<string> GetParagraphsListFromHtml(string sourceHtml)
{
    var pars = new List<string>();
    HtmlAgilityPack.HtmlDocument doc = new HtmlAgilityPack.HtmlDocument();
    doc.LoadHtml(sourceHtml);

    var getHtmlWeb = new HtmlWeb();
    var document = getHtmlWeb.Load(""http://www.montereycountyweekly.com/opinion/letters/article_e333a222-942d-11e3-ba9c-001a4bcf6878.html""); 
    //http://www.bigsurgarrapata.com/ only returned one paragraph
    // http://usamaporama.azurewebsites.net/ <-- none
    // http://www.awardwinnersonly.com/ <- same as bigsurgarrapata
    var pTags = document.DocumentNode.SelectNodes(""//p"");
    int counter = 1;
    if (pTags != null)
    {
        foreach (var pTag in pTags)
        {
            pars.Add(pTag.InnerText);
            MessageBox.Show(pTag.InnerText);
            counter++;
        }
    }
    MessageBox.Show(""done!"");
    return pars;
}

",331,"
            0
        ","['\nIt turns out to be pretty easy; this is not complete, but this, inspired by this answer, is enough to get started:\nHtmlAgilityPack.HtmlDocument htmlDoc = new HtmlAgilityPack.HtmlDocument();\n\n// There are various options, set as needed\nhtmlDoc.OptionFixNestedTags = true;\n\nhtmlDoc.Load(@""C:\\Platypus\\dplatypus.htm"");\n\nif (htmlDoc.DocumentNode != null)\n{\n    IEnumerable<HtmlAgilityPack.HtmlNode> textNodes = htmlDoc.DocumentNode.SelectNodes(""//text()"");\n    foreach (HtmlNode node in textNodes)\n    {\n        if (!string.IsNullOrWhiteSpace(node.InnerText))\n        {\n            MessageBox.Show(node.InnerText);\n        }\n    }\n}\n\n']",https://stackoverflow.com/questions/21788078/why-is-this-htmlagilitypack-operation-invalid-when-there-are-indeed-matching-e,screen-scraping
Parse a .Net Page with Postbacks,"
I need to read data from an online database that's displayed using an aspx page from the UN. I've done HTML parsing before, but it was always by manipulating query-string values. In this case, the site uses asp.net postbacks. So, you click on a value in box one, then box two shows, click on a value in box 2 and click a button to get your results.
Does anybody know how I could automate that process? 
Thanks,
Mike
",2k,"
            0
        ","[""\nYou may still only need to send one request, but that one request can be rather complicated.  ASP.Net is notoriously difficult (though not impossible) to screen scrape.  Between event validation and the ViewState, it's tricky to get your requests just right.  The simplest way to do it is often to use a sniffer tool like fiddler to see exactly what the http request looks like, and then just mimic that request.\nIf you do still need to send two requests, it's because the first request also places some state in a session somewhere, and that means whatever you use to send those requests needs to be able to send them with the same session.  This often means supporting cookies.\n"", '\nWatin would be my first choice.  You would code the selecting and clicking, then parse the HTML after.\n', ""\nI'd look at HtmlAgilityPack with the FormProcessor addon.\n""]",https://stackoverflow.com/questions/1245782/parse-a-net-page-with-postbacks,screen-scraping
Options for web scraping - C++ version only,"
I'm looking for a good C++ library for web scraping.
It has to be C/C++ and nothing else so please do not direct me to Options for HTML scraping or other SO questions/answers where C++ is not even mentioned.
",60k,"
            46
        ","['\n\nlibcurl to download the html file\nlibtidy to convert to valid xml\nlibxml to parse/navigate the xml\n\n', '\nUse myhtml C/C++ parser here; dead simple, very fast. No dependencies except C99. And has CSS selectors built in (example here)\n', '\nI recommend Qt5.6.2, this powerful library offer us\n\nHigh level, intuitive, asynchronous network api like QNetworkAccessManager, QNetworkReply, QNetworkProxy etc\nPowerful regex class like QRegularExpression\nDecent web engine like QtWebEngine\nRobust, mature gui like QWidgets\nMost of the Qt5 api are well designed, signal and slot make writing asynchronous codes become much easier too\nGreat unicode support\nFeature rich file system library. Whether create, remove, rename or find standard path to save files is piece of cake in Qt5\nAsynchronous api of QNetworkAccessManager make it easy to spawn many download request at once\nCross major desktop platforms, windows, mac os and linux, write once compiled anywhere, one code bases only.\nEasy to deploy on windows and mac(linux?maybe linuxdeployqt can save us tons of troubles)    \nEasy to install on windows, mac and linux\nAnd so on\n\nI already wrote an image scraper apps by Qt5, this app can scrape almost every image searched by Google, Bing and Yahoo. \nTo know more details about it, please visit my github project.\nI wrote down high level overview about how to scrape data by Qt5 on \nmy blogs(it is too long to post at stack overflow).\n\nDownload Bing images by Qt5\nCreate a better images downloader(Google, Bing and Yahoo) by Qt5\n\n', '\n// download winhttpclient.h\n// --------------------------------\n#include <winhttp\\WinHttpClient.h>\nusing namespace std;\ntypedef unsigned char byte;\n#define foreach         BOOST_FOREACH\n#define reverse_foreach BOOST_REVERSE_FOREACH\n\nbool substrexvealue(const std::wstring& html,const std::string& tg1,const std::string& tg2,std::string& value, long& next) {\n    long p1,p2;\n    std::wstring wtmp;\n    std::wstring wtg1(tg1.begin(),tg1.end());\n    std::wstring wtg2(tg2.begin(),tg2.end());\n\n    p1=html.find(wtg1,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(wtg2,next);\n        if(p2!=std::wstring::npos) {\n            p1+=wtg1.size();\n            wtmp=html.substr(p1,p2-p1-1);\n            value=std::string(wtmp.begin(),wtmp.end());\n            boost::trim(value);\n            next=p1+1;\n        }\n    }\n    return p1!=std::wstring::npos;\n}\nbool extractvalue(const std::wstring& html,const std::string& tag,std::string& value, long& next) {\n    long p1,p2,p3;\n    std::wstring wtmp;\n    std::wstring wtag(tag.begin(),tag.end());\n\n    p1=html.find(wtag,next);\n    if(p1!=std::wstring::npos) {\n        p2=html.find(L"">"",p1+wtag.size()-1);\n        p3=html.find(L""<"",p2+1);\n        wtmp=html.substr(p2+1,p3-p2-1);\n        value=std::string(wtmp.begin(),wtmp.end());\n        boost::trim(value);\n        next=p1+1;\n    }\n    return p1!=std::wstring::npos;\n}\nbool GetHTML(const std::string& url,std::wstring& header,std::wstring& hmtl) {\n    std::wstring wurl = std::wstring(url.begin(),url.end());\n    bool ret=false;\n    try {\n        WinHttpClient client(wurl.c_str());\n        std::string url_protocol=url.substr(0,5);\n        std::transform(url_protocol.begin(), url_protocol.end(), url_protocol.begin(), (int (*)(int))std::toupper);\n        if(url_protocol==""HTTPS"")    client.SetRequireValidSslCertificates(false);\n        client.SetUserAgent(L""User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:19.0) Gecko/20100101 Firefox/19.0"");\n        if(client.SendHttpRequest()) {\n            header = client.GetResponseHeader();\n            hmtl = client.GetResponseContent();\n            ret=true;\n        }\n    }catch(...) {\n        header=L""Error"";\n        hmtl=L"""";\n    }\n    return ret;\n}\nint main() {\n    std::string url = ""http://www.google.fr"";\n    std::wstring header,html;\n    GetHTML(url,header,html));\n}\n\n']",https://stackoverflow.com/questions/834768/options-for-web-scraping-c-version-only,screen-scraping
TypeError: can't use a string pattern on a bytes-like object in re.findall(),"
I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:
import urllib.request
import re

url = ""http://www.google.com""
regex = r'<title>(,+?)</title>'
pattern  = re.compile(regex)

with urllib.request.urlopen(url) as response:
   html = response.read()

title = re.findall(pattern, html)
print(title)

And I get this unexpected error:
Traceback (most recent call last):
  File ""path\to\file\Crawler.py"", line 11, in <module>
    title = re.findall(pattern, html)
  File ""C:\Python33\lib\re.py"", line 201, in findall
    return _compile(pattern, flags).findall(string)
TypeError: can't use a string pattern on a bytes-like object

What am I doing wrong?
",276k,"
            164
        ","[""\nYou want to convert html (a byte-like object) into a string using .decode, e.g.  html = response.read().decode('utf-8'). \nSee Convert bytes to a Python String\n"", ""\nThe problem is that your regex is a string, but html is bytes:\n>>> type(html)\n<class 'bytes'>\n\nSince python doesn't know how those bytes are encoded, it throws an exception when you try to use a string regex on them.\nYou can either decode the bytes to a string:\nhtml = html.decode('ISO-8859-1')  # encoding may vary!\ntitle = re.findall(pattern, html)  # no more error\n\nOr use a bytes regex:\nregex = rb'<title>(,+?)</title>'\n#        ^\n\n\nIn this particular context, you can get the encoding from the response headers:\nwith urllib.request.urlopen(url) as response:\n    encoding = response.info().get_param('charset', 'utf8')\n    html = response.read().decode(encoding)\n\nSee the urlopen documentation for more details.\n"", ""\nBased upon last one, this was smimple to do when pdf read was done .\ntext = text.decode('ISO-8859-1') \n\nThanks @Aran-fey\n""]",https://stackoverflow.com/questions/31019854/typeerror-cant-use-a-string-pattern-on-a-bytes-like-object-in-re-findall,web-crawler
"Sending ""User-agent"" using Requests library in Python","
I want to send a value for ""User-agent"" while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below:
debug = {'verbose': sys.stderr}
user_agent = {'User-agent': 'Mozilla/5.0'}
response  = requests.get(url, headers = user_agent, config=debug)

The debug information isn't showing the headers being sent during the request.
Is it acceptable to send this information in the header?  If not, how can I send it?
",441k,"
            318
        ","[""\nThe user-agent should be specified as a field in the header.\nHere is a list of HTTP header fields, and you'd probably be interested in request-specific fields, which includes User-Agent.\nIf you're using requests v2.13 and newer\nThe simplest way to do what you want is to create a dictionary and specify your headers directly, like so:\nimport requests\n\nurl = 'SOME URL'\n\nheaders = {\n    'User-Agent': 'My User Agent 1.0',\n    'From': 'youremail@domain.example'  # This is another valid field\n}\n\nresponse = requests.get(url, headers=headers)\n\nIf you're using requests v2.12.x and older\nOlder versions of requests clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them.\nimport requests\n\nurl = 'SOME URL'\n\n# Get a copy of the default headers that requests would use\nheaders = requests.utils.default_headers()\n\n# Update the headers with your custom ones\n# You don't have to worry about case-sensitivity with\n# the dictionary keys, because default_headers uses a custom\n# CaseInsensitiveDict implementation within requests' source code.\nheaders.update(\n    {\n        'User-Agent': 'My User Agent 1.0',\n    }\n)\n\nresponse = requests.get(url, headers=headers)\n\n"", ""\nIt's more convenient to use a session, this way you don't have to remember to set headers each time:\nsession = requests.Session()\nsession.headers.update({'User-Agent': 'Custom user agent'})\n\nsession.get('https://httpbin.org/headers')\n\nBy default, session also manages cookies for you. In case you want to disable that, see this question.\n"", ""\nIt will send the request like browser\nimport requests\n\nurl = 'https://Your-url'\nheaders={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n\nresponse= requests.get(url.strip(), headers=headers, timeout=10)\n\n""]",https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python,web-crawler
How can I scrape pages with dynamic content using node.js?,"
I am trying to scrape a website but I don't get some of the elements, because these elements are dynamically created.
I use the cheerio in node.js and My code is below.
var request = require('request');
var cheerio = require('cheerio');
var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";

request(url, function (err, res, html) {
    var $ = cheerio.load(html);
    $('.listMain > li').each(function () {
        console.log($(this).find('a').attr('href'));
    });
});

This code returns empty response, because when the page is loaded, the <ul id=""store_list"" class=""listMain""> is empty. 
The content has not been appended yet. 
How can I get these elements using node.js? How can I scrape pages with dynamic content?
",33k,"
            30
        ","['\nHere you go;\nvar phantom = require(\'phantom\');\n\nphantom.create(function (ph) {\n  ph.createPage(function (page) {\n    var url = ""http://www.bdtong.co.kr/index.php?c_category=C02"";\n    page.open(url, function() {\n      page.includeJs(""http://ajax.googleapis.com/ajax/libs/jquery/1.6.1/jquery.min.js"", function() {\n        page.evaluate(function() {\n          $(\'.listMain > li\').each(function () {\n            console.log($(this).find(\'a\').attr(\'href\'));\n          });\n        }, function(){\n          ph.exit()\n        });\n      });\n    });\n  });\n});\n\n', ""\nCheck out GoogleChrome/puppeteer\n\nHeadless Chrome Node API\n\nIt makes scraping pretty trivial. The following example will scrape the headline over at npmjs.com (assuming .npm-expansions remains)\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n\n  await page.goto('https://www.npmjs.com/');\n\n  const textContent = await page.evaluate(() => {\n    return document.querySelector('.npm-expansions').textContent\n  });\n\n  console.log(textContent); /* No Problem Mate */\n\n  browser.close();\n})();\n\nevaluate will allow for the inspection of the dynamic element as this will run scripts on the page.\n"", ""\nUse the new npm module x-ray, with a pluggable web driver x-ray-phantom.\nExamples in the pages above, but here's how to do dynamic scraping:\nvar phantom = require('x-ray-phantom');\nvar Xray = require('x-ray');\n\nvar x = Xray()\n  .driver(phantom());\n\nx('http://google.com', 'title')(function(err, str) {\n  if (err) return done(err);\n  assert.equal('Google', str);\n  done();\n})\n\n"", '\nAnswering this as a canonical, an alternative to Puppeteer for scraping dynamic sites which is also well-supported as of 2023 is Playwright. Here\'s a simple example:\nconst playwright = require(""playwright""); // ^1.28.1\n\nlet browser;\n(async () => {\n  browser = await playwright.chromium.launch();\n  const page = await browser.newPage();\n  await page.goto(""https://example.com"");\n  const text = await page.locator(\'h1:text(""Example"")\').textContent();\n  console.log(text); // => Example Domain\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n\n', '\nEasiest and reliable solution is to use puppeteer. As mentioned in https://pusher.com/tutorials/web-scraper-node which is suitable for both static + dynamic scraping.\nOnly change the timeout in Browser.js, TimeoutSettings.js, Launcher.js 300000 to 3000000\n']",https://stackoverflow.com/questions/28739098/how-can-i-scrape-pages-with-dynamic-content-using-node-js,web-crawler
how to detect search engine bots with php?,"
How can one detect the search engine bots using php?
",153k,"
            144
        ","[""\nI use the following code which seems to be working fine:\nfunction _bot_detected() {\n\n  return (\n    isset($_SERVER['HTTP_USER_AGENT'])\n    && preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT'])\n  );\n}\n\nupdate 16-06-2017 \nhttps://support.google.com/webmasters/answer/1061943?hl=en\nadded mediapartners\n"", '\nHere\'s a Search Engine Directory of Spider names\nThen you use $_SERVER[\'HTTP_USER_AGENT\']; to check if the agent is said spider.\nif(strstr(strtolower($_SERVER[\'HTTP_USER_AGENT\']), ""googlebot""))\n{\n    // what to do\n}\n\n', ""\nCheck the $_SERVER['HTTP_USER_AGENT'] for some of the strings listed here:\nhttp://www.useragentstring.com/pages/useragentstring.php\nOr more specifically for crawlers:\nhttp://www.useragentstring.com/pages/useragentstring.php?typ=Crawler\nIf you want to -say- log the number of visits of most common search engine crawlers, you could use\n$interestingCrawlers = array( 'google', 'yahoo' );\n$pattern = '/(' . implode('|', $interestingCrawlers) .')/';\n$matches = array();\n$numMatches = preg_match($pattern, strtolower($_SERVER['HTTP_USER_AGENT']), $matches, 'i');\nif($numMatches > 0) // Found a match\n{\n  // $matches[1] contains an array of all text matches to either 'google' or 'yahoo'\n}\n\n"", '\nYou can checkout if it\'s a search engine with this function :\n<?php\nfunction crawlerDetect($USER_AGENT)\n{\n$crawlers = array(\n\'Google\' => \'Google\',\n\'MSN\' => \'msnbot\',\n      \'Rambler\' => \'Rambler\',\n      \'Yahoo\' => \'Yahoo\',\n      \'AbachoBOT\' => \'AbachoBOT\',\n      \'accoona\' => \'Accoona\',\n      \'AcoiRobot\' => \'AcoiRobot\',\n      \'ASPSeek\' => \'ASPSeek\',\n      \'CrocCrawler\' => \'CrocCrawler\',\n      \'Dumbot\' => \'Dumbot\',\n      \'FAST-WebCrawler\' => \'FAST-WebCrawler\',\n      \'GeonaBot\' => \'GeonaBot\',\n      \'Gigabot\' => \'Gigabot\',\n      \'Lycos spider\' => \'Lycos\',\n      \'MSRBOT\' => \'MSRBOT\',\n      \'Altavista robot\' => \'Scooter\',\n      \'AltaVista robot\' => \'Altavista\',\n      \'ID-Search Bot\' => \'IDBot\',\n      \'eStyle Bot\' => \'eStyle\',\n      \'Scrubby robot\' => \'Scrubby\',\n      \'Facebook\' => \'facebookexternalhit\',\n  );\n  // to get crawlers string used in function uncomment it\n  // it is better to save it in string than use implode every time\n  // global $crawlers\n   $crawlers_agents = implode(\'|\',$crawlers);\n  if (strpos($crawlers_agents, $USER_AGENT) === false)\n      return false;\n    else {\n    return TRUE;\n    }\n}\n?>\n\nThen you can use it like :\n<?php $USER_AGENT = $_SERVER[\'HTTP_USER_AGENT\'];\n  if(crawlerDetect($USER_AGENT)) return ""no need to lang redirection"";?>\n\n', ""\nI'm using this to detect bots:\nif (preg_match('/bot|crawl|curl|dataprovider|search|get|spider|find|java|majesticsEO|google|yahoo|teoma|contaxe|yandex|libwww-perl|facebookexternalhit/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // is bot\n}\n\nIn addition I use a whitelist to block unwanted bots:\nif (preg_match('/apple|baidu|bingbot|facebookexternalhit|googlebot|-google|ia_archiver|msnbot|naverbot|pingdom|seznambot|slurp|teoma|twitter|yandex|yeti/i', $_SERVER['HTTP_USER_AGENT'])) {\n    // allowed bot\n}\n\nAn unwanted bot (= false-positive user) is then able to solve a captcha to unblock himself for 24 hours. And as no one solves this captcha, I know it does not produce false-positives. So the bot detection seem to work perfectly.\nNote: My whitelist is based on Facebooks robots.txt.\n"", ""\nBecause any client can set the user-agent to what they want, looking for 'Googlebot', 'bingbot' etc is only half the job.\nThe 2nd part is verifying the client's IP. In the old days this required maintaining IP lists. All the lists you find online are outdated. The top search engines officially support verification through DNS, as explained by Google https://support.google.com/webmasters/answer/80553 and Bing http://www.bing.com/webmaster/help/how-to-verify-bingbot-3905dc26\nAt first perform a reverse DNS lookup of the client IP. For Google this brings a host name under googlebot.com, for Bing it's under search.msn.com. Then, because someone could set such a reverse DNS on his IP, you need to verify with a forward DNS lookup on that hostname. If the resulting IP is the same as the one of the site's visitor, you're sure it's a crawler from that search engine.\nI've written a library in Java that performs these checks for you. Feel free to port it to PHP. It's on GitHub: https://github.com/optimaize/webcrawler-verifier\n"", '\nIf you really need to detect GOOGLE engine bots you should never rely on ""user_agent"" or ""IP"" address because ""user_agent"" can be changed  and acording to what google said in: Verifying Googlebot\n\nTo verify Googlebot as the caller:\n1.Run a reverse DNS lookup on the accessing IP address from your logs, using the host command.\n2.Verify that the domain name is in either googlebot.com or google.com\n3.Run a forward DNS lookup on the domain name retrieved in step 1 using the host command on the retrieved domain name. Verify that it is the same as the original accessing IP address from your logs.\n\nHere is my tested code :\n<?php\n$remote_add=$_SERVER[\'REMOTE_ADDR\'];\n$hostname = gethostbyaddr($remote_add);\n$googlebot = \'googlebot.com\';\n$google = \'google.com\';\nif (stripos(strrev($hostname), strrev($googlebot)) === 0 or stripos(strrev($hostname),strrev($google)) === 0 ) \n{\n//add your code\n}\n\n?>\n\nIn this code we check ""hostname"" which should contain ""googlebot.com"" or ""google.com"" at the end of ""hostname"" which is really important to check exact domain not subdomain.\nI hope you enjoy ;)\n', ""\nI use this function ... part of the regex comes from prestashop but I added some more bot to it.   \n    public function isBot()\n{\n    $bot_regex = '/BotLink|bingbot|AhrefsBot|ahoy|AlkalineBOT|anthill|appie|arale|araneo|AraybOt|ariadne|arks|ATN_Worldwide|Atomz|bbot|Bjaaland|Ukonline|borg\\-bot\\/0\\.9|boxseabot|bspider|calif|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|CoolBot|cosmos|Internet Cruiser Robot|cusco|cyberspyder|cydralspider|desertrealm, desert realm|digger|DIIbot|grabber|downloadexpress|DragonBot|dwcp|ecollector|ebiness|elfinbot|esculapio|esther|fastcrawler|FDSE|FELIX IDE|ESI|fido|H�m�h�kki|KIT\\-Fireball|fouineur|Freecrawl|gammaSpider|gazz|gcreep|golem|googlebot|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|iajabot|INGRID\\/0\\.1|Informant|InfoSpiders|inspectorwww|irobot|Iron33|JBot|jcrawler|Teoma|Jeeves|jobo|image\\.kapsi\\.net|KDD\\-Explorer|ko_yappo_robot|label\\-grabber|larbin|legs|Linkidator|linkwalker|Lockon|logo_gif_crawler|marvin|mattie|mediafox|MerzScope|NEC\\-MeshExplorer|MindCrawler|udmsearch|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|sharp\\-info\\-agent|WebMechanic|NetScoop|newscan\\-online|ObjectsSearch|Occam|Orbsearch\\/1\\.0|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|Getterrobo\\-Plus|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Search\\-AU|searchprocess|Senrigan|Shagseeker|sift|SimBot|Site Valet|skymob|SLCrawler\\/2\\.0|slurp|ESI|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|nil|suke|http:\\/\\/www\\.sygol\\.com|tach_bw|TechBOT|templeton|titin|topiclink|UdmSearch|urlck|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|crawlpaper|wapspider|WebBandit\\/1\\.0|webcatcher|T\\-H\\-U\\-N\\-D\\-E\\-R\\-S\\-T\\-O\\-N\\-E|WebMoose|webquest|webreaper|webs|webspider|WebWalker|wget|winona|whowhere|wlm|WOLP|WWWC|none|XGET|Nederland\\.zoek|AISearchBot|woriobot|NetSeer|Nutch|YandexBot|YandexMobileBot|SemrushBot|FatBot|MJ12bot|DotBot|AddThis|baiduspider|SeznamBot|mod_pagespeed|CCBot|openstat.ru\\/Bot|m2e/i';\n    $userAgent = empty($_SERVER['HTTP_USER_AGENT']) ? FALSE : $_SERVER['HTTP_USER_AGENT'];\n    $isBot = !$userAgent || preg_match($bot_regex, $userAgent);\n\n    return $isBot;\n}\n\nAnyway take care that some bots uses browser like user agent to fake their identity\n ( I got many russian ip that has this behaviour on my site )\nOne distinctive feature of most of the bot is that they don't carry any cookie and so no session is attached to them.\n( I am not sure how but this is for sure the best way to track them ) \n"", ""\nYou could analyse the user agent ($_SERVER['HTTP_USER_AGENT']) or compare the client’s IP address ($_SERVER['REMOTE_ADDR']) with a list of IP addresses of search engine bots.\n"", '\nUse Device Detector open source library, it offers a isBot() function: https://github.com/piwik/device-detector\n', ""\nI made one good and fast function for this\nfunction is_bot(){\n\n        if(isset($_SERVER['HTTP_USER_AGENT']))\n        {\n            return preg_match('/rambler|abacho|acoi|accona|aspseek|altavista|estyle|scrubby|lycos|geona|ia_archiver|alexa|sogou|skype|facebook|twitter|pinterest|linkedin|naver|bing|google|yahoo|duckduckgo|yandex|baidu|teoma|xing|java\\/1.7.0_45|bot|crawl|slurp|spider|mediapartners|\\sask\\s|\\saol\\s/i', $_SERVER['HTTP_USER_AGENT']);\n        }\n\n        return false;\n    }\n\nThis cover 99% of all possible bots, search engines etc.\n"", '\n <?php // IPCLOACK HOOK\nif (CLOAKING_LEVEL != 4) {\n    $lastupdated = date(""Ymd"", filemtime(FILE_BOTS));\n    if ($lastupdated != date(""Ymd"")) {\n        $lists = array(\n        \'http://labs.getyacg.com/spiders/google.txt\',\n        \'http://labs.getyacg.com/spiders/inktomi.txt\',\n        \'http://labs.getyacg.com/spiders/lycos.txt\',\n        \'http://labs.getyacg.com/spiders/msn.txt\',\n        \'http://labs.getyacg.com/spiders/altavista.txt\',\n        \'http://labs.getyacg.com/spiders/askjeeves.txt\',\n        \'http://labs.getyacg.com/spiders/wisenut.txt\',\n        );\n        foreach($lists as $list) {\n            $opt .= fetch($list);\n        }\n        $opt = preg_replace(""/(^[\\r\\n]*|[\\r\\n]+)[\\s\\t]*[\\r\\n]+/"", ""\\n"", $opt);\n        $fp =  fopen(FILE_BOTS,""w"");\n        fwrite($fp,$opt);\n        fclose($fp);\n    }\n    $ip = isset($_SERVER[\'REMOTE_ADDR\']) ? $_SERVER[\'REMOTE_ADDR\'] : \'\';\n    $ref = isset($_SERVER[\'HTTP_REFERER\']) ? $_SERVER[\'HTTP_REFERER\'] : \'\';\n    $agent = isset($_SERVER[\'HTTP_USER_AGENT\']) ? $_SERVER[\'HTTP_USER_AGENT\'] : \'\';\n    $host = strtolower(gethostbyaddr($ip));\n    $file = implode("" "", file(FILE_BOTS));\n    $exp = explode(""."", $ip);\n    $class = $exp[0].\'.\'.$exp[1].\'.\'.$exp[2].\'.\';\n    $threshold = CLOAKING_LEVEL;\n    $cloak = 0;\n    if (stristr($host, ""googlebot"") && stristr($host, ""inktomi"") && stristr($host, ""msn"")) {\n        $cloak++;\n    }\n    if (stristr($file, $class)) {\n        $cloak++;\n    }\n    if (stristr($file, $agent)) {\n        $cloak++;\n    }\n    if (strlen($ref) > 0) {\n        $cloak = 0;\n    }\n\n    if ($cloak >= $threshold) {\n        $cloakdirective = 1;\n    } else {\n        $cloakdirective = 0;\n    }\n}\n?>\n\nThat would be the ideal way to cloak for spiders. It\'s from an open source script called [YACG] - http://getyacg.com\nNeeds a bit of work, but definitely the way to go.\n', ""\n100% Working Bot detector. It is working on my website successfully.\nfunction isBotDetected() {\n\n    if ( preg_match('/abacho|accona|AddThis|AdsBot|ahoy|AhrefsBot|AISearchBot|alexa|altavista|anthill|appie|applebot|arale|araneo|AraybOt|ariadne|arks|aspseek|ATN_Worldwide|Atomz|baiduspider|baidu|bbot|bingbot|bing|Bjaaland|BlackWidow|BotLink|bot|boxseabot|bspider|calif|CCBot|ChinaClaw|christcrawler|CMC\\/0\\.01|combine|confuzzledbot|contaxe|CoolBot|cosmos|crawler|crawlpaper|crawl|curl|cusco|cyberspyder|cydralspider|dataprovider|digger|DIIbot|DotBot|downloadexpress|DragonBot|DuckDuckBot|dwcp|EasouSpider|ebiness|ecollector|elfinbot|esculapio|ESI|esther|eStyle|Ezooms|facebookexternalhit|facebook|facebot|fastcrawler|FatBot|FDSE|FELIX IDE|fetch|fido|find|Firefly|fouineur|Freecrawl|froogle|gammaSpider|gazz|gcreep|geona|Getterrobo-Plus|get|girafabot|golem|googlebot|\\-google|grabber|GrabNet|griffon|Gromit|gulliver|gulper|hambot|havIndex|hotwired|htdig|HTTrack|ia_archiver|iajabot|IDBot|Informant|InfoSeek|InfoSpiders|INGRID\\/0\\.1|inktomi|inspectorwww|Internet Cruiser Robot|irobot|Iron33|JBot|jcrawler|Jeeves|jobo|KDD\\-Explorer|KIT\\-Fireball|ko_yappo_robot|label\\-grabber|larbin|legs|libwww-perl|linkedin|Linkidator|linkwalker|Lockon|logo_gif_crawler|Lycos|m2e|majesticsEO|marvin|mattie|mediafox|mediapartners|MerzScope|MindCrawler|MJ12bot|mod_pagespeed|moget|Motor|msnbot|muncher|muninn|MuscatFerret|MwdSearch|NationalDirectory|naverbot|NEC\\-MeshExplorer|NetcraftSurveyAgent|NetScoop|NetSeer|newscan\\-online|nil|none|Nutch|ObjectsSearch|Occam|openstat.ru\\/Bot|packrat|pageboy|ParaSite|patric|pegasus|perlcrawler|phpdig|piltdownman|Pimptrain|pingdom|pinterest|pjspider|PlumtreeWebAccessor|PortalBSpider|psbot|rambler|Raven|RHCS|RixBot|roadrunner|Robbie|robi|RoboCrawl|robofox|Scooter|Scrubby|Search\\-AU|searchprocess|search|SemrushBot|Senrigan|seznambot|Shagseeker|sharp\\-info\\-agent|sift|SimBot|Site Valet|SiteSucker|skymob|SLCrawler\\/2\\.0|slurp|snooper|solbot|speedy|spider_monkey|SpiderBot\\/1\\.0|spiderline|spider|suke|tach_bw|TechBOT|TechnoratiSnoop|templeton|teoma|titin|topiclink|twitterbot|twitter|UdmSearch|Ukonline|UnwindFetchor|URL_Spider_SQL|urlck|urlresolver|Valkyrie libwww\\-perl|verticrawl|Victoria|void\\-bot|Voyager|VWbot_K|wapspider|WebBandit\\/1\\.0|webcatcher|WebCopier|WebFindBot|WebLeacher|WebMechanic|WebMoose|webquest|webreaper|webspider|webs|WebWalker|WebZip|wget|whowhere|winona|wlm|WOLP|woriobot|WWWC|XGET|xing|yahoo|YandexBot|YandexMobileBot|yandex|yeti|Zeus/i', $_SERVER['HTTP_USER_AGENT'])\n    ) {\n        return true; // 'Above given bots detected'\n    }\n\n    return false;\n\n} // End :: isBotDetected()\n\n"", '\nI\'m using this code, pretty good. You will very easy to know user-agents visitted your site. This code is opening a file and write the user_agent down the file. You can check each day this file by go to yourdomain.com/useragent.txt and know about new user_agents and put them in your condition of if clause.\n$user_agent = strtolower($_SERVER[\'HTTP_USER_AGENT\']);\nif(!preg_match(""/Googlebot|MJ12bot|yandexbot/i"", $user_agent)){\n    // if not meet the conditions then\n    // do what you need\n\n    // here open a file and write the user_agent down the file. You can check each day this file useragent.txt and know about new user_agents and put them in your condition of if clause\n    if($user_agent!=""""){\n        $myfile = fopen(""useragent.txt"", ""a"") or die(""Unable to open file useragent.txt!"");\n        fwrite($myfile, $user_agent);\n        $user_agent = ""\\n"";\n        fwrite($myfile, $user_agent);\n        fclose($myfile);\n    }\n}\n\nThis is the content of useragent.txt\nMozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (compatible; MJ12bot/v1.4.6; http://mj12bot.com/)Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nMozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)mozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (compatible; yandexbot/3.0; +http://yandex.com/bots)\nmozilla/5.0 (iphone; cpu iphone os 9_3 like mac os x) applewebkit/601.1.46 (khtml, like gecko) version/9.0 mobile/13e198 safari/601.1\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; linkdexbot/2.2; +http://www.linkdex.com/bots/)\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:49.0) gecko/20100101 firefox/49.0\nmozilla/5.0 (windows nt 6.1; wow64; rv:33.0) gecko/20100101 firefox/33.0\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/53.0.2785.143 safari/537.36\nmozilla/5.0 (compatible; baiduspider/2.0; +http://www.baidu.com/search/spider.html)\nzoombot (linkbot 1.0 http://suite.seozoom.it/bot.html)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\nsogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)\nmozilla/5.0 (windows nt 10.0; wow64) applewebkit/537.36 (khtml, like gecko) chrome/44.0.2403.155 safari/537.36 opr/31.0.1889.174\n\n', ""\nFor Google i'm using this method.\nfunction is_google() {\n    $ip   = $_SERVER['REMOTE_ADDR'];\n    $host = gethostbyaddr( $ip );\n    if ( strpos( $host, '.google.com' ) !== false || strpos( $host, '.googlebot.com' ) !== false ) {\n\n        $forward_lookup = gethostbyname( $host );\n\n        if ( $forward_lookup == $ip ) {\n            return true;\n        }\n\n        return false;\n    } else {\n        return false;\n    }\n\n}\n\nvar_dump( is_google() );\n\nCredits: https://support.google.com/webmasters/answer/80553\n"", '\nVerifying Googlebot\nAs useragent can be changed...\n\nthe only official supported way to identify a google bot is to run a\nreverse DNS lookup on the accessing IP address and run a forward DNS\nlookup on the result to verify that it points to accessing IP address\nand the resulting domain name is in either googlebot.com or google.com\ndomain.\n\nTaken from here.\n\nso you must run a DNS lookup\n\nBoth, reverse and forward.\nSee this guide on Google Search Central.\n', ""\nfunction bot_detected() {\n\n  if(preg_match('/bot|crawl|slurp|spider|mediapartners/i', $_SERVER['HTTP_USER_AGENT']){\n    return true;\n  }\n  else{\n    return false;\n  }\n}\n\n"", '\nmight be late, but what about a hidden a link. All bots will use the rel attribute follow, only bad bots will use the nofollow rel attribute.\n<a style=""display:none;"" rel=""follow"" href=""javascript:void(0);"" onclick=""isabot();"">.</a>\n\nfunction isabot(){\n//define a variable to pass with ajax to php\n// || send bots info direct to where ever.\nisabot = true;\n}\n\nfor a bad bot you can use this:\n<a style=""display:none;"" href=""javascript:void(0);"" rel=""nofollow"" onclick=""isBadbot();"">.</a>\n\nfor PHP specific you can remove the onclick attribute and replace the href attribute with a link to your ip detector/ bot detector like so:\n<a style=""display:none;"" rel=""follow"" href=""https://somedomain.com/botdetector.php"">.</a>\n\nOR\n<a style=""display:none;"" rel=""nofollow"" href=""https://somedomain.com/badbotdetector.php"">.</a>\n\nyou can work with it and maybe use both, one detects a bot, while the other proves it to be a bad bot.\nhope you find this useful\n']",https://stackoverflow.com/questions/677419/how-to-detect-search-engine-bots-with-php,web-crawler
How do I make a simple crawler in PHP? [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    


Closed 3 years ago.










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I have a web page with a bunch of links. I want to write a script which would dump all the data contained in those links in a local file.
Has anybody done that with PHP? General guidelines and gotchas would suffice as an answer.
",202k,,"['\nMeh. Don\'t parse HTML with regexes.\nHere\'s a DOM version inspired by Tatu\'s:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n    static $seen = array();\n    if (isset($seen[$url]) || $depth === 0) {\n        return;\n    }\n\n    $seen[$url] = true;\n\n    $dom = new DOMDocument(\'1.0\');\n    @$dom->loadHTMLFile($url);\n\n    $anchors = $dom->getElementsByTagName(\'a\');\n    foreach ($anchors as $element) {\n        $href = $element->getAttribute(\'href\');\n        if (0 !== strpos($href, \'http\')) {\n            $path = \'/\' . ltrim($href, \'/\');\n            if (extension_loaded(\'http\')) {\n                $href = http_build_url($url, array(\'path\' => $path));\n            } else {\n                $parts = parse_url($url);\n                $href = $parts[\'scheme\'] . \'://\';\n                if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                    $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                }\n                $href .= $parts[\'host\'];\n                if (isset($parts[\'port\'])) {\n                    $href .= \':\' . $parts[\'port\'];\n                }\n                $href .= dirname($parts[\'path\'], 1).$path;\n            }\n        }\n        crawl_page($href, $depth - 1);\n    }\n    echo ""URL:"",$url,PHP_EOL,""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL;\n}\ncrawl_page(""http://hobodave.com"", 2);\n\nEdit: I fixed some bugs from Tatu\'s version (works with relative URLs now).\nEdit: I added a new bit of functionality that prevents it from following the same URL twice.\nEdit: echoing output to STDOUT now so you can redirect it to whatever file you want\nEdit: Fixed a bug pointed out by George in his answer. Relative urls will no longer append to the end of the url path, but overwrite it. Thanks to George for this. Note that George\'s answer doesn\'t account for any of: https, user, pass, or port. If you have the http PECL extension loaded this is quite simply done using http_build_url. Otherwise, I have to manually glue together using parse_url. Thanks again George.\n', '\nHere my implementation based on the above example/answer.\n\nIt is class based \nuses Curl\nsupport HTTP Auth\nSkip Url not belonging to the base domain\nReturn Http header Response Code for each page\nReturn time for each page\n\nCRAWL CLASS:\nclass crawler\n{\n    protected $_url;\n    protected $_depth;\n    protected $_host;\n    protected $_useHttpAuth = false;\n    protected $_user;\n    protected $_pass;\n    protected $_seen = array();\n    protected $_filter = array();\n\n    public function __construct($url, $depth = 5)\n    {\n        $this->_url = $url;\n        $this->_depth = $depth;\n        $parse = parse_url($url);\n        $this->_host = $parse[\'host\'];\n    }\n\n    protected function _processAnchors($content, $url, $depth)\n    {\n        $dom = new DOMDocument(\'1.0\');\n        @$dom->loadHTML($content);\n        $anchors = $dom->getElementsByTagName(\'a\');\n\n        foreach ($anchors as $element) {\n            $href = $element->getAttribute(\'href\');\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($url, array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($url);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            // Crawl only link that belongs to the start domain\n            $this->crawl_page($href, $depth - 1);\n        }\n    }\n\n    protected function _getContent($url)\n    {\n        $handle = curl_init($url);\n        if ($this->_useHttpAuth) {\n            curl_setopt($handle, CURLOPT_HTTPAUTH, CURLAUTH_ANY);\n            curl_setopt($handle, CURLOPT_USERPWD, $this->_user . "":"" . $this->_pass);\n        }\n        // follows 302 redirect, creates problem wiht authentication\n//        curl_setopt($handle, CURLOPT_FOLLOWLOCATION, TRUE);\n        // return the content\n        curl_setopt($handle, CURLOPT_RETURNTRANSFER, TRUE);\n\n        /* Get the HTML or whatever is linked in $url. */\n        $response = curl_exec($handle);\n        // response total time\n        $time = curl_getinfo($handle, CURLINFO_TOTAL_TIME);\n        /* Check for 404 (file not found). */\n        $httpCode = curl_getinfo($handle, CURLINFO_HTTP_CODE);\n\n        curl_close($handle);\n        return array($response, $httpCode, $time);\n    }\n\n    protected function _printResult($url, $depth, $httpcode, $time)\n    {\n        ob_end_flush();\n        $currentDepth = $this->_depth - $depth;\n        $count = count($this->_seen);\n        echo ""N::$count,CODE::$httpcode,TIME::$time,DEPTH::$currentDepth URL::$url <br>"";\n        ob_start();\n        flush();\n    }\n\n    protected function isValid($url, $depth)\n    {\n        if (strpos($url, $this->_host) === false\n            || $depth === 0\n            || isset($this->_seen[$url])\n        ) {\n            return false;\n        }\n        foreach ($this->_filter as $excludePath) {\n            if (strpos($url, $excludePath) !== false) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public function crawl_page($url, $depth)\n    {\n        if (!$this->isValid($url, $depth)) {\n            return;\n        }\n        // add to the seen URL\n        $this->_seen[$url] = true;\n        // get Content and Return Code\n        list($content, $httpcode, $time) = $this->_getContent($url);\n        // print Result for current Page\n        $this->_printResult($url, $depth, $httpcode, $time);\n        // process subPages\n        $this->_processAnchors($content, $url, $depth);\n    }\n\n    public function setHttpAuth($user, $pass)\n    {\n        $this->_useHttpAuth = true;\n        $this->_user = $user;\n        $this->_pass = $pass;\n    }\n\n    public function addFilterPath($path)\n    {\n        $this->_filter[] = $path;\n    }\n\n    public function run()\n    {\n        $this->crawl_page($this->_url, $this->_depth);\n    }\n}\n\nUSAGE:\n// USAGE\n$startURL = \'http://YOUR_URL/\';\n$depth = 6;\n$username = \'YOURUSER\';\n$password = \'YOURPASS\';\n$crawler = new crawler($startURL, $depth);\n$crawler->setHttpAuth($username, $password);\n// Exclude path with the following structure to be processed \n$crawler->addFilterPath(\'customer/account/login/referer\');\n$crawler->run();\n\n', '\nCheck out PHP Crawler\nhttp://sourceforge.net/projects/php-crawler/\nSee if it helps.\n', '\nIn it\'s simplest form:\nfunction crawl_page($url, $depth = 5) {\n    if($depth > 0) {\n        $html = file_get_contents($url);\n\n        preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $html, $matches);\n\n        foreach($matches[1] as $newurl) {\n            crawl_page($newurl, $depth - 1);\n        }\n\n        file_put_contents(\'results.txt\', $newurl.""\\n\\n"".$html.""\\n\\n"", FILE_APPEND);\n    }\n}\n\ncrawl_page(\'http://www.domain.com/index.php\', 5);\n\nThat function will get contents from a page, then crawl all found links and save the contents to \'results.txt\'. The functions accepts an second parameter, depth, which defines how long the links should be followed. Pass 1 there if you want to parse only links from the given page.\n', '\nWhy use PHP for this, when you can use wget, e.g.\nwget -r -l 1 http://www.example.com\n\nFor how to parse the contents, see Best Methods to parse HTML and use the search function for examples. How to parse HTML has been answered multiple times before.\n', '\nWith some little changes to hobodave\'s code, here is a codesnippet you can use to crawl pages. This needs the curl extension to be enabled in your server.\n<?php\n//set_time_limit (0);\nfunction crawl_page($url, $depth = 5){\n$seen = array();\nif(($depth == 0) or (in_array($url, $seen))){\n    return;\n}   \n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\n$result = curl_exec ($ch);\ncurl_close ($ch);\nif( $result ){\n    $stripped_file = strip_tags($result, ""<a>"");\n    preg_match_all(""/<a[\\s]+[^>]*?href[\\s]?=[\\s\\""\\\']+"".""(.*?)[\\""\\\']+.*?>"".""([^<]+|.*?)?<\\/a>/"", $stripped_file, $matches, PREG_SET_ORDER ); \n    foreach($matches as $match){\n        $href = $match[1];\n            if (0 !== strpos($href, \'http\')) {\n                $path = \'/\' . ltrim($href, \'/\');\n                if (extension_loaded(\'http\')) {\n                    $href = http_build_url($href , array(\'path\' => $path));\n                } else {\n                    $parts = parse_url($href);\n                    $href = $parts[\'scheme\'] . \'://\';\n                    if (isset($parts[\'user\']) && isset($parts[\'pass\'])) {\n                        $href .= $parts[\'user\'] . \':\' . $parts[\'pass\'] . \'@\';\n                    }\n                    $href .= $parts[\'host\'];\n                    if (isset($parts[\'port\'])) {\n                        $href .= \':\' . $parts[\'port\'];\n                    }\n                    $href .= $path;\n                }\n            }\n            crawl_page($href, $depth - 1);\n        }\n}   \necho ""Crawled {$href}"";\n}   \ncrawl_page(""http://www.sitename.com/"",3);\n?>\n\nI have explained this tutorial in this crawler script tutorial\n', '\nHobodave you were very close. The only thing I have changed is within the if statement that checks to see if the href attribute of the found anchor tag begins with \'http\'. Instead of simply adding the $url variable which would contain the page that was passed in you must first strip it down to the host which can be done using the parse_url php function.\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  static $seen = array();\n  if (isset($seen[$url]) || $depth === 0) {\n    return;\n  }\n\n  $seen[$url] = true;\n\n  $dom = new DOMDocument(\'1.0\');\n  @$dom->loadHTMLFile($url);\n\n  $anchors = $dom->getElementsByTagName(\'a\');\n  foreach ($anchors as $element) {\n    $href = $element->getAttribute(\'href\');\n    if (0 !== strpos($href, \'http\')) {\n       /* this is where I changed hobodave\'s code */\n        $host = ""http://"".parse_url($url,PHP_URL_HOST);\n        $href = $host. \'/\' . ltrim($href, \'/\');\n    }\n    crawl_page($href, $depth - 1);\n  }\n\n  echo ""New Page:<br /> "";\n  echo ""URL:"",$url,PHP_EOL,""<br />"",""CONTENT:"",PHP_EOL,$dom->saveHTML(),PHP_EOL,PHP_EOL,""  <br /><br />"";\n}\n\ncrawl_page(""http://hobodave.com/"", 5);\n?>\n\n', ""\nAs mentioned, there are crawler frameworks all ready for customizing out there, but if what you're doing is as simple as you mentioned, you could make it from scratch pretty easily.\nScraping the links: http://www.phpro.org/examples/Get-Links-With-DOM.html\nDumping results to a file: http://www.tizag.com/phpT/filewrite.php\n"", ""\nI used @hobodave's code, with this little tweak to prevent re-crawling all fragment variants of the same URL:\n<?php\nfunction crawl_page($url, $depth = 5)\n{\n  $parts = parse_url($url);\n  if(array_key_exists('fragment', $parts)){\n    unset($parts['fragment']);\n    $url = http_build_url($parts);\n  }\n\n  static $seen = array();\n  ...\n\nThen you can also omit the $parts = parse_url($url); line within the for loop.\n"", '\nYou can try this it may be help to you\n$search_string = \'american golf News: Fowler beats stellar field in Abu Dhabi\';\n$html = file_get_contents(url of the site);\n$dom = new DOMDocument;\n$titalDom = new DOMDocument;\n$tmpTitalDom = new DOMDocument;\nlibxml_use_internal_errors(true);\n@$dom->loadHTML($html);\nlibxml_use_internal_errors(false);\n$xpath = new DOMXPath($dom);\n$videos = $xpath->query(\'//div[@class=""primary-content""]\');\nforeach ($videos as $key => $video) {\n$newdomaindom = new DOMDocument;    \n$newnode = $newdomaindom->importNode($video, true);\n$newdomaindom->appendChild($newnode);\n@$titalDom->loadHTML($newdomaindom->saveHTML());\n$xpath1 = new DOMXPath($titalDom);\n$titles = $xpath1->query(\'//div[@class=""listingcontainer""]/div[@class=""list""]\');\nif(strcmp(preg_replace(\'!\\s+!\',\' \',  $titles->item(0)->nodeValue),$search_string)){     \n    $tmpNode = $tmpTitalDom->importNode($video, true);\n    $tmpTitalDom->appendChild($tmpNode);\n    break;\n}\n}\necho $tmpTitalDom->saveHTML();\n\n', '\nThank you @hobodave.\nHowever I found two weaknesses in your code.\nYour parsing of the original url to get the ""host"" segment stops at the first single slash. This presumes that all relative links start in the root directory. This only true sometimes.\noriginal url   :  http://example.com/game/index.html\nhref in <a> tag:  highscore.html\nauthor\'s intent:  http://example.com/game/highscore.html  <-200->\ncrawler result :  http://example.com/highscore.html       <-404->\n\nfix this by breaking at the last single slash not the first\na second unrelated bug, is that $depth does not really track recursion depth, it tracks breadth of the first level of recursion. \nIf I believed this page were in active use I might debug this second issue, but I suspect the text I am writing now will never be read by anyone, human or robot, since this issue is six years old and I do not even have enough reputation to notify +hobodave directly about these defects by commmenting on his code. Thanks anyway hobodave.\n', '\nI came up with the following spider code.\nI adapted it a bit from the following:\nPHP - Is the there a safe way to perform deep recursion?\nit seems fairly rapid....\n    <?php\nfunction  spider( $base_url , $search_urls=array() ) {\n    $queue[] = $base_url;\n    $done           =   array();\n    $found_urls     =   array();\n    while($queue) {\n            $link = array_shift($queue);\n            if(!is_array($link)) {\n                $done[] = $link;\n                foreach( $search_urls as $s) { if (strstr( $link , $s )) { $found_urls[] = $link; } }\n                if( empty($search_urls)) { $found_urls[] = $link; }\n                if(!empty($link )) {\necho \'LINK:::\'.$link;\n                      $content =    file_get_contents( $link );\n//echo \'P:::\'.$content;\n                    preg_match_all(\'~<a.*?href=""(.*?)"".*?>~\', $content, $sublink);\n                    if (!in_array($sublink , $done) && !in_array($sublink , $queue)  ) {\n                           $queue[] = $sublink;\n                    }\n                }\n            } else {\n                    $result=array();\n                    $return = array();\n                    // flatten multi dimensional array of URLs to one dimensional.\n                    while(count($link)) {\n                         $value = array_shift($link);\n                         if(is_array($value))\n                             foreach($value as $sub)\n                                $link[] = $sub;\n                         else\n                               $return[] = $value;\n                     }\n                     // now loop over one dimensional array.\n                     foreach($return as $link) {\n                                // echo \'L::\'.$link;\n                                // url may be in form <a href.. so extract what\'s in the href bit.\n                                preg_match_all(\'/<a[^>]+href=([\\\'""])(?<href>.+?)\\1[^>]*>/i\', $link, $result);\n                                if ( isset( $result[\'href\'][0] )) { $link = $result[\'href\'][0]; }\n                                // add the new URL to the queue.\n                                if( (!strstr( $link , ""http"")) && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                     $queue[]=$base_url.$link;\n                                } else {\n                                    if ( (strstr( $link , $base_url  ))  && (!in_array($base_url.$link , $done)) && (!in_array($base_url.$link , $queue)) ) {\n                                         $queue[] = $link;\n                                    }\n                                }\n                      }\n            }\n    }\n\n\n    return $found_urls;\n}    \n\n\n    $base_url       =   \'https://www.houseofcheese.co.uk/\';\n    $search_urls    =   array(  $base_url.\'acatalog/\' );\n    $done = spider( $base_url  , $search_urls  );\n\n    //\n    // RESULT\n    //\n    //\n    echo \'<br /><br />\';\n    echo \'RESULT:::\';\n    foreach(  $done as $r )  {\n        echo \'URL:::\'.$r.\'<br />\';\n    }\n\n', '\nIts worth remembering that when crawling external links (I do appreciate the OP relates to a users own page) you should be aware of robots.txt. I have found the following which will hopefully help http://www.the-art-of-web.com/php/parse-robots/.\n', '\nI created a small class to grab data from the provided url, then extract html elements of your choice. The class makes use of CURL and DOMDocument.\nphp class:\nclass crawler {\n\n\n   public static $timeout = 2;\n   public static $agent   = \'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\';\n\n\n   public static function http_request($url) {\n      $ch = curl_init();\n      curl_setopt($ch, CURLOPT_URL,            $url);\n      curl_setopt($ch, CURLOPT_USERAGENT,      self::$agent);\n      curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, self::$timeout);\n      curl_setopt($ch, CURLOPT_TIMEOUT,        self::$timeout);\n      curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n      $response = curl_exec($ch);\n      curl_close($ch);\n      return $response;\n   }\n\n\n   public static function strip_whitespace($data) {\n      $data = preg_replace(\'/\\s+/\', \' \', $data);\n      return trim($data);\n   }\n\n\n   public static function extract_elements($tag, $data) {\n      $response = array();\n      $dom      = new DOMDocument;\n      @$dom->loadHTML($data);\n      foreach ( $dom->getElementsByTagName($tag) as $index => $element ) {\n         $response[$index][\'text\'] = self::strip_whitespace($element->nodeValue);\n         foreach ( $element->attributes as $attribute ) {\n            $response[$index][\'attributes\'][strtolower($attribute->nodeName)] = self::strip_whitespace($attribute->nodeValue);\n         }\n      }\n      return $response;\n   }\n\n\n}\n\n\nexample usage:\n$data  = crawler::http_request(\'https://stackoverflow.com/questions/2313107/how-do-i-make-a-simple-crawler-in-php\');\n$links = crawler::extract_elements(\'a\', $data);\nif ( count($links) > 0 ) {\n   file_put_contents(\'links.json\', json_encode($links, JSON_PRETTY_PRINT));\n}\n\n\nexample response:\n[\n    {\n        ""text"": ""Stack Overflow"",\n        ""attributes"": {\n            ""href"": ""https:\\/\\/stackoverflow.com"",\n            ""class"": ""-logo js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:8})""\n        }\n    },\n    {\n        ""text"": ""Questions"",\n        ""attributes"": {\n            ""id"": ""nav-questions"",\n            ""href"": ""\\/questions"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:true, location:2, destination:1})""\n        }\n    },\n    {\n        ""text"": ""Developer Jobs"",\n        ""attributes"": {\n            ""id"": ""nav-jobs"",\n            ""href"": ""\\/jobs?med=site-ui&ref=jobs-tab"",\n            ""class"": ""-link js-gps-track"",\n            ""data-gps-track"": ""top_nav.click({is_current:false, location:2, destination:6})""\n        }\n    }\n]\n\n', '\nIt\'s an old question. A lot of good things happened since then. Here are my two cents on this topic:\n\nTo accurately track the visited pages you have to normalize URI first. The normalization algorithm includes multiple steps:\n\nSort query parameters. For example, the following URIs are equivalent after normalization:\n\nGET http://www.example.com/query?id=111&cat=222\nGET http://www.example.com/query?cat=222&id=111\n\nConvert the empty path.\nExample: http://example.org → http://example.org/\nCapitalize percent encoding. All letters within a percent-encoding triplet (e.g., ""%3A"") are case-insensitive.\nExample: http://example.org/a%c2%B1b → http://example.org/a%C2%B1b\nRemove unnecessary dot-segments.\nExample: http://example.org/../a/b/../c/./d.html → http://example.org/a/c/d.html\nPossibly some other normalization rules\n\nNot only <a> tag has href attribute, <area> tag has it too https://html.com/tags/area/. If you don\'t want to miss anything, you have to scrape <area> tag too.\nTrack crawling progress. If the website is small, it is not a problem. Contrarily it might be very frustrating if you crawl half of the site and it failed. Consider using a database or a filesystem to store the progress.\nBe kind to the site owners.\nIf you are ever going to use your crawler outside of your website, you have to use delays. Without delays, the script is too fast and might significantly slow down some small sites. From sysadmins perspective, it looks like a DoS attack. A static delay between the requests will do the trick.\n\nIf you don\'t want to deal with that, try Crawlzone and let me know your feedback. Also, check out the article I wrote a while back https://www.codementor.io/zstate/this-is-how-i-crawl-n98s6myxm\n']",,web-crawler
How to run Scrapy from within a Python script,"
I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:
http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/
http://snipplr.com/view/67006/using-scrapy-from-a-script/
I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:
# This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. 
# 
# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.
# 
# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. 

#!/usr/bin/python
import os
os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports

from scrapy import log, signals, project
from scrapy.xlib.pydispatch import dispatcher
from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
from multiprocessing import Process, Queue

class CrawlerScript():

    def __init__(self):
        self.crawler = CrawlerProcess(settings)
        if not hasattr(project, 'crawler'):
            self.crawler.install()
        self.crawler.configure()
        self.items = []
        dispatcher.connect(self._item_passed, signals.item_passed)

    def _item_passed(self, item):
        self.items.append(item)

    def _crawl(self, queue, spider_name):
        spider = self.crawler.spiders.create(spider_name)
        if spider:
            self.crawler.queue.append_spider(spider)
        self.crawler.start()
        self.crawler.stop()
        queue.put(self.items)

    def crawl(self, spider):
        queue = Queue()
        p = Process(target=self._crawl, args=(queue, spider,))
        p.start()
        p.join()
        return queue.get(True)

# Usage
if __name__ == ""__main__"":
    log.start()

    """"""
    This example runs spider1 and then spider2 three times. 
    """"""
    items = list()
    crawler = CrawlerScript()
    items.append(crawler.crawl('spider1'))
    for i in range(3):
        items.append(crawler.crawl('spider2'))
    print items

# Snippet imported from snippets.scrapy.org (which no longer works)
# author: joehillen
# date  : Oct 24, 2010

Thank you.
",83k,"
            85
        ","[""\nAll other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n\n"", '\nSimply we can use\nfrom scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n\nUse these arguments inside spider __init__ function with the global scope.\n', ""\nThough I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n\nFrom what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.\n"", '\nIn scrapy 0.19.x you should do this:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain=\'scrapinghub.com\')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n\nNote these lines     \nsettings = get_project_settings()\ncrawler = Crawler(settings)\n\nWithout it your spider won\'t use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn\'t saving my items. I sent a pull request to fix the doc example.\nOne more to do so is just call command directly from you script\nfrom scrapy import cmdline\ncmdline.execute(""scrapy crawl followall"".split())  #followall is the spider\'s name\n\nCopied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286\n', '\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. \nHowever, I found while doing my project that using \nos.system(""scrapy crawl yourspider"")\n\nis the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.\nIf Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:\ndef _crawl(spider_name=None):\n    if spider_name:\n        os.system(\'scrapy crawl %s\' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = [\'spider1\', \'spider2\', \'spider2\']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n\n', '\nit  is an improvement of\nScrapy throws an error when run using crawlerprocess\nand https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\nFirst create your usual spider for successful command line running. it is very very important that it should run and export data or image or file\nOnce it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.\nit will get necessary settings which ""from scrapy.utils.project import get_project_settings"" failed to do which is recommended by many\nboth above and below portions should be there together. only one don\'t run.\nSpider will run in scrapy.cfg folder not any other folder\ntree  diagram may be displayed by the moderators for reference\n#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r\'D:\\ivana\\flow\') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == ""__main__"":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n\n', ""\n# -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n\nPut this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)\n"", ""\nIf you want to run a simple crawling, It's easy by just running command: \nscrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. \nscrapy crawl  -o result.csv or result.json or result.xml. \nyou may want to try it\n""]",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,web-crawler
Scrapy - Reactor not Restartable [duplicate],"






This question already has answers here:
                        
                    



ReactorNotRestartable error in while loop with scrapy

                                (10 answers)
                            

Closed 3 years ago.



with:
from twisted.internet import reactor
from scrapy.crawler import CrawlerProcess

I've always ran this process sucessfully:
process = CrawlerProcess(get_project_settings())
process.crawl(*args)
# the script will block here until the crawling is finished
process.start() 

but since I've moved this code into a web_crawler(self) function, like so:
def web_crawler(self):
    # set up a crawler
    process = CrawlerProcess(get_project_settings())
    process.crawl(*args)
    # the script will block here until the crawling is finished
    process.start() 

    # (...)

    return (result1, result2) 

and started calling the method using class instantiation, like:
def __call__(self):
    results1 = test.web_crawler()[1]
    results2 = test.web_crawler()[0]

and running:
test()

I am getting the following error:
Traceback (most recent call last):
  File ""test.py"", line 573, in <module>
    print (test())
  File ""test.py"", line 530, in __call__
    artists = test.web_crawler()
  File ""test.py"", line 438, in web_crawler
    process.start() 
  File ""/Library/Python/2.7/site-packages/scrapy/crawler.py"", line 280, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1194, in run
    self.startRunning(installSignalHandlers=installSignalHandlers)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 1174, in startRunning
    ReactorBase.startRunning(self)
  File ""/Library/Python/2.7/site-packages/twisted/internet/base.py"", line 684, in startRunning
    raise error.ReactorNotRestartable()
twisted.internet.error.ReactorNotRestartable

what is wrong?
",38k,"
            29
        ","['\nYou cannot restart the reactor, but you should be able to run it more times by forking a separate process:\nimport scrapy\nimport scrapy.crawler as crawler\nfrom scrapy.utils.log import configure_logging\nfrom multiprocessing import Process, Queue\nfrom twisted.internet import reactor\n\n# your spider\nclass QuotesSpider(scrapy.Spider):\n    name = ""quotes""\n    start_urls = [\'http://quotes.toscrape.com/tag/humor/\']\n\n    def parse(self, response):\n        for quote in response.css(\'div.quote\'):\n            print(quote.css(\'span.text::text\').extract_first())\n\n\n# the wrapper to make it run more times\ndef run_spider(spider):\n    def f(q):\n        try:\n            runner = crawler.CrawlerRunner()\n            deferred = runner.crawl(spider)\n            deferred.addBoth(lambda _: reactor.stop())\n            reactor.run()\n            q.put(None)\n        except Exception as e:\n            q.put(e)\n\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    result = q.get()\n    p.join()\n\n    if result is not None:\n        raise result\n\nRun it twice:\nconfigure_logging()\n\nprint(\'first run:\')\nrun_spider(QuotesSpider)\n\nprint(\'\\nsecond run:\')\nrun_spider(QuotesSpider)\n\nResult:\nfirst run:\n“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n“A day without sunshine is like, you know, night.”\n...\n\nsecond run:\n“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n“A day without sunshine is like, you know, night.”\n...\n\n', '\nThis is what helped for me to win the battle against ReactorNotRestartable error: last answer from the author of the question\n0) pip install crochet\n1) import from crochet import setup\n2) setup() - at the top of the file\n3) remove 2 lines:\na) d.addBoth(lambda _: reactor.stop())\nb) reactor.run()\n\nI had the same problem with this error, and spend 4+ hours to solve this problem, read all questions here about it. Finally found that one - and share it. That is how i solved this. The only meaningful lines from Scrapy docs left are 2 last lines in this my code:\n#some more imports\nfrom crochet import setup\nsetup()\n\ndef run_spider(spiderName):\n    module_name=""first_scrapy.spiders.{}"".format(spiderName)\n    scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n    spiderObj=scrapy_var.mySpider()           #get mySpider-object from spider module\n    crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n    crawler.crawl(spiderObj)                          #from Scrapy docs\n\nThis code allows me to select what spider to run just with its name passed to run_spider function and after scrapping finishes - select another spider and run it again. \nHope this will help somebody, as it helped for me :)\n', '\nAs per the Scrapy documentation, the start() method of the CrawlerProcess class does the following:\n\n""[...] starts a Twisted reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE.""\n\nThe error you are receiving is being thrown by Twisted, because a Twisted reactor cannot be restarted.  It uses a ton of globals, and even if you do jimmy-rig some sort of code to restart it (I\'ve seen it done), there\'s no guarantee it will work.  \nHonestly, if you think you need to restart the reactor, you\'re likely doing something wrong.\nDepending on what you want to do, I would also review the Running Scrapy from a Script portion of the documentation, too.\n', '\nAs some people pointed out already: You shouldn\'t need to restart the reactor.\nIdeally if you want to chain your processes (crawl1 then crawl2 then crawl3) you simply add callbacks.\nFor example, I\'ve been using this loop spider that follows this pattern:\n1. Crawl A\n2. Sleep N\n3. goto 1\n\nAnd this is how it looks in scrapy:\nimport time\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nfrom twisted.internet import reactor\n\nclass HttpbinSpider(scrapy.Spider):\n    name = \'httpbin\'\n    allowed_domains = [\'httpbin.org\']\n    start_urls = [\'http://httpbin.org/ip\']\n\n    def parse(self, response):\n        print(response.body)\n\ndef sleep(_, duration=5):\n    print(f\'sleeping for: {duration}\')\n    time.sleep(duration)  # block here\n\n\ndef crawl(runner):\n    d = runner.crawl(HttpbinSpider)\n    d.addBoth(sleep)\n    d.addBoth(lambda _: crawl(runner))\n    return d\n\n\ndef loop_crawl():\n    runner = CrawlerRunner(get_project_settings())\n    crawl(runner)\n    reactor.run()\n\n\nif __name__ == \'__main__\':\n    loop_crawl()\n\nTo explain the process more the crawl function schedules a crawl and adds two extra callbacks that are being called when crawling is over: blocking sleep and recursive call to itself (schedule another crawl).\n$ python endless_crawl.py \nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\nb\'{\\n  ""origin"": ""000.000.000.000""\\n}\\n\'\nsleeping for: 5\n\n', '\nThe mistake is in this code:\ndef __call__(self):\n    result1 = test.web_crawler()[1]\n    result2 = test.web_crawler()[0] # here\n\nweb_crawler() returns two results, and for that purpose it is trying to start the process twice, restarting the Reactor, as pointed by @Rejected.\nobtaining results running one single process, and storing both results in a tuple, is the way to go here:\ndef __call__(self):\n    result1, result2 = test.web_crawler()\n\n', '\nThis solved my problem,put below code after reactor.run() or process.start():\ntime.sleep(0.5)\n\nos.execl(sys.executable, sys.executable, *sys.argv)\n\n']",https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable,web-crawler
How to pass a user defined argument in scrapy spider,"
I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?
I read about a parameter -a somewhere but have no idea how to use it.
",75k,"
            125
        ","[""\nSpider arguments are passed in the crawl command using the -a option. For example:\nscrapy crawl myspider -a category=electronics -a domain=system\n\nSpiders can access arguments as attributes:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def __init__(self, category='', **kwargs):\n        self.start_urls = [f'http://www.example.com/{category}']  # py36\n        super().__init__(**kwargs)  # python3\n\n    def parse(self, response)\n        self.log(self.domain)  # system\n\nTaken from the Scrapy doc: http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nUpdate 2013: Add second argument\nUpdate 2015: Adjust wording\nUpdate 2016: Use newer base class and add super, thanks @Birla\nUpdate 2017: Use Python3 super\n# previously\nsuper(MySpider, self).__init__(**kwargs)  # python2\n\nUpdate 2018: As @eLRuLL points out, spiders can access arguments as attributes\n"", ""\nPrevious answers were correct, but you don't have to declare the constructor (__init__) every time you want to code a scrapy's spider, you could just specify the parameters as before:\nscrapy crawl myspider -a parameter1=value1 -a parameter2=value2\n\nand in your spider code you can just use them as spider arguments:\nclass MySpider(Spider):\n    name = 'myspider'\n    ...\n    def parse(self, response):\n        ...\n        if self.parameter1 == value1:\n            # this is True\n\n        # or also\n        if getattr(self, parameter2) == value2:\n            # this is also True\n\nAnd it just works.\n"", '\nTo pass arguments with crawl command\n\nscrapy crawl myspider -a category=\'mycategory\' -a domain=\'example.com\'\n\nTo pass arguments to run on scrapyd replace -a with -d\n\ncurl http://your.ip.address.here:port/schedule.json -d \n   spider=myspider -d category=\'mycategory\' -d domain=\'example.com\'\n\nThe spider will receive arguments in its constructor.\n\nclass MySpider(Spider):\n    name=""myspider""\n    def __init__(self,category=\'\',domain=\'\', *args,**kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.category = category\n        self.domain = domain\nScrapy puts all the arguments as spider attributes and you can skip the init method completely. Beware use getattr method for getting those attributes so your code does not break.\n\nclass MySpider(Spider):\n    name=""myspider""\n    start_urls = (\'https://httpbin.org/ip\',)\n\n    def parse(self,response):\n        print getattr(self,\'category\',\'\')\n        print getattr(self,\'domain\',\'\')\n\n\n', '\nSpider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this-\n\nscrapy crawl myspider -a domain=""http://www.example.com""\n\nAnd receive arguments in spider\'s constructors:\nclass MySpider(BaseSpider):\n    name = \'myspider\'\n    def __init__(self, domain=\'\', *args, **kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.start_urls = [domain]\n        #\n\n...\nit will work :)\n', '\nAlternatively we can use ScrapyD which expose an API where we can pass the start_url and spider name. ScrapyD has api\'s to stop/start/status/list the spiders.\npip install scrapyd scrapyd-deploy\nscrapyd\nscrapyd-deploy local -p default\n\nscrapyd-deploy will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use.\nclass MySpider(CrawlSpider):\n\n    def __init__(self, start_urls, *args, **kwargs):\n        self.start_urls = start_urls.split(\'|\')\n        super().__init__(*args, **kwargs)\n    name = testspider\n\ncurl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls=""https://www.anyurl...|https://www.anyurl2""\nAdded advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule API\nRefer scrapyd API documentation for more details\n']",https://stackoverflow.com/questions/15611605/how-to-pass-a-user-defined-argument-in-scrapy-spider,web-crawler
python: [Errno 10054] An existing connection was forcibly closed by the remote host,"
I am writing python to crawl Twitter space using Twitter-py. I have set the crawler to sleep for a while (2 seconds) between each request to api.twitter.com. However, after some times of running (around 1), when the Twitter's rate limit not exceeded yet, I got this error.
[Errno 10054] An existing connection was forcibly closed by the remote host.

What are possible causes of this problem and how to solve this?
I have searched through and found that the Twitter server itself may force to close the connection due to many requests.
Thank you very much in advance.
",213k,"
            70
        ","[""\nThis can be caused by the two sides of the connection disagreeing over whether the connection timed out or not during a keepalive. (Your code tries to reused the connection just as the server is closing it because it has been idle for too long.) You should basically just retry the operation over a new connection. (I'm surprised your library doesn't do this automatically.)\n"", ""\nI know this is a very old question but it may be that you need to set the request headers. This solved it for me.\nFor example 'user-agent', 'accept' etc. here is an example with user-agent:\nurl = 'your-url-here'\nheaders = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\nr = requests.get(url, headers=headers)\n\n"", '\nthere are many causes such as \n\nThe network link between server and client may be temporarily going down.\nrunning out of system resources.\nsending malformed data.\n\nTo examine the problem in detail, you can use Wireshark.\nor you can just re-request or re-connect again.\n', '\nFor me this problem arised while trying to connect to the SAP Hana database. When I got this error, \nOperationalError: Lost connection to HANA server (ConnectionResetError(10054, \'An existing connection was forcibly closed by the remote host\', None, 10054, None))\nI tried to run the code for connection(mentioned below), which created that error, again and it worked. \n\n\n    import pyhdb\n    connection = pyhdb.connect(host=""example.com"",port=30015,user=""user"",password=""secret"")\n    cursor = connection.cursor()\n    cursor.execute(""SELECT \'Hello Python World\' FROM DUMMY"")\n    cursor.fetchone()\n    connection.close()\n\n\nIt was because the server refused to connect. It might require you to wait for a while and try again. Try closing the Hana Studio by logging off and then logging in again. Keep running the code for a number of times.\n', '\nI got the same error ([WinError 10054] An existing connection was forcibly closed by the remote host) with websocket-client after setting ping_interval = 2 in websocket.run_forever(). (I had multiple threads connecting to the same host.)\nSetting ping_interval = 10 and ping_timeout = 9 solved the issue. May be you need to reduce the amount of requests and stop making host busy otherwise it will forcibly disconnect you.\n', '\nI fixed it with a while try loop, waiting for the response to set the variable in order to exit the loop.\nWhen the connection has an exception, it waits five seconds, and continues looking for the response from the connection.\nMy code before fix, with the failed response HTTPSConnectionPool(host=\'etc.com\', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E9955A2050>, \'Connection to example.net timed out. (connect timeout=None)\'))\n \n\nfrom __future__ import print_function\nimport sys\nimport requests\n\n\ndef condition_questions(**kwargs):\n    proxies = {\'https\': \'example.com\', \'http\': \'example.com:3128\'}\n    print(kwargs, file=sys.stdout)\n    headers = {\'etc\':\'etc\',}\n    body = f\'\'\'<etc>\n                </etc>\'\'\'\n\n    try:\n        response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n    except Exception as ex:\n        print(""exception"", ex, file=sys.stdout)\n        log.exception(ex)\n    finally:\n        print(""response_xml"", response_xml, file=sys.stdout)\n        return response_xml\n\nAfter fix, with successful response response_xml <Response [200]>:\n\nimport time\n...\n\nresponse_xml = \'\'\n    while response_xml == \'\':\n        try:\n            response_xml = requests.post(\'https://example.com\', data=body, headers=headers, proxies=proxies)\n            break\n        except Exception as ex:\n            print(""exception"", ex, file=sys.stdout)\n            log.exception(ex)\n            time.sleep(5)\n            continue\n        finally:\n            print(""response_xml"", response_xml, file=sys.stdout)\n            return response_xml\n\nbased on Jatin\'s answer here  --""Just do this,\nimport time\n\npage = \'\'\nwhile page == \'\':\n    try:\n        page = requests.get(url)\n        break\n    except:\n        print(""Connection refused by the server.."")\n        print(""Let me sleep for 5 seconds"")\n        print(""ZZzzzz..."")\n        time.sleep(5)\n        print(""Was a nice sleep, now let me continue..."")\n        continue\n\nYou\'re welcome :)""\n']",https://stackoverflow.com/questions/8814802/python-errno-10054-an-existing-connection-was-forcibly-closed-by-the-remote-h,web-crawler
Anyone know of a good Python based web crawler that I could use?,"









Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                        
                    





I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of open source crawlers but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project.
Thanks in advance.  Also, this is my first SO question!
",98k,,"[""\n\nMechanize is my favorite; great high-level browsing capabilities (super-simple form filling and submission).\nTwill is a simple scripting language built on top of Mechanize\nBeautifulSoup + urllib2 also works quite nicely.\nScrapy looks like an extremely promising project; it's new.\n\n"", '\nUse Scrapy.\nIt is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies:\n\nBuilt-in support for parsing HTML, XML, CSV, and Javascript\nA media pipeline for scraping items with images (or any other media) and download the image files as well\nSupport for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines\nWide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc\nInteractive scraping shell console, very useful for developing and debugging\nWeb management console for monitoring and controlling your bot\nTelnet console for low-level access to the Scrapy process\n\nExample code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:\nclass Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = \'mininova.org\'\n    start_urls = [\'http://www.mininova.org/today\']\n    rules = [Rule(RegexLinkExtractor(allow=[\'/tor/\\d+\']), \'parse_torrent\')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(""//h1/text()"").extract()\n        torrent.description = x.x(""//div[@id=\'description\']"").extract()\n        torrent.size = x.x(""//div[@id=\'info-left\']/p[2]/text()[2]"").extract()\n        return [torrent]\n\n', '\nCheck the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module.\nAnd here you can find code samples to build a simple web-crawler.\n', ""\nI've used Ruya and found it pretty good.\n"", '\nI hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there.\n#!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = """"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return \'\'\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = \'http://\'+site+\'\'+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print "" Error retrieving: ""+page\n        sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n    return """" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = \'http://\'+site_url \n        params = {\'name\': \'customer_admin\', \'pass\': \'customer_admin123\', \'opt\': \'Log in\', \'form_build_id\': \'form-3560fb42948a06b01d063de48aa216ab\', \'form_id\':\'user_login_block\'}\n    user_agent = \'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\'\n    self.headers = { \'User-Agent\' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print ""Logged in""\n    return response.read() \n\n    except Exception, err:\n    print "" Error logging in""\n    sys.stderr.write(\'ERROR: %s\\n\' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == \'a\':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search(\'http\', newstr) == None:\n        if re.search(\'mailto\', newstr) == None:\n          if re.search(\'#\', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print ""  adding"", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print ""  ignoring"", newstr\n        else:\n          print ""  ignoring"", newstr\n      else:\n        print ""  ignoring"", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print ""usage is ./minispider.py site link""\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+""/node?destination=node""\n  print ""\\nLogging in"", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != \'\':\n\n    print ""\\nChecking link "", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print ""\\ndone\\n""\n\nif __name__ == ""__main__"":\n  main()\n\n', '\nTrust me nothing is better than curl.. . the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2\nCAUTION: Don\'t hit the same domain at such a high speed.. .\n#! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py <file with URLs to fetch> [<# of\n#          concurrent connections>]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == ""-"":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) >= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print ""Usage: %s <file with URLs to fetch> [<# of concurrent connections>]"" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == ""#"":\n        continue\n    filename = ""doc_%03d.dat"" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, ""no URLs given""\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 <= num_conn <= 10000, ""invalid number of concurrent connections""\nprint ""PycURL %s (compiled against 0x%x)"" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint ""----- Getting"", num_urls, ""URLs using"", num_conn, ""connections -----""\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed < num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, ""wb"")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Success:"", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print ""Failed: "", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n\n', ""\nAnother simple spider \nUses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it.\n"", '\npyspider.py\n']",,web-crawler
how to extract links and titles from a .html page?,"
for my website, i'd like to add a new functionality.
I would like user to be able to upload his bookmarks backup file (from any browser if possible) so I can upload it to their profile and they don't have to insert all of them manually...
the only part i'm missing to do this it's the part of extracting title and URL from the uploaded file.. can anyone give a clue where to start or where to read? 
used search option and (How to extract data from a raw HTML file?) this is the most related question for mine and it doesn't talk about it..
I really don't mind if its using jquery or php
Thank you very much.
",73k,"
            41
        ","['\nThank you everyone, I GOT IT! \nThe final code:\n$html = file_get_contents(\'bookmarks.html\');\n//Create a new DOM document\n$dom = new DOMDocument;\n\n//Parse the HTML. The @ is used to suppress any parsing errors\n//that will be thrown if the $html string isn\'t valid XHTML.\n@$dom->loadHTML($html);\n\n//Get all links. You could also use any other tag name here,\n//like \'img\' or \'table\', to extract other tags.\n$links = $dom->getElementsByTagName(\'a\');\n\n//Iterate over the extracted links and display their URLs\nforeach ($links as $link){\n    //Extract and show the ""href"" attribute.\n    echo $link->nodeValue;\n    echo $link->getAttribute(\'href\'), \'<br>\';\n}\n\nThis shows you the anchor text assigned and the href for all  links in a .html file.\nAgain, thanks a lot.\n', '\nThis is probably sufficient:\n$dom = new DOMDocument;\n$dom->loadHTML($html);\nforeach ($dom->getElementsByTagName(\'a\') as $node)\n{\n  echo $node->nodeValue.\': \'.$node->getAttribute(""href"").""\\n"";\n}\n\n', ""\nAssuming the stored links are in a html file the best solution is probably to use a html parser such as PHP Simple HTML DOM Parser (never tried it myself). (The other option is to search using basic string search or regexp, and you should probably never use regexp to parse html).\nAfter reading the html file using the parser use it's functions to find the a tags:\nfrom the tutorial:\n// Find all links\nforeach($html->find('a') as $element)\n       echo $element->href . '<br>'; \n\n"", '\n$html = file_get_contents(\'your file path\');\n\n$dom = new DOMDocument;\n\n@$dom->loadHTML($html);\n\n$styles = $dom->getElementsByTagName(\'link\');\n\n$links = $dom->getElementsByTagName(\'a\');\n\n$scripts = $dom->getElementsByTagName(\'script\');\n\nforeach($styles as $style)\n{\n\n    if($style->getAttribute(\'href\')!=""#"")\n\n    {\n        echo $style->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach ($links as $link){\n\n    if($link->getAttribute(\'href\')!=""#"")\n    {\n        echo $link->getAttribute(\'href\');\n        echo\'<br>\';\n    }\n}\n\nforeach($scripts as $script)\n{\n\n        echo $script->getAttribute(\'src\');\n        echo\'<br>\';\n\n}\n\n', ""\nI wanted to create a CSV of link paths and their text from html pages so I could rip menus etc from sites.\nIn this example you specify the domain you are interested in so you don't get off site links and then it produces a CSV per document\n/**\n * Extracts links to the given domain from the files and creates CSVs of the links\n */\n\n\n$LinkExtractor = new LinkExtractor('https://www.example.co.uk');\n\n$LinkExtractor->extract(__DIR__ . '/hamburger.htm');\n$LinkExtractor->extract(__DIR__ . '/navbar.htm');\n$LinkExtractor->extract(__DIR__ . '/footer.htm');\n\nclass LinkExtractor {\n    public $domain;\n\n    public function __construct($domain) {\n      $this->domain = $domain;\n    }\n\n    public function extract($file) {\n        $html = file_get_contents($file);\n        //Create a new DOM document\n        $dom = new DOMDocument;\n\n        //Parse the HTML. The @ is used to suppress any parsing errors\n        //that will be thrown if the $html string isn't valid XHTML.\n        @$dom->loadHTML($html);\n\n        //Get all links. You could also use any other tag name here,\n        //like 'img' or 'table', to extract other tags.\n        $links = $dom->getElementsByTagName('a');\n\n        $results = [];\n        //Iterate over the extracted links and display their URLs\n        foreach ($links as $link){\n            //Extract and sput the matching links in an array for the CSV\n            $href = $link->getAttribute('href');\n            $parts = parse_url($href);\n            if (!empty($parts['path']) && strpos($this->domain, $parts['host']) !== false) {\n                $results[$parts['path']] = [$parts['path'], $link->nodeValue];\n            }\n        }\n\n        asort($results);\n        // Make the CSV\n        $fp = fopen($file .'.csv', 'w');\n        foreach ($results as $fields) {\n            fputcsv($fp, $fields);\n        }\n        fclose($fp);\n    }\n}\n\n"", '\nHere is my work for one of my client and make it as a function to use everywhere.\nfunction getValidUrlsFrompage($source)\n  {\n    $links = [];\n    $content = file_get_contents($source);\n    $content = strip_tags($content, ""<a>"");\n    $subString = preg_split(""/<\\/a>/"", $content);\n    foreach ($subString as $val) {\n      if (strpos($val, ""<a href="") !== FALSE) {\n        $val = preg_replace(""/.*<a\\s+href=\\""/sm"", """", $val);\n        $val = preg_replace(""/\\"".*/"", """", $val);\n        $val = trim($val);\n      }\n      if (strlen($val) > 0 && filter_var($val, FILTER_VALIDATE_URL)) {\n        if (!in_array($val, $links)) {\n          $links[] = $val;\n        }\n      }\n    }\n    return $links;\n  }\n\nAnd use it like\n$links = getValidUrlsFrompage(""https://www.w3resource.com/"");\n\nAnd The expected output is get 99 URLs in an array,\nArray ( [0] => https://www.w3resource.com [1] => https://www.w3resource.com/html/HTML-tutorials.php [2] => https://www.w3resource.com/css/CSS-tutorials.php [3] => https://www.w3resource.com/javascript/javascript.php [4] => https://www.w3resource.com/html5/introduction.php [5] => https://www.w3resource.com/schema.org/introduction.php [6] => https://www.w3resource.com/phpjs/use-php-functions-in-javascript.php [7] => https://www.w3resource.com/twitter-bootstrap/tutorial.php [8] => https://www.w3resource.com/responsive-web-design/overview.php [9] => https://www.w3resource.com/zurb-foundation3/introduction.php [10] => https://www.w3resource.com/pure/ [11] => https://www.w3resource.com/html5-canvas/ [12] => https://www.w3resource.com/course/javascript-course.html [13] => https://www.w3resource.com/icon/ [14] => https://www.w3resource.com/linux-system-administration/installation.php [15] => https://www.w3resource.com/linux-system-administration/linux-commands-introduction.php [16] => https://www.w3resource.com/php/php-home.php [17] => https://www.w3resource.com/python/python-tutorial.php [18] => https://www.w3resource.com/java-tutorial/ [19] => https://www.w3resource.com/node.js/node.js-tutorials.php [20] => https://www.w3resource.com/ruby/ [21] => https://www.w3resource.com/c-programming/programming-in-c.php [22] => https://www.w3resource.com/sql/tutorials.php [23] => https://www.w3resource.com/mysql/mysql-tutorials.php [24] => https://w3resource.com/PostgreSQL/tutorial.php [25] => https://www.w3resource.com/sqlite/ [26] => https://www.w3resource.com/mongodb/nosql.php [27] => https://www.w3resource.com/API/google-plus/tutorial.php [28] => https://www.w3resource.com/API/youtube/tutorial.php [29] => https://www.w3resource.com/API/google-maps/index.php [30] => https://www.w3resource.com/API/flickr/tutorial.php [31] => https://www.w3resource.com/API/last.fm/tutorial.php [32] => https://www.w3resource.com/API/twitter-rest-api/ [33] => https://www.w3resource.com/xml/xml.php [34] => https://www.w3resource.com/JSON/introduction.php [35] => https://www.w3resource.com/ajax/introduction.php [36] => https://www.w3resource.com/html-css-exercise/index.php [37] => https://www.w3resource.com/javascript-exercises/ [38] => https://www.w3resource.com/jquery-exercises/ [39] => https://www.w3resource.com/jquery-ui-exercises/ [40] => https://www.w3resource.com/coffeescript-exercises/ [41] => https://www.w3resource.com/php-exercises/ [42] => https://www.w3resource.com/python-exercises/ [43] => https://www.w3resource.com/c-programming-exercises/ [44] => https://www.w3resource.com/csharp-exercises/ [45] => https://www.w3resource.com/java-exercises/ [46] => https://www.w3resource.com/sql-exercises/ [47] => https://www.w3resource.com/oracle-exercises/ [48] => https://www.w3resource.com/mysql-exercises/ [49] => https://www.w3resource.com/sqlite-exercises/ [50] => https://www.w3resource.com/postgresql-exercises/ [51] => https://www.w3resource.com/mongodb-exercises/ [52] => https://www.w3resource.com/twitter-bootstrap/examples.php [53] => https://www.w3resource.com/euler-project/ [54] => https://w3resource.com/w3skills/html5-quiz/ [55] => https://w3resource.com/w3skills/php-fundamentals/ [56] => https://w3resource.com/w3skills/sql-beginner/ [57] => https://w3resource.com/w3skills/python-beginner-quiz/ [58] => https://w3resource.com/w3skills/mysql-basic-quiz/ [59] => https://w3resource.com/w3skills/javascript-basic-skill-test/ [60] => https://w3resource.com/w3skills/javascript-advanced-quiz/ [61] => https://w3resource.com/w3skills/javascript-quiz-part-iii/ [62] => https://w3resource.com/w3skills/mongodb-basic-quiz/ [63] => https://www.w3resource.com/form-template/ [64] => https://www.w3resource.com/slides/ [65] => https://www.w3resource.com/convert/number/binary-to-decimal.php [66] => https://www.w3resource.com/excel/ [67] => https://www.w3resource.com/video-tutorial/php/some-basics-of-php.php [68] => https://www.w3resource.com/video-tutorial/javascript/list-of-tutorial.php [69] => https://www.w3resource.com/web-development-tools/firebug-tutorials.php [70] => https://www.w3resource.com/web-development-tools/useful-web-development-tools.php [71] => https://www.facebook.com/w3resource [72] => https://twitter.com/w3resource [73] => https://plus.google.com/+W3resource [74] => https://in.linkedin.com/in/w3resource [75] => https://feeds.feedburner.com/W3resource [76] => https://www.w3resource.com/ruby-exercises/ [77] => https://www.w3resource.com/graphics/matplotlib/ [78] => https://www.w3resource.com/python-exercises/numpy/index.php [79] => https://www.w3resource.com/python-exercises/pandas/index.php [80] => https://w3resource.com/plsql-exercises/ [81] => https://w3resource.com/swift-programming-exercises/ [82] => https://www.w3resource.com/angular/getting-started-with-angular.php [83] => https://www.w3resource.com/react/react-js-overview.php [84] => https://www.w3resource.com/vue/installation.php [85] => https://www.w3resource.com/jest/jest-getting-started.php [86] => https://www.w3resource.com/numpy/ [87] => https://www.w3resource.com/php/composer/a-gentle-introduction-to-composer.php [88] => https://www.w3resource.com/php/PHPUnit/a-gentle-introduction-to-unit-test-and-testing.php [89] => https://www.w3resource.com/laravel/laravel-tutorial.php [90] => https://www.w3resource.com/oracle/index.php [91] => https://www.w3resource.com/redis/index.php [92] => https://www.w3resource.com/cpp-exercises/ [93] => https://www.w3resource.com/r-programming-exercises/ [94] => https://w3resource.com/w3skills/ [95] => https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US [96] => https://www.w3resource.com/privacy.php [97] => https://www.w3resource.com/about.php [98] => https://www.w3resource.com/contact.php [99] => https://www.w3resource.com/feedback.php [100] => https://www.w3resource.com/advertise.php )\n\nHope, this will help someone. And here is a gist - \nhttps://gist.github.com/ManiruzzamanAkash/74cffb9ffdfc92f57bd9cf214cf13491\n']",https://stackoverflow.com/questions/4423272/how-to-extract-links-and-titles-from-a-html-page,web-crawler
"Pulling data from a webpage, parsing it for specific pieces, and displaying it","
I've been using this site for a long time to find answers to my questions, but I wasn't able to find the answer on this one.
I am working with a small group on a class project. We're to build a small ""game trading"" website that allows people to register, put in a game they have they want to trade, and accept trades from others or request a trade.
We have the site functioning long ahead of schedule so we're trying to add more to the site. One thing I want to do myself is to link the games that are put in to Metacritic.
Here's what I need to do. I need to (using asp and c# in visual studio 2012) get the correct game page on metacritic, pull its data, parse it for specific parts, and then display the data on our page. 
Essentially when you choose a game you want to trade for we want a small div to display with the game's information and rating. I'm wanting to do it this way to learn more and get something out of this project I didn't have to start with. 
I was wondering if anyone could tell me where to start. I don't know how to pull data from a page. I'm still trying to figure out if I need to try and write something to automatically search for the game's title and find the page that way or if I can find some way to go straight to the game's page. And once I've gotten the data, I don't know how to pull the specific information I need from it.
One of the things that doesn't make this easy is that I'm learning c++ along with c# and asp so I keep getting my wires crossed. If someone could point me in the right direction it would be a big help. Thanks
",102k,"
            19
        ","['\nThis small example uses HtmlAgilityPack, and using XPath selectors to get to the desired elements.\nprotected void Page_Load(object sender, EventArgs e)\n{\n    string url = ""http://www.metacritic.com/game/pc/halo-spartan-assault"";\n    var web = new HtmlAgilityPack.HtmlWeb();\n    HtmlDocument doc = web.Load(url);\n\n    string metascore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[1]/div/div/div[2]/a/span[1]"")[0].InnerText;\n    string userscore = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[1]/div[2]/div[1]/div/div[2]/a/span[1]"")[0].InnerText;\n    string summary = doc.DocumentNode.SelectNodes(""//*[@id=\\""main\\""]/div[3]/div/div[2]/div[2]/div[1]/ul/li/span[2]/span/span[1]"")[0].InnerText;\n}\n\nAn easy way to obtain the XPath for a given element is by using your web browser (I use Chrome) Developer Tools:\n\nOpen the Developer Tools (F12 or Ctrl + Shift + C on Windows or Command + Shift + C for Mac).\nSelect the element in the page that you want the XPath for.\nRight click the element in the ""Elements"" tab.\nClick on ""Copy as XPath"".\n\nYou can paste it exactly like that in c# (as shown in my code), but make sure to escape the quotes.\nYou have to make sure you use some error handling techniques because Web scraping can cause errors if they change the HTML formatting of the page.\nEdit\nPer @knocte\'s suggestion,  here is the link to the Nuget package for HTMLAgilityPack:\nhttps://www.nuget.org/packages/HtmlAgilityPack/\n', '\nI looked and Metacritic.com doesn\'t have an API.\nYou can use an HttpWebRequest to get the contents of a website as a string. \nusing System.Net;\nusing System.IO;\nusing System.Windows.Forms;\n\nstring result = null;\nstring url = ""http://www.stackoverflow.com"";\nWebResponse response = null;\nStreamReader reader = null;\n\ntry\n{\n    HttpWebRequest request = (HttpWebRequest)WebRequest.Create(url);\n    request.Method = ""GET"";\n    response = request.GetResponse();\n    reader = new StreamReader(response.GetResponseStream(), Encoding.UTF8);\n    result = reader.ReadToEnd();\n}\ncatch (Exception ex)\n{\n    // handle error\n    MessageBox.Show(ex.Message);\n}\nfinally\n{\n    if (reader != null)\n        reader.Close();\n    if (response != null)\n        response.Close();\n}\n\nThen you can parse the string for the data that you want by taking advantage of Metacritic\'s use of meta tags. Here\'s the information they have available in meta tags:\n\nog:title\nog:type\nog:url\nog:image\nog:site_name\nog:description\n\nThe format of each tag is: meta name=""og:title"" content=""In a World...""\n', '\nI recommend Dcsoup.  There\'s a nuget package for it and it uses CSS selectors so it is familiar if you use jquery.  I\'ve tried others but it is the best and easiest to use that I\'ve found.  There\'s not much documentation, but it\'s open source and a port of the java jsoup library that has good documentation. (Documentation for the .NET API here.) I absolutely love it.\nvar timeoutInMilliseconds = 5000;\nvar uri = new Uri(""http://www.metacritic.com/game/pc/fallout-4"");\nvar doc = Supremes.Dcsoup.Parse(uri, timeoutInMilliseconds);\n\n// <span itemprop=""ratingValue"">86</span>\nvar ratingSpan = doc.Select(""span[itemprop=ratingValue]"");\nint ratingValue = int.Parse(ratingSpan.Text);\n\n// selectors match both critic and user scores\nvar scoreDiv = doc.Select(""div.score_summary"");\nvar scoreAnchor = scoreDiv.Select(""a.metascore_anchor"");\nint criticRating = int.Parse(scoreAnchor[0].Text);\nfloat userRating = float.Parse(scoreAnchor[1].Text);\n\n', '\nI\'d recomend you WebsiteParser - it\'s based on HtmlAgilityPack (mentioned by Hanlet Escaño) but it makes web scraping easier with attributes and css selectors:\nclass PersonModel\n{\n    [Selector(""#BirdthDate"")]\n    [Converter(typeof(DateTimeConverter))]\n    public DateTime BirdthDate { get; set; }\n}\n\n// ...\n\nPersonModel person = WebContentParser.Parse<PersonModel>(html);\n\nNuget link\n']",https://stackoverflow.com/questions/18065526/pulling-data-from-a-webpage-parsing-it-for-specific-pieces-and-displaying-it,web-crawler
Parse HTML content in VBA,"
I have a question relating to HTML parsing. I have a website with some products and I would like to catch text within page into my current spreadsheet. This spreadsheet is quite big but contains ItemNbr in 3rd column, I expect the text in the 14th column and one row corresponds to one product (item).
My idea is to fetch the 'Material' on the webpage which is inside the Innertext after  tag. The id number changes from one page to page (sometimes ).
Here is the structure of the website:
<div style=""position:relative;"">
    <div></div>
    <table id=""list-table"" width=""100%"" tabindex=""1"" cellspacing=""0"" cellpadding=""0"" border=""0"" role=""grid"" aria-multiselectable=""false"" aria-labelledby=""gbox_list-table"" class=""ui-jqgrid-btable"" style=""width: 930px;"">
        <tbody>
            <tr class=""jqgfirstrow"" role=""row"" style=""height:auto"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""1"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""2"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""3"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""4"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""5"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""6"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td ...</td>
                <td ...</td>
            </tr>
            <tr role=""row"" id=""7"" tabindex=""-1"" class=""ui-widget-content jqgrow ui-row-ltr"">
                <td role=""gridcell"" style=""padding-left:10px"" title=""Material"" aria-describedby=""list-table_"">Material</td>
                <td role=""gridcell"" style="""" title=""600D polyester."" aria-describedby=""list-table_"">600D polyester.</td>
            </tr>           
            <tr ...>
            </tr>
        </tbody>
    </table> </div>

I would like to get ""600D Polyester"" as a result.
My (not working) code snippet is as is:
Sub ParseMaterial()

    Dim Cell As Integer
    Dim ItemNbr As String

    Dim AElement As Object
    Dim AElements As IHTMLElementCollection
Dim IE As MSXML2.XMLHTTP60
Set IE = New MSXML2.XMLHTTP60

Dim HTMLDoc As MSHTML.HTMLDocument
Dim HTMLBody As MSHTML.HTMLBody

Set HTMLDoc = New MSHTML.HTMLDocument
Set HTMLBody = HTMLDoc.body

For Cell = 1 To 5                            'I iterate through the file row by row

    ItemNbr = Cells(Cell, 3).Value           'ItemNbr isin the 3rd Column of my spreadsheet

    IE.Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False
    IE.send

    While IE.ReadyState <> 4
        DoEvents
    Wend

    HTMLBody.innerHTML = IE.responseText

    Set AElements = HTMLDoc.getElementById(""list-table"").getElementsByTagName(""tr"")
    For Each AElement In AElements
        If AElement.Title = ""Material"" Then
            Cells(Cell, 14) = AElement.nextNode.value     'I write the material in the 14th column
        End If
    Next AElement

        Application.Wait (Now + TimeValue(""0:00:2""))

Next Cell

Thanks for your help !
",83k,"
            15
        ","['\nJust a couple things that hopefully will get you in the right direction:\n\nclean up a bit: remove the readystate property testing loop. The value returned by the readystate property will never change in this context - code will pause after the send instruction, to resume only once the server response is received, or has failed to do so. The readystate property will be set accordingly, and the code will resume execution. You should still test for the ready state, but the loop is just unnecessary\n\ntarget the right HTML elements: you are searching through the tr elements - while the logic of how you use these elements in your code actually looks to point to td elements\n\nmake sure the properties are actually available for the objects you are using them on: to help you with this, try and declare all your variable as specific objects instead of the generic Object. This will activate intellisense. If you have a difficult time finding the actual name of your object as defined in the relevant library in a first place, declare it as the generic Object, run your code, and then inspect the type of the object - by printing typename(your_object) to the debug window for instance. This should put you on your way\n\n\nI have also included some code below that may help. If you still can\'t get this to work and you can share your urls - plz do that.\nSub getInfoWeb()\n\n    Dim cell As Integer\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSHTML.HTMLDocument\n    Dim table As MSHTML.HTMLTable\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    \n    Set xhr = New MSXML2.XMLHTTP60\n   \n    For cell = 1 To 5\n        \n        ItemNbr = Cells(cell, 3).Value\n        \n        With xhr\n        \n            .Open ""GET"", ""http://www.example.com/?item="" & ItemNbr, False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSHTML.HTMLDocument\n                doc.body.innerHTML = .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set table = doc.getElementById(""list-table"")\n        Set tableCells = table.getElementsByTagName(""td"")\n        \n        For Each tableCell In tableCells\n            If tableCell.getAttribute(""title"") = ""Material"" Then\n                Cells(cell, 14).Value = tableCell.NextSibling.innerHTML\n            End If\n        Next tableCell\n    \n    Next cell\n    \nEnd Sub\n\nEDIT: as a follow-up to the further information you provided in the comment below - and the additional comments I have added\n\'Determine your product number\n    \'Open an xhr for your source url, and retrieve the product number from there - search for the tag which\n    \'text include the ""productnummer:"" substring, and extract the product number from the outerstring\n    \'OR\n    \'if the product number consistently consists of the fctkeywords you are entering in your source url\n    \'with two ""0"" appended - just build the product number like that\n\'Open an new xhr for this url ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc="" & product_number & ""&_search=false&rows=-1&page=1&sidx=&sord=asc""\n\'Load the response in an XML document, and retrieve the material information\n\nSub getInfoWeb()\n\n    Dim xhr As MSXML2.XMLHTTP60\n    Dim doc As MSXML2.DOMDocument60\n    Dim xmlCell As MSXML2.IXMLDOMElement\n    Dim xmlCells As MSXML2.IXMLDOMNodeList\n    Dim materialValueElement As MSXML2.IXMLDOMElement\n    \n    Set xhr = New MSXML2.XMLHTTP60\n        \n        With xhr\n            \n            .Open ""GET"", ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2productspec-ajax.p?itemc=10031700&_search=false&rows=-1&page=1&sidx=&sord=asc"", False\n            .send\n            \n            If .readyState = 4 And .Status = 200 Then\n                Set doc = New MSXML2.DOMDocument60\n                doc.LoadXML .responseText\n            Else\n                MsgBox ""Error"" & vbNewLine & ""Ready state: "" & .readyState & _\n                vbNewLine & ""HTTP request status: "" & .Status\n            End If\n            \n        End With\n        \n        Set xmlCells = doc.getElementsByTagName(""cell"")\n\n        For Each xmlCell In xmlCells\n            If xmlCell.Text = ""Materiaal"" Then\n                Set materialValueElement = xmlCell.NextSibling\n            End If\n        Next\n        \n        MsgBox materialValueElement.Text\n    \nEnd Sub\n\nEDIT2: an alternative automating IE\nSub searchWebViaIE()\n    Dim ie As SHDocVw.InternetExplorer\n    Dim doc As MSHTML.HTMLDocument\n    Dim anchors As MSHTML.IHTMLElementCollection\n    Dim anchor As MSHTML.HTMLAnchorElement\n    Dim prodSpec As MSHTML.HTMLAnchorElement\n    Dim tableCells As MSHTML.IHTMLElementCollection\n    Dim materialValueElement As MSHTML.HTMLTableCell\n    Dim tableCell As MSHTML.HTMLTableCell\n    \n    Set ie = New SHDocVw.InternetExplorer\n    \n    With ie\n        .navigate ""http://www.pfconcept.com/cgi-bin/wspd_pcdb_cgi.sh/y/y2facetmain.p?fctkeywords=100317&world=general#tabs-4""\n        .Visible = True\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n        \n        Set doc = .document\n        \n        Set anchors = doc.getElementsByTagName(""a"")\n        \n        For Each anchor In anchors\n            If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n                anchor.Click\n                Exit For\n            End If\n        Next anchor\n        \n        Do While .readyState <> READYSTATE_COMPLETE Or .Busy = True\n            DoEvents\n        Loop\n    \n    End With\n        \n    For Each anchor In anchors\n        If InStr(anchor.innerHTML, ""Product Specificatie"") <> 0 Then\n            Set prodSpec = anchor\n        End If\n    Next anchor\n    \n    Set tableCells = doc.getElementById(""list-table"").getElementsByTagName(""td"")\n    \n    If Not tableCells Is Nothing Then\n        For Each tableCell In tableCells\n            If tableCell.innerHTML = ""Materiaal"" Then\n                Set materialValueElement = tableCell.NextSibling\n            End If\n        Next tableCell\n    End If\n    \n    MsgBox materialValueElement.innerHTML\n    \nEnd Sub\n\n', '\nNot related to tables or Excel ( I use MS-Access 2013) but directly related to the topic title. My solution is \nPrivate Sub Sample(urlSource)\nDim httpRequest As New WinHttpRequest\nDim doc As MSHTML.HTMLDocument\nDim tags As MSHTML.IHTMLElementCollection\nDim tag As MSHTML.HTMLHtmlElement\nhttpRequest.Option(WinHttpRequestOption_UserAgentString) = ""Mozilla/4.0 (compatible;MSIE 7.0; Windows NT 6.0)""\nhttpRequest.Open ""GET"", urlSource\nhttpRequest.send \' fetching webpage\nSet doc = New MSHTML.HTMLDocument\ndoc.body.innerHTML = httpRequest.responseText\nSet tags = doc.getElementsByTagName(""a"")\ni = 1\nFor Each tag In tags\n  Debug.Print i\n  Debug.Print tag.href\n  Debug.Print tag.innerText\n  \'Debug.Print tag.Attributes(""any other attributes you need"")() \' may return an object\n  i = i + 1\n  If i Mod 50 = 0 Then Stop\n  \' or code to store results in a table\nNext\nEnd Sub\n\n']",https://stackoverflow.com/questions/25488687/parse-html-content-in-vba,web-crawler
Do Google's crawlers interpret Javascript? What if I load a page through AJAX? [closed],"






Closed. This question is off-topic. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it's on-topic for Stack Overflow.


Closed 10 years ago.







                        Improve this question
                    



When a user enters my page, I have to make another AJAX call...to load data inside a div.
That's just how my application works.
The problem is...when I view the source of this code, it does not contain the source of that AJAX.  Of course, when I do wget URL ...it also does not show the AJAX HTML. Makes sense.
But what about Google? Will Google be able to crawl the content, as if it's a browser?  How do I allow Google to crawl my page just like a user would see it?
",13k,"
            15
        ","['\nDespite the answers above, apparently it does interpret JavaScript, to an extent, according to Matt Cutts:\n\n""For a while, we were scanning within JavaScript, and we were looking for links. Google has gotten smarter about JavaScript and can execute some JavaScript. I wouldn\'t say that we execute all JavaScript, so there are some conditions in which we don\'t execute JavaScript. Certainly there are some common, well-known JavaScript things like Google Analytics, which you wouldn\'t even want to execute because you wouldn\'t want to try to generate phantom visits from Googlebot into your Google Analytics"".\n\n(Why answer an answered question? Mostly because I just saw it because of a duplicate question posted today, and didn\'t see this info here.)\n', '\nActually... Google does have a solution for crawling Ajax applications...\nhttp://code.google.com/web/ajaxcrawling/docs/getting-started.html\n', '\nUpdated: From the answer to this question about ""Ajax generated content, crawling and black listing"" I found this document about the way Google crawls AJAX requests which is part of a collection of documents about Making AJAX Applications Crawlable. \nIn short, it means you need to use <a href=""#!data"">...</a> rather than <a href=""#data"">...</a> and then supply a real server-side answer to the URL path/to/path?_escaped_fragment_=data.\nAlso consider a <link/> tag to supply crawlers with a hint to SEO-friendly content. <link rel=""canonical""/>, which this article explains a bit, is a good candidate\nNote: I took the answer from: https://stackoverflow.com/questions/10006825/search-engine-misunderstanting/10006925#comment12792862_10006925  because it seems I can\'t delete mine here.\n', '\nWhat I do in this situation is always initially populate the page with content based upon the default parameters of whatever the Ajax call is doing. Then I only use the ajax javascript to do updates to the page.\n', '\nAs other answers say, Google\'s crawler (and I believe those of other search engines) does not interpret Javascript -- and you should not try to differentiate by user-agent or the like (at the risk of having your site downgraded or blocked for presenting different contents to users vs robots).  Rather, do offer some (perhaps minimal) level of content to visitors that have Javascript blocked for whatever reason (including the cases where the reason is ""being robots"";-) -- after all, that\'s the very reason the noscript tag exists... to make it very, very easy to offer such ""minimal level of content"" (or, more than minimal, if you so choose;-) to non-users of Javascript!\n', '\nWeb crawlers have a difficult time with ajax and javascript that dynamically loads content.  This site has some ideas that show you how to help google index your site http://www.softwaredeveloper.com/features/google-ajax-play-nice-061907/\n', '\nIf you make your pages such that they will work with OR without javascript (i.e. fall back to using frames or standard GET / POST requests to the server if javascript fails, either automatically, or via a ""display as plain html"" link ), it will be much easier for search engines to crawl the page.\nIt makes sense for them not to crawl ""dynamic"" content - because it is just that...dynamic.\nMy understanding is that in most situations, Google does not crawl the client-side-dynamic-content.\n', '\nNow It looks ike Google bot is not limited to simple lynx like browser.\nGoogle bot tries to grab the Humanly visible and Humanly contrasting text to give importance in different sectors of page. So it renders the page with a Layout Engine just like another browser like FF or Chrome have.\nIt might even have v8 Javascript Engine support. and the bot might load the page and wait till dom is ready and may even wait for few seconds for the page to come to a stable view. and then crop the contrasting text.\n']",https://stackoverflow.com/questions/2061844/do-googles-crawlers-interpret-javascript-what-if-i-load-a-page-through-ajax,web-crawler
Fetch contents(loaded through AJAX call) of a web page,"
I am a beginner to crawling. I have a requirement to fetch the posts and comments from a link. I want to automate this process. I considered using webcrawler and jsoup for this but was told that webcrawlers are mostly used for websites with greater depth. 
Sample for a page: Jive community website
For this page, when I view the source of the page, I can see only the post and not the comments. Think this is because comments are fetched through an AJAX call to the server. 
Hence, when I use jsoup, it doesn't fetch the comments. 
So how can I automate the process of fetching posts and comments?
",10k,"
            6
        ","[""\nJsoup is a html parser only. Unfortunately it's not possible to parse any javascript / ajax content, since jsoup can't execute those.\nThe solution: using a library which can handle Scripts.\nHere are some examples i know:\n\nHtmlUnit\nJava Script Engine\nApache Commons BSF\nRhino\n\nIf such a library doesn't support parsing or selectors, you can at least use them to get Html out of the scripts (which then can be parsed by jsoup).\n"", '\nJsoup does not handle with Javascript and Ajax, so you need to use Htmlunit or selenium.\nAfter loading page using Htmlunit or any you can use jsoup for rest of task.\n']",https://stackoverflow.com/questions/20633294/fetch-contentsloaded-through-ajax-call-of-a-web-page,web-crawler
How can I handle Javascript in a Perl web crawler?,"
I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed. 
Usually I use LWP / Mechanize etc to crawl sites, but neither support JavaScript.
any idea?
",5k,"
            19
        ","['\nAnother option might be Selenium with WWW::Selenium module\n', ""\nThe WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however.\n"", '\nWWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.\n', '\nI would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.\n']",https://stackoverflow.com/questions/3769015/how-can-i-handle-javascript-in-a-perl-web-crawler,web-crawler
Detecting 'stealth' web-crawlers,"
What options are there to detect web-crawlers that do not want to be detected?
(I know that listing detection techniques will allow the smart stealth-crawler programmer to make a better spider, but I do not think that we will ever be able to block smart stealth-crawlers anyway, only the ones that make mistakes.)
I'm not talking about the nice crawlers such as Googlebot and Yahoo! Slurp.
I consider a bot nice if it:

identifies itself as a bot in the user agent string
reads robots.txt (and obeys it)

I'm talking about the bad crawlers, hiding behind common user agents, using my bandwidth and never giving me anything in return.
There are some trapdoors that can be constructed updated list (thanks Chris, gs):

Adding a directory only listed (marked as disallow) in the robots.txt,
Adding invisible links (possibly marked as rel=""nofollow""?),

style=""display: none;"" on link or parent container
placed underneath another element with higher z-index


detect who doesn't understand CaPiTaLiSaTioN,
detect who tries to post replies but always fail the Captcha.
detect GET requests to POST-only resources
detect interval between requests
detect order of pages requested
detect who (consistently) requests HTTPS resources over HTTP
detect who does not request image file (this in combination with a list of user-agents of known image capable browsers works surprisingly nice)

Some traps would be triggered by both 'good' and 'bad' bots.
you could combine those with a whitelist:

It trigger a trap
It request robots.txt?
It doest not trigger another trap because it obeyed robots.txt

One other important thing here is:
Please consider blind people using a screen readers: give people a way to contact you, or solve a (non-image) Captcha to continue browsing.
What methods are there to automatically detect the web crawlers trying to mask themselves as normal human visitors.
The question is not: How do I catch every crawler. The question is: How can I maximize the chance of detecting a crawler.
Some spiders are really good, and actually parse and understand HTML, xhtml, CSS JavaScript, VBScript etc...
I have no illusions: I won't be able to beat them.
You would however be surprised how stupid some crawlers are. With the best example  of stupidity (in my opinion) being: cast all URLs to lower case before requesting them.
And then there is a whole bunch of crawlers that are just 'not good enough' to avoid the various trapdoors.
",27k,"
            112
        ","[""\nA while back, I worked with a smallish hosting company to help them implement a solution to this.  The system I developed examined web server logs for excessive activity from any given IP address and issued firewall rules to block offenders.  It included whitelists of IP addresses/ranges based on http://www.iplists.com/, which were then updated automatically as needed by checking claimed user-agent strings and, if the client claimed to be a legitimate spider but not on the whitelist, it performed DNS/reverse-DNS lookups to verify that the source IP address corresponds to the claimed owner of the bot.  As a failsafe, these actions were reported to the admin by email, along with links to black/whitelist the address in case of an incorrect assessment.\nI haven't talked to that client in 6 months or so, but, last I heard, the system was performing quite effectively.\nSide point:  If you're thinking about doing a similar detection system based on hit-rate-limiting, be sure to use at least one-minute (and preferably at least five-minute) totals.  I see a lot of people talking about these kinds of schemes who want to block anyone who tops 5-10 hits in a second, which may generate false positives on image-heavy pages (unless images are excluded from the tally) and will generate false positives when someone like me finds an interesting site that he wants to read all of, so he opens up all the links in tabs to load in the background while he reads the first one.\n"", '\nSee Project Honeypot - they\'re setting up bot traps on large scale (and have DNSRBL with their IPs).\nUse tricky URLs and HTML:\n<a href=""//example.com/""> = http://example.com/ on http pages.\n<a href=""page&amp;&#x23;hash""> = page& + #hash\n\nIn HTML you can use plenty of tricks with comments, CDATA elements, entities, etc:\n<a href=""foo<!--bar-->""> (comment should not be removed)\n<script>var haha = \'<a href=""bot"">\'</script>\n<script>// <!-- </script> <!--><a href=""bot""> <!-->\n\n', '\nAn easy solution is to create a link and make it invisible\n<a href=""iamabot.script"" style=""display:none;"">Don\'t click me!</a>\n\nOf course you should expect that some people who look at the source code follow that link just to see where it leads. But you could present those users with a captcha...\nValid crawlers would, of course, also follow the link. But you should not implement a rel=nofollow, but look for the sign of a valid crawler. (like the user agent)\n', ""\nOne thing you didn't list, that are used commonly to detect bad crawlers.\nHit speed, good web crawlers will break their hits up so they don't deluge a site with requests.  Bad ones will do one of three things:\n\nhit sequential links one after the other\nhit sequential links in some paralell sequence (2 or more at a time.)\nhit sequential links at a fixed interval\n\nAlso, some offline browsing programs will slurp up a number of pages, I'm not sure what kind of threshold you'd want to use, to start blocking by IP address.\nThis method will also catch mirroring programs like fmirror or wget.\nIf the bot randomizes the time interval, you could check to see if the links are traversed in a sequential or depth-first manner, or you can see if the bot is traversing a huge amount of text (as in words to read) in a too-short period of time.  Some sites limit the number of requests per hour, also.\nActually, I heard an idea somewhere, I don't remember where, that if a user gets too much data, in terms of kilobytes, they can be presented with a captcha asking them to prove they aren't a bot.  I've never seen that implemented though.\n\nUpdate on Hiding Links\n\nAs far as hiding links goes, you can put a div under another, with CSS (placing it first in the draw order) and possibly setting the z-order.  A bot could not ignore that, without parsing all your javascript to see if it is a menu.  To some extent, links inside invisible DIV elements also can't be ignored without the bot parsing all the javascript.\nTaking that idea to completion, uncalled javascript which could potentially show the hidden elements would possilby fool a subset of javascript parsing bots.  And, it is not a lot of work to implement.\n"", ""\nIt's not actually that easy to keep up with the good user agent strings. Browser versions come and go. Making a statistic about user agent strings by different behaviors can reveal interesting things.\nI don't know how far this could be automated, but at least it is one differentiating thing.\n"", '\nOne simple bot detection method I\'ve heard of for forms is the hidden input technique. If you are trying to secure a form put a input in the form with an id that looks completely legit. Then use css in an external file to hide it. Or if you are really paranoid, setup something like jquery to hide the input box on page load. If you do this right I imagine it would be very hard for a bot to figure out. You know those bots have it in there nature to fill out everything on a page especially if you give your hidden input an id of something like id=""fname"", etc.\n', '\nUntested, but here is a nice list of user-agents you could make a regular expression out of.  Could get you most of the way there:\nADSARobot|ah-ha|almaden|aktuelles|Anarchie|amzn_assoc|ASPSeek|ASSORT|ATHENS|Atomz|attach|attache|autoemailspider|BackWeb|Bandit|BatchFTP|bdfetch|big.brother|BlackWidow|bmclient|Boston\\ Project|BravoBrian\\ SpiderEngine\\ MarcoPolo|Bot\\ mailto:craftbot@yahoo.com|Buddy|Bullseye|bumblebee|capture|CherryPicker|ChinaClaw|CICC|clipping|Collector|Copier|Crescent|Crescent\\ Internet\\ ToolPak|Custo|cyberalert|DA$|Deweb|diagem|Digger|Digimarc|DIIbot|DISCo|DISCo\\ Pump|DISCoFinder|Download\\ Demon|Download\\ Wonder|Downloader|Drip|DSurf15a|DTS.Agent|EasyDL|eCatch|ecollector|efp@gmx\\.net|Email\\ Extractor|EirGrabber|email|EmailCollector|EmailSiphon|EmailWolf|Express\\ WebPictures|ExtractorPro|EyeNetIE|FavOrg|fastlwspider|Favorites\\ Sweeper|Fetch|FEZhead|FileHound|FlashGet\\ WebWasher|FlickBot|fluffy|FrontPage|GalaxyBot|Generic|Getleft|GetRight|GetSmart|GetWeb!|GetWebPage|gigabaz|Girafabot|Go\\!Zilla|Go!Zilla|Go-Ahead-Got-It|GornKer|gotit|Grabber|GrabNet|Grafula|Green\\ Research|grub-client|Harvest|hhjhj@yahoo|hloader|HMView|HomePageSearch|http\\ generic|HTTrack|httpdown|httrack|ia_archiver|IBM_Planetwide|Image\\ Stripper|Image\\ Sucker|imagefetch|IncyWincy|Indy*Library|Indy\\ Library|informant|Ingelin|InterGET|Internet\\ Ninja|InternetLinkagent|Internet\\ Ninja|InternetSeer\\.com|Iria|Irvine|JBH*agent|JetCar|JOC|JOC\\ Web\\ Spider|JustView|KWebGet|Lachesis|larbin|LeechFTP|LexiBot|lftp|libwww|likse|Link|Link*Sleuth|LINKS\\ ARoMATIZED|LinkWalker|LWP|lwp-trivial|Mag-Net|Magnet|Mac\\ Finder|Mag-Net|Mass\\ Downloader|MCspider|Memo|Microsoft.URL|MIDown\\ tool|Mirror|Missigua\\ Locator|Mister\\ PiX|MMMtoCrawl\\/UrlDispatcherLLL|^Mozilla$|Mozilla.*Indy|Mozilla.*NEWT|Mozilla*MSIECrawler|MS\\ FrontPage*|MSFrontPage|MSIECrawler|MSProxy|multithreaddb|nationaldirectory|Navroad|NearSite|NetAnts|NetCarta|NetMechanic|netprospector|NetResearchServer|NetSpider|Net\\ Vampire|NetZIP|NetZip\\ Downloader|NetZippy|NEWT|NICErsPRO|Ninja|NPBot|Octopus|Offline\\ Explorer|Offline\\ Navigator|OpaL|Openfind|OpenTextSiteCrawler|OrangeBot|PageGrabber|Papa\\ Foto|PackRat|pavuk|pcBrowser|PersonaPilot|Ping|PingALink|Pockey|Proxy|psbot|PSurf|puf|Pump|PushSite|QRVA|RealDownload|Reaper|Recorder|ReGet|replacer|RepoMonkey|Robozilla|Rover|RPT-HTTPClient|Rsync|Scooter|SearchExpress|searchhippo|searchterms\\.it|Second\\ Street\\ Research|Seeker|Shai|Siphon|sitecheck|sitecheck.internetseer.com|SiteSnagger|SlySearch|SmartDownload|snagger|Snake|SpaceBison|Spegla|SpiderBot|sproose|SqWorm|Stripper|Sucker|SuperBot|SuperHTTP|Surfbot|SurfWalker|Szukacz|tAkeOut|tarspider|Teleport\\ Pro|Templeton|TrueRobot|TV33_Mercator|UIowaCrawler|UtilMind|URLSpiderPro|URL_Spider_Pro|Vacuum|vagabondo|vayala|visibilitygap|VoidEYE|vspider|Web\\ Downloader|w3mir|Web\\ Data\\ Extractor|Web\\ Image\\ Collector|Web\\ Sucker|Wweb|WebAuto|WebBandit|web\\.by\\.mail|Webclipping|webcollage|webcollector|WebCopier|webcraft@bea|webdevil|webdownloader|Webdup|WebEMailExtrac|WebFetch|WebGo\\ IS|WebHook|Webinator|WebLeacher|WEBMASTERS|WebMiner|WebMirror|webmole|WebReaper|WebSauger|Website|Website\\ eXtractor|Website\\ Quester|WebSnake|Webster|WebStripper|websucker|webvac|webwalk|webweasel|WebWhacker|WebZIP|Wget|Whacker|whizbang|WhosTalking|Widow|WISEbot|WWWOFFLE|x-Tractor|^Xaldon\\ WebSpider|WUMPUS|Xenu|XGET|Zeus.*Webster|Zeus [NC]\n\nTaken from:\nhttp://perishablepress.com/press/2007/10/15/ultimate-htaccess-blacklist-2-compressed-version/\n', '\nYou can also check referrals. No referral could raise bot suspition. Bad referral means certainly it is not browser.\n\nAdding invisible links (possibly marked as rel=""nofollow""?),\n\n* style=""display: none;"" on link or parent container\n* placed underneath another element with higher z-index\n\nI would\'nt do that. You can end up blacklisted by google for black hat SEO :)\n', ""\nI currently work for a company that scans web sites in order to classify them. We also check sites for malware.\nIn my experience the number one blockers of our web crawler (which of course uses a IE or Firefox UA and does not obey robots.txt. Duh.) are sites intentionally hosting malware.  It's a pain because the site then falls back to a human who has to manually load the site, classify it and check it for malware.\nI'm just saying, by blocking web crawlers you're putting yourself in some bad company.\nOf course, if they are horribly rude and suck up tons of your bandwidth it's a different story because then you've got a good reason.\n"", ""\nPeople keep addressing broad crawlers but not crawlers that are specialized for your website.\nI write stealth crawlers and if they are individually built no amount of honey pots or hidden links will have any effect whatsoever - the only real way to detect specialised crawlers is by inspecting connection patterns. \nThe best systems use AI (e.g. Linkedin) use AI to address this.\nThe easiest solution is write log parsers that analyze IP connections and simply blacklist those IPs or serve captcha, at least temporary. \ne.g.\nif IP X is seen every 2 seconds connecting to foo.com/cars/*.html but not any other pages - it's most likely a bot or a hungry power user.\nAlternatively there are various javascript challenges that act as protection (e.g. Cloudflare's anti-bot system), but those are easily solvable, you can write something custom and that might be enough deterrent to make it not worth the effort for the crawler.\nHowever you must ask a question are you willing to false-positive legit users and introduce inconvenience for them to prevent bot traffic. Protecting public data is an impossible paradox.\n"", ""\nshort answer: if a mid level programmer knows what he's doing you won't be able to detect a crawler without affecting the real user. Having your information publicly you won't be able to defend it against a crawler... it's like the 1st amendment right :)\n""]",https://stackoverflow.com/questions/233192/detecting-stealth-web-crawlers,web-crawler
Click a Button in Scrapy,"
I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).
I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need.
How can I simply click a button, which then shows the information I need?
Do I have to use an external library like mechanize or lxml?
",76k,"
            67
        ","[""\nScrapy cannot interpret javascript.\nIf you absolutely must interact with the javascript on the page, you want to be using Selenium.\nIf using Scrapy, the solution to the problem depends on what the button is doing.\nIf it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.\nIf it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.\n\nDo I have to use an external library like mechanize or lxml?\n\nIf you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.\nIf you can give the URL of the page you're working on scraping I can take a look.\n"", '\nSelenium browser provide very nice solution. Here is an example (pip install -U selenium):\nfrom selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = \'xxx\'\n    allowed_domains = [\'www.example.org\']\n    start_urls = [\'https://www.example.org\']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get(\'https://www.example.org/abc\')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath(\'//*[@id=""BTN_NEXT""]\')\n                    url = \'http://www.example.org/abcd\'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print \'you are here!\'\n\n', '\nTo properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.\n', ""\nAlthough it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:\nfrom helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n\n""]",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,web-crawler
How to give URL to scrapy for crawling?,"
I want to use scrapy for crawling web pages. Is there a way to pass the start URL from the terminal itself?
It is given in the documentation that either the name of the spider or the URL can be given, but when i given the url it throws an error:
//name of my spider is example, but i am giving url instead of my spider name(It works fine if i give spider name).

scrapy crawl example.com                 

ERROR:

File
  ""/usr/local/lib/python2.7/dist-packages/Scrapy-0.14.1-py2.7.egg/scrapy/spidermanager.py"",
  line 43, in create
      raise KeyError(""Spider not found: %s"" % spider_name) KeyError: 'Spider not found: example.com'

How can i make scrapy to use my spider on the url given in the terminal??
",24k,"
            35
        ","['\nI\'m  not really sure about the commandline option. However, you could write your spider like this.\nclass MySpider(BaseSpider):\n\n    name = \'my_spider\'    \n\n    def __init__(self, *args, **kwargs): \n      super(MySpider, self).__init__(*args, **kwargs) \n\n      self.start_urls = [kwargs.get(\'start_url\')] \n\nAnd start it like:\nscrapy crawl my_spider -a start_url=""http://some_url""\n', '\nAn even easier way to allow multiple url-arguments than what Peter suggested is by giving them as a string with the urls separated by a comma, like this:\n-a start_urls=""http://example1.com,http://example2.com""\n\nIn the spider you would then simply split the string on \',\' and get an array of urls:\nself.start_urls = kwargs.get(\'start_urls\').split(\',\')\n\n', '\nUse scrapy parse command. You can parse a url with your spider. url is passed from command.\n$ scrapy parse http://www.example.com/ --spider=spider-name\n\nhttp://doc.scrapy.org/en/latest/topics/commands.html#parse\n', '\nSjaak Trekhaak has the right idea and here is how to allow multiples:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n', ""\nThis is an extension to the approach given by Sjaak Trekhaak in this thread. The approach as it is so far only works if you provide exactly one url. For example, if you want to provide more than one url like this, for instance: \n-a start_url=http://url1.com,http://url2.com\n\nthen Scrapy (I'm using the current stable version 0.14.4) will terminate with the following exception:\nerror: running 'scrapy crawl' with more than one spider is no longer supported\n\nHowever, you can circumvent this problem by choosing a different variable for each start url, together with an argument that holds the number of passed urls. Something like this:\n-a start_url1=http://url1.com \n-a start_url2=http://url2.com \n-a urls_num=2\n\nYou can then do the following in your spider:\nclass MySpider(BaseSpider):\n\n    name = 'my_spider'    \n\n    def __init__(self, *args, **kwargs): \n        super(MySpider, self).__init__(*args, **kwargs) \n\n        urls_num = int(kwargs.get('urls_num'))\n\n        start_urls = []\n        for i in xrange(1, urls_num):\n            start_urls.append(kwargs.get('start_url{0}'.format(i)))\n\n        self.start_urls = start_urls\n\nThis is a somewhat ugly hack but it works. Of course, it's tedious to explicitly write down all command line arguments for each url. Therefore, it makes sense to wrap the scrapy crawl command in a Python subprocess and generate the command line arguments in a loop or something.\nHope it helps. :) \n"", '\nYou can also try this:\n>>> scrapy view http://www.sitename.com\n\nIt will open a window in browser of requested URL.\n']",https://stackoverflow.com/questions/9681114/how-to-give-url-to-scrapy-for-crawling,web-crawler
Spider a Website and Return URLs Only,"
I'm looking for a way to pseudo-spider a website. The key is that I don't actually want the content, but rather a simple list of URIs. I can get reasonably close to this idea with Wget using the --spider option, but when piping that output through a grep, I can't seem to find the right magic to make it work:
wget --spider --force-html -r -l1 http://somesite.com | grep 'Saving to:'

The grep filter seems to have absolutely no affect on the wget output. Have I got something wrong or is there another tool I should try that's more geared towards providing this kind of limited result set?
UPDATE
So I just found out offline that, by default, wget writes to stderr. I missed that in the man pages (in fact, I still haven't found it if it's in there). Once I piped the return to stdout, I got closer to what I need:
wget --spider --force-html -r -l1 http://somesite.com 2>&1 | grep 'Saving to:'

I'd still be interested in other/better means for doing this kind of thing, if any exist.
",88k,"
            71
        ","[""\nThe absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\nwget --spider --force-html -r -l2 $url 2>&1 \\\n  | grep '^--' | awk '{ print $3 }' \\\n  | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n  > urls.m3u\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself.\n"", '\nCreate a few regular expressions to extract the addresses from all\n<a href=""(ADDRESS_IS_HERE)"">.\n\nHere is the solution I would use:\nwget -q http://example.com -O - | \\\n    tr ""\\t\\r\\n\'"" \'   ""\' | \\\n    grep -i -o \'<a[^>]\\+href[ ]*=[ \\t]*""\\(ht\\|f\\)tps\\?:[^""]\\+""\' | \\\n    sed -e \'s/^.*""\\([^""]\\+\\)"".*$/\\1/g\'\n\nThis will output all http, https, ftp, and ftps links from a webpage.  It will not give you relative urls, only full urls.\nExplanation regarding the options used in the series of piped commands:\nwget -q makes it not have excessive output (quiet mode).\nwget -O - makes it so that the downloaded file is echoed to stdout, rather than saved to disk.\ntr is the unix character translator, used in this example to translate newlines and tabs to spaces, as well as convert single quotes into double quotes so we can simplify our regular expressions.\ngrep -i makes the search case-insensitive\ngrep -o makes it output only the matching portions.\nsed is the Stream EDitor unix utility which allows for filtering and transformation operations.\nsed -e just lets you feed it an expression.\nRunning this little script on ""http://craigslist.org"" yielded quite a long list of links:\nhttp://blog.craigslist.org/\nhttp://24hoursoncraigslist.com/subs/nowplaying.html\nhttp://craigslistfoundation.org/\nhttp://atlanta.craigslist.org/\nhttp://austin.craigslist.org/\nhttp://boston.craigslist.org/\nhttp://chicago.craigslist.org/\nhttp://cleveland.craigslist.org/\n...\n\n', '\nI\'ve used a tool called xidel \nxidel http://server -e \'//a/@href\' | \ngrep -v ""http"" | \nsort -u | \nxargs -L1 -I {}  xidel http://server/{} -e \'//a/@href\' | \ngrep -v ""http"" | sort -u\n\nA little hackish but gets you closer! This is only the first level. Imagine packing this up into a self recursive script!\n']",https://stackoverflow.com/questions/2804467/spider-a-website-and-return-urls-only,web-crawler
Selenium wait for Ajax content to load - universal approach,"
Is there a universal approach for Selenium to wait till all ajax content has loaded? (not tied to a specific website - so it works for every ajax website)
",32k,"
            23
        ","['\nYou need to wait for Javascript and jQuery to finish loading. Execute Javascript to check if jQuery.active is 0 and document.readyState is complete, which means the JS and jQuery load is complete.\npublic boolean waitForJSandJQueryToLoad() {\n\n    WebDriverWait wait = new WebDriverWait(driver, 30);\n\n    // wait for jQuery to load\n    ExpectedCondition<Boolean> jQueryLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        try {\n          return ((Long)((JavascriptExecutor)getDriver()).executeScript(""return jQuery.active"") == 0);\n        }\n        catch (Exception e) {\n          // no jQuery present\n          return true;\n        }\n      }\n    };\n\n    // wait for Javascript to load\n    ExpectedCondition<Boolean> jsLoad = new ExpectedCondition<Boolean>() {\n      @Override\n      public Boolean apply(WebDriver driver) {\n        return ((JavascriptExecutor)getDriver()).executeScript(""return document.readyState"")\n        .toString().equals(""complete"");\n      }\n    };\n\n  return wait.until(jQueryLoad) && wait.until(jsLoad);\n}\n\n', '\nAs Mark Collin described in his book ""Mastering Selenium Webdriver"", use JavascriptExecutor let you figure out whether a website using jQuery has finished making AJAX calls\npublic class AdditionalConditions {\n\n  public static ExpectedCondition<Boolean> jQueryAJAXCallsHaveCompleted() {\n    return new ExpectedCondition<Boolean>() {\n\n        @Override\n        public Boolean apply(WebDriver driver) {\n            return (Boolean) ((JavascriptExecutor) driver).executeScript(""return (window.jQuery != null) && (jQuery.active === 0);"");\n        }\n    };\n  }\n}\n\n', '\nI have been using this simple do while to iterate until an AJAX is finished. \nIt consistently works for me. \npublic void waitForAjax() throws InterruptedException{\n    while (true)\n    {\n        Boolean ajaxIsComplete = (Boolean) ((JavascriptExecutor)driver).executeScript(""return jQuery.active == 0"");\n        if (ajaxIsComplete){\n            info(""Ajax Call completed. "");\n            break;\n        }\n        Thread.sleep(150);\n    }\n}\n\n', ""\nI don't believe that there is a universal approach out of the box. I typically make a method that does a .waituntilrowcount(2) or waituntilvisible() that polls an element.\n""]",https://stackoverflow.com/questions/33348600/selenium-wait-for-ajax-content-to-load-universal-approach,web-crawler
Scrapy CrawlSpider doesn't crawl the first landing page,"
I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.
Although the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:
class DownloadSpider(CrawlSpider):
    name = 'downloader'
    allowed_domains = ['bnt-chemicals.de']
    start_urls = [
        ""http://www.bnt-chemicals.de""        
        ]
    rules = (   
        Rule(SgmlLinkExtractor(aloow='prod'), callback='parse_item', follow=True),
        )
    fname = 1

    def parse_item(self, response):
        open(str(self.fname)+ '.txt', 'a').write(response.url)
        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))
        open(str(self.fname)+ '.txt', 'a').write('\n')
        open(str(self.fname)+ '.txt', 'a').write(response.body)
        open(str(self.fname)+ '.txt', 'a').write('\n')
        self.fname = self.fname + 1

",7k,"
            18
        ","['\nJust change your callback to parse_start_url and override it:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [\n        ""http://www.bnt-chemicals.de"",\n    ]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_start_url\', follow=True),\n    )\n    fname = 0\n\n    def parse_start_url(self, response):\n        self.fname += 1\n        fname = \'%s.txt\' % self.fname\n\n        with open(fname, \'w\') as f:\n            f.write(\'%s, %s\\n\' % (response.url, response.meta.get(\'depth\', 0)))\n            f.write(\'%s\\n\' % response.body)\n\n', '\nThere\'s a number of ways of doing this, but one of the simplest is to implement parse_start_url and then modify start_urls\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DownloadSpider(CrawlSpider):\n    name = \'downloader\'\n    allowed_domains = [\'bnt-chemicals.de\']\n    start_urls = [""http://www.bnt-chemicals.de/tunnel/index.htm""]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=\'prod\'), callback=\'parse_item\', follow=True),\n        )\n    fname = 1\n\n    def parse_start_url(self, response):\n        return self.parse_item(response)\n\n\n    def parse_item(self, response):\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.url)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\',\'+ str(response.meta[\'depth\']))\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        open(str(self.fname)+ \'.txt\', \'a\').write(response.body)\n        open(str(self.fname)+ \'.txt\', \'a\').write(\'\\n\')\n        self.fname = self.fname + 1\n\n']",https://stackoverflow.com/questions/15836062/scrapy-crawlspider-doesnt-crawl-the-first-landing-page,web-crawler
How to find all links / pages on a website,"
Is it possible to find all the pages and links on ANY given website? I'd like to enter a URL and produce a directory tree of all links from that site?
I've looked at HTTrack but that downloads the whole site and I simply need the directory tree.
",532k,"
            125
        ","['\nCheck out linkchecker—it will crawl the site (while obeying robots.txt) and generate a report. From there, you can script up a solution for creating the directory tree.\n', ""\nIf you have the developer console (JavaScript) in your browser, you can type this code in:\nurls = document.querySelectorAll('a'); for (url in urls) console.log(urls[url].href);\n\nShortened:\nn=$$('a');for(u in n)console.log(n[u].href)\n\n"", '\nAnother alternative might be\nArray.from(document.querySelectorAll(""a"")).map(x => x.href)\n\nWith your $$( its even shorter\nArray.from($$(""a"")).map(x => x.href)\n\n', '\nIf this is a programming question, then I would suggest you write your own regular expression to parse all the retrieved contents. Target tags are IMG and A for standard HTML. For JAVA, \nfinal String openingTags = ""(<a [^>]*href=[\'\\""]?|<img[^> ]* src=[\'\\""]?)"";\n\nthis along with Pattern and Matcher classes should detect the beginning of the tags. Add LINK tag if you also want CSS.\nHowever, it is not as easy as you may have intially thought. Many web pages are not well-formed. Extracting all the links programmatically that human being can ""recognize"" is really difficult if you need to take into account all the irregular expressions.\nGood luck!\n', '\nfunction getalllinks($url) {\n    $links = array();\n    if ($fp = fopen($url, \'r\')) {\n        $content = \'\';\n        while ($line = fread($fp, 1024)) {\n            $content. = $line;\n        }\n    }\n    $textLen = strlen($content);\n    if ($textLen > 10) {\n        $startPos = 0;\n        $valid = true;\n        while ($valid) {\n            $spos = strpos($content, \'<a \', $startPos);\n            if ($spos < $startPos) $valid = false;\n            $spos = strpos($content, \'href\', $spos);\n            $spos = strpos($content, \'""\', $spos) + 1;\n            $epos = strpos($content, \'""\', $spos);\n            $startPos = $epos;\n            $link = substr($content, $spos, $epos - $spos);\n            if (strpos($link, \'http://\') !== false) $links[] = $link;\n        }\n    }\n    return $links;\n}\n\ntry this code....\n']",https://stackoverflow.com/questions/1439326/how-to-find-all-links-pages-on-a-website,web-crawler
how to filter duplicate requests based on url in scrapy,"
I am writing a crawler for a website using scrapy with CrawlSpider.
Scrapy provides an in-built duplicate-request filter which filters duplicate requests based on urls. Also, I can filter requests using rules member of CrawlSpider. 
What I want to do is to filter requests like:
http:://www.abc.com/p/xyz.html?id=1234&refer=5678

If I have already visited
http:://www.abc.com/p/xyz.html?id=1234&refer=4567


NOTE: refer is a parameter that doesn't affect the response I get, so I don't care if the value of that parameter changes.

Now, if I have a set which accumulates all ids I could ignore it in my callback function parse_item (that's my callback function) to achieve this functionality.
But that would mean I am still at least fetching that page, when I don't need to.
So what is the way in which I can tell scrapy that it shouldn't send a particular request based on the url?
",19k,"
            42
        ","['\nYou can write custom middleware for duplicate removal and add it in settings\nimport os\n\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass CustomFilter(RFPDupeFilter):\n""""""A dupe filter that considers specific ids in the url""""""\n\n    def __getid(self, url):\n        mm = url.split(""&refer"")[0] #or something like that\n        return mm\n\n    def request_seen(self, request):\n        fp = self.__getid(request.url)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)\n\nThen you need to set the correct DUPFILTER_CLASS in settings.py\nDUPEFILTER_CLASS = \'scraper.duplicate_filter.CustomFilter\'\n\nIt should work after that\n', '\nFollowing ytomar\'s lead, I wrote this filter that filters based purely on URLs that have already been seen by checking an in-memory set.  I\'m a Python noob so let me know if I screwed something up, but it seems to work all right:\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass SeenURLFilter(RFPDupeFilter):\n    """"""A dupe filter that considers the URL""""""\n\n    def __init__(self, path=None):\n        self.urls_seen = set()\n        RFPDupeFilter.__init__(self, path)\n\n    def request_seen(self, request):\n        if request.url in self.urls_seen:\n            return True\n        else:\n            self.urls_seen.add(request.url)\n\nAs ytomar mentioned, be sure to add the DUPEFILTER_CLASS constant to settings.py:\nDUPEFILTER_CLASS = \'scraper.custom_filters.SeenURLFilter\'\n\n', '\nhttps://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py\nThis file might help you. This file creates a database of unique delta fetch key from the url ,a user pass in a scrapy.Reqeust(meta={\'deltafetch_key\':uniqe_url_key}).\nThis this let you avoid duplicate requests you already have visited in the past.\nA sample mongodb implementation using deltafetch.py \n        if isinstance(r, Request):\n            key = self._get_key(r)\n            key = key+spider.name\n\n            if self.db[\'your_collection_to_store_deltafetch_key\'].find_one({""_id"":key}):\n                spider.log(""Ignoring already visited: %s"" % r, level=log.INFO)\n                continue\n        elif isinstance(r, BaseItem):\n\n            key = self._get_key(response.request)\n            key = key+spider.name\n            try:\n                self.db[\'your_collection_to_store_deltafetch_key\'].insert({""_id"":key,""time"":datetime.now()})\n            except:\n                spider.log(""Ignoring already visited: %s"" % key, level=log.ERROR)\n        yield r\n\neg. id = 345\nscrapy.Request(url,meta={deltafetch_key:345},callback=parse)\n', ""\nHere is my custom filter base on scrapy 0.24.6.\nIn this filter, it only cares id in the url. for example\nhttp://www.example.com/products/cat1/1000.html?p=1\nhttp://www.example.com/products/cat2/1000.html?p=2\nare treated as same url. But\nhttp://www.example.com/products/cat2/all.html\nwill not.\nimport re\nimport os\nfrom scrapy.dupefilter import RFPDupeFilter\n\n\nclass MyCustomURLFilter(RFPDupeFilter):\n\n    def _get_id(self, url):\n        m = re.search(r'(\\d+)\\.html', url)\n        return None if m is None else m.group(1)\n\n    def request_fingerprint(self, request):\n        style_id = self._get_id(request.url)\n        return style_id\n\n"", ""\nIn the latest scrapy, we can use the default duplication filter or extend and have custom one.\ndefine the below config in spider settings\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n""]",https://stackoverflow.com/questions/12553117/how-to-filter-duplicate-requests-based-on-url-in-scrapy,web-crawler
crawl site that has infinite scrolling using python,"
I have been doing research and so far I found out the python package that I will plan on using its scrapy, now I am trying to find out what is a good way to build a scraper using scrapy to crawl site with infinite scrolling. After digging around I found out that there is a package call selenium and it has python module. I have a feeling someone has already done that using Scrapy and Selenium to scrape site with infinite scrolling. It would be great if someone can point towards to an example. 
",28k,"
            10
        ","['\nYou can use selenium to scrap the infinite scrolling website like twitter or facebook. \nStep 1 : Install Selenium using pip \npip install selenium \n\nStep 2 : use the code below to automate infinite scroll and extract the source code\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = ""https://twitter.com""\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + ""/search?q=stackoverflow&src=typd"")\n        driver.find_element_by_link_text(""All"").click()\n        for i in range(1,100):\n            self.driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode(\'utf-8\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\nThe for loop allows you to parse through the infinite scrolls and post which you can extract the loaded data.\nStep 3 : Print the data if required.\n', '\nThis is short & simple code which is working for me:\nSCROLL_PAUSE_TIME = 20\n\n# Get scroll height\nlast_height = driver.execute_script(""return document.body.scrollHeight"")\n\nwhile True:\n    # Scroll down to bottom\n    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = driver.execute_script(""return document.body.scrollHeight"")\n    if new_height == last_height:\n        break\n    last_height = new_height\n\nposts = driver.find_elements_by_class_name(""post-text"")\n\nfor block in posts:\n    print(block.text)\n\n']",https://stackoverflow.com/questions/22702277/crawl-site-that-has-infinite-scrolling-using-python,web-crawler
find a word on a website and get its page link,"
I want to scrape a few websites and see if the word ""katalog"" is present there. If yes, I want to retrieve the link of all the tabs/sub pages where that word is present. Is it possible to do so?
I tried following this tutorial but the wordlist.csv I get at the end is empty even though the word catalog does exist on the website.
https://www.phooky.com/blog/find-specific-words-on-web-pages-with-scrapy/
        wordlist = [
            ""katalog"",
            ""downloads"",
            ""download""
            ]

def find_all_substrings(string, sub):
    starts = [match.start() for match in re.finditer(re.escape(sub), string)]
    return starts

class WebsiteSpider(CrawlSpider):

    name = ""webcrawler""
    allowed_domains = [""www.reichelt.com/""]
    start_urls = [""https://www.reichelt.com/""]
    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]

    crawl_count = 0
    words_found = 0                                 

    def check_buzzwords(self, response):

        self.__class__.crawl_count += 1

        crawl_count = self.__class__.crawl_count

        url = response.url
        contenttype = response.headers.get(""content-type"", """").decode('utf-8').lower()
        data = response.body.decode('utf-8')

        for word in wordlist:
                substrings = find_all_substrings(data, word)
                print(""substrings"", substrings)
                for pos in substrings:
                        ok = False
                        if not ok:
                                self.__class__.words_found += 1
                                print(word + "";"" + url + "";"")
        return Item()

    def _requests_to_follow(self, response):
        if getattr(response, ""encoding"", None) != None:
                return CrawlSpider._requests_to_follow(self, response)
        else:
                return []

How can I find all instances of a word on a website and obtain the link of the page where the word is founded?
",2k,"
            2
        ","['\nMain problem is wrong allowed_domain - it has to be without path /\n    allowed_domains = [""www.reichelt.com""]\n\nOther problems can be this tutorial is 3 years old (there is link to documentation for Scarpy 1.5 but newest version is 2.5.0).\nIt also uses some useless lines of code.\nIt gets contenttype but never use it to decode request.body. Your url  uses iso8859-1 for original language and utf-8 for ?LANGUAGE=PL - but you can simply use request.text and it will automatically decode it.\nIt also uses ok = False and later check it but it is totally useless.\n\nMinimal working code - you can copy it to single file and run as python script.py without creating project.\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nimport re\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\ndef find_all_substrings(string, sub):\n    return [match.start() for match in re.finditer(re.escape(sub), string)]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    #start_urls = [""https://www.reichelt.com/?LANGUAGE=PL""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    #crawl_count = 0\n    #words_found = 0                                 \n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        #self.crawl_count += 1\n\n        #content_type = response.headers.get(""content-type"", """").decode(\'utf-8\').lower()\n        #print(\'content_type:\', content_type)\n        #data = response.body.decode(\'utf-8\')\n        \n        data = response.text\n\n        for word in wordlist:\n            print(\'[check_buzzwords] check word:\', word)\n            substrings = find_all_substrings(data, word)\n            print(\'[check_buzzwords] substrings:\', substrings)\n            \n            for pos in substrings:\n                #self.words_found += 1\n                # only display\n                print(\'[check_buzzwords] word: {} | pos: {} | sub: {} | url: {}\'.format(word, pos, data[pos-20:pos+20], response.url))\n                # send to file\n                yield {\'word\': word, \'pos\': pos, \'sub\': data[pos-20:pos+20], \'url\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n\nEDIT:\nI added data[pos-20:pos+20] to yielded data to see where is substring and sometimes it is in URL like .../elements/adw_2018/catalog/... or other place like <img alt=""""catalog"""" - so using regex doesn\'t have to be good idea. Maybe better is to use xpath or css selector to search text only in some places or in links.\n\nEDIT:\nVersion which search links with words from list. It uses response.xpath to search all linsk and later it check if there is word in href - so it doesn\'t need regex.\nProblem can be that it treats link with -downloads- (with s) as link with word download and downloads so it would need more complex method to check (ie. using regex) to treats it only as link with word downloads\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nwordlist = [\n    ""katalog"",\n    ""catalog"",\n    ""downloads"",\n    ""download"",\n]\n\nclass WebsiteSpider(CrawlSpider):\n\n    name = ""webcrawler""\n    \n    allowed_domains = [""www.reichelt.com""]\n    start_urls = [""https://www.reichelt.com/""]\n    \n    rules = [Rule(LinkExtractor(), follow=True, callback=""check_buzzwords"")]\n\n    def check_buzzwords(self, response):\n        print(\'[check_buzzwords] url:\', response.url)\n        \n        links = response.xpath(\'//a[@href]\')\n        \n        for word in wordlist:\n            \n            for link in links:\n                url = link.attrib.get(\'href\')\n                if word in url:\n                    print(\'[check_buzzwords] word: {} | url: {} | page: {}\'.format(word, url, response.url))\n                    # send to file\n                    yield {\'word\': word, \'url\': url, \'page\': response.url}\n\n# --- run without project and save in `output.csv` ---\n\nfrom scrapy.crawler import CrawlerProcess\n\nc = CrawlerProcess({\n    \'USER_AGENT\': \'Mozilla/5.0\',\n    # save in file CSV, JSON or XML\n    \'FEEDS\': {\'output.csv\': {\'format\': \'csv\'}},  # new in 2.1\n})\nc.crawl(WebsiteSpider)\nc.start() \n\n', '\nYou can do it with requests-html and rendering the page:\nfrom requests_html import HTMLSession\n\nsession = HTMLSession()\nurl = ""https://www.reichelt.com/""\n\nr = session.get(url)\nr.html.render(sleep=2)\n\nif ""your_word"" in r.html.text: #or r.html.html if you want it in raw html\n    print([link for link in r.html.absolute_links if ""your_word"" in link])\n\n']",https://stackoverflow.com/questions/68193300/find-a-word-on-a-website-and-get-its-page-link,web-crawler
Nodejs: Async request with a list of URL,"
I am working on a crawler. I have a list of URL need to be requested. There are several hundreds of request at the same time if I don't set it to be async. I am afraid that it would explode my bandwidth or produce to much network access to the target website. What should I do?
Here is what I am doing: 
urlList.forEach((url, index) => {

    console.log('Fetching ' + url);
    request(url, function(error, response, body) {
        //do sth for body

    });
});

I want one request is called after one request is completed.
",967,"
            0
        ","['\nYou can use something like Promise library e.g. snippet\nconst Promise = require(""bluebird"");\nconst axios = require(""axios"");\n\n//Axios wrapper for error handling\nconst axios_wrapper = (options) => {\n    return axios(...options)\n        .then((r) => {\n            return Promise.resolve({\n                data: r.data,\n                error: null,\n            });\n        })\n        .catch((e) => {\n            return Promise.resolve({\n                data: null,\n                error: e.response ? e.response.data : e,\n            });\n        });\n};\n\nPromise.map(\n    urls,\n    (k) => {\n        return axios_wrapper({\n            method: ""GET"",\n            url: k,\n        });\n    },\n    { concurrency: 1 } // Here 1 represents how many requests you want to run in parallel\n)\n    .then((r) => {\n        console.log(r);\n        //Here r will be an array of objects like {data: [{}], error: null}, where if the request was successfull it will have data value present otherwise error value will be non-null\n    })\n    .catch((e) => {\n        console.error(e);\n    });\n\n', ""\nThe things you need to watch for are:\n\nWhether the target site has rate limiting and you may be blocked from access if you try to request too much too fast?\nHow many simultaneous requests the target site can handle without degrading its performance?\nHow much bandwidth your server has on its end of things?\nHow many simultaneous requests your own server can have in flight and process without causing excess memory usage or a pegged CPU.\n\nIn general, the scheme for managing all this is to create a way to tune how many requests you launch.  There are many different ways to control this by number of simultaneous requests, number of requests per second, amount of data used, etc...\nThe simplest way to start would be to just control how many simultaneous requests you make.  That can be done like this:\nfunction runRequests(arrayOfData, maxInFlight, fn) {\n    return new Promise((resolve, reject) => {\n        let index = 0;\n        let inFlight = 0;\n\n        function next() {\n            while (inFlight < maxInFlight && index < arrayOfData.length) {\n                ++inFlight;\n                fn(arrayOfData[index++]).then(result => {\n                    --inFlight;\n                    next();\n                }).catch(err => {\n                    --inFlight;\n                    console.log(err);\n                    // purposely eat the error and let the rest of the processing continue\n                    // if you want to stop further processing, you can call reject() here\n                    next();\n                });\n            }\n            if (inFlight === 0) {\n                // all done\n                resolve();\n            }\n        }\n        next();\n    });\n}\n\nAnd, then you would use that like this:\nconst rp = require('request-promise');\n\n// run the whole urlList, no more than 10 at a time\nrunRequests(urlList, 10, function(url) {\n    return rp(url).then(function(data) {\n        // process fetched data here for one url\n    }).catch(function(err) {\n        console.log(url, err);\n    });\n}).then(function() {\n    // all requests done here\n});\n\nThis can be made as sophisticated as you want by adding a time element to it (no more than N requests per second) or even a bandwidth element to it.\n\nI want one request is called after one request is completed.\n\nThat's a very slow way to do things.  If you really want that, then you can just pass a 1 for the maxInFlight parameter to the above function, but typically, things would work a lot faster and not cause problems by allowing somewhere between 5 and 50 simultaneous requests.  Only testing would tell you where the sweet spot is for your particular target sites and your particular server infrastructure and amount of processing you need to do on the results.\n"", '\nyou can use set timeout function to process all request within loop. for that you must know maximum time to process a request.\n']",https://stackoverflow.com/questions/47299174/nodejs-async-request-with-a-list-of-url,web-crawler
How can I use different pipelines for different spiders in a single Scrapy project,"
I have a scrapy project which contains multiple spiders.
Is there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider.
Thanks
",32k,"
            97
        ","[""\nJust remove all pipelines from main settings and use this inside spider.\nThis will define the pipeline to user per spider\nclass testSpider(InitSpider):\n    name = 'test'\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'app.MyPipeline': 400\n        }\n    }\n\n"", ""\nBuilding on the solution from Pablo Hoffman, you can use the following decorator on the process_item method of a Pipeline object so that it checks the pipeline attribute of your spider for whether or not it should be executed. For example:\ndef check_spider_pipeline(process_item_method):\n\n    @functools.wraps(process_item_method)\n    def wrapper(self, item, spider):\n\n        # message template for debugging\n        msg = '%%s %s pipeline step' % (self.__class__.__name__,)\n\n        # if class is in the spider's pipeline, then use the\n        # process_item method normally.\n        if self.__class__ in spider.pipeline:\n            spider.log(msg % 'executing', level=log.DEBUG)\n            return process_item_method(self, item, spider)\n\n        # otherwise, just return the untouched item (skip this step in\n        # the pipeline)\n        else:\n            spider.log(msg % 'skipping', level=log.DEBUG)\n            return item\n\n    return wrapper\n\nFor this decorator to work correctly, the spider must have a pipeline attribute with a container of the Pipeline objects that you want to use to process the item, for example:\nclass MySpider(BaseSpider):\n\n    pipeline = set([\n        pipelines.Save,\n        pipelines.Validate,\n    ])\n\n    def parse(self, response):\n        # insert scrapy goodness here\n        return item\n\nAnd then in a pipelines.py file:\nclass Save(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do saving here\n        return item\n\nclass Validate(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do validating here\n        return item\n\nAll Pipeline objects should still be defined in ITEM_PIPELINES in settings (in the correct order -- would be nice to change so that the order could be specified on the Spider, too).\n"", '\nThe other solutions given here are good, but I think they could be slow, because we are not really not using the pipeline per spider, instead we are checking if a pipeline exists every time an item is returned (and in some cases this could reach millions).\nA good way to completely disable (or enable) a feature per spider is using custom_setting and from_crawler for all extensions like this:\npipelines.py\nfrom scrapy.exceptions import NotConfigured\n\nclass SomePipeline(object):\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool(\'SOMEPIPELINE_ENABLED\'):\n            # if this isn\'t specified in settings, the pipeline will be completely disabled\n            raise NotConfigured\n        return cls()\n\n    def process_item(self, item, spider):\n        # change my item\n        return item\n\nsettings.py\nITEM_PIPELINES = {\n   \'myproject.pipelines.SomePipeline\': 300,\n}\nSOMEPIPELINE_ENABLED = True # you could have the pipeline enabled by default\n\nspider1.py\nclass Spider1(Spider):\n\n    name = \'spider1\'\n\n    start_urls = [""http://example.com""]\n\n    custom_settings = {\n        \'SOMEPIPELINE_ENABLED\': False\n    }\n\nAs you check, we have specified custom_settings that will override the things specified in settings.py, and we are disabling SOMEPIPELINE_ENABLED for this spider.\nNow when you run this spider, check for something like:\n[scrapy] INFO: Enabled item pipelines: []\n\nNow scrapy has completely disabled the pipeline, not bothering of its existence for the whole run. Check that this also works for scrapy extensions and middlewares.\n', ""\nYou can use the name attribute of the spider in your pipeline\nclass CustomPipeline(object)\n\n    def process_item(self, item, spider)\n         if spider.name == 'spider1':\n             # do something\n             return item\n         return item\n\nDefining all pipelines this way can accomplish what you want.\n"", ""\nI can think of at least four approaches:\n\nUse a different scrapy project per set of spiders+pipelines (might be appropriate if your spiders are different enough warrant being in different projects)\nOn the scrapy tool command line, change the pipeline setting with scrapy settings in between each invocation of your spider\nIsolate your spiders into their own scrapy tool commands, and define the default_settings['ITEM_PIPELINES'] on your command class to the pipeline list you want for that command. See line 6 of this example.\nIn the pipeline classes themselves, have process_item() check what spider it's running against, and do nothing if it should be ignored for that spider. See the example using resources per spider to get you started. (This seems like an ugly solution because it tightly couples spiders and item pipelines. You probably shouldn't use this one.)\n\n"", ""\nThe most simple and effective solution is to set custom settings in each spider itself.\ncustom_settings = {'ITEM_PIPELINES': {'project_name.pipelines.SecondPipeline': 300}}\n\nAfter that you need to set them in the settings.py file\nITEM_PIPELINES = {\n   'project_name.pipelines.FistPipeline': 300,\n   'project_name.pipelines.SecondPipeline': 400\n}\n\nin that way each spider will use the respective pipeline.\n"", '\nYou can just set the item pipelines settings inside of the spider like this:\nclass CustomSpider(Spider):\n    name = \'custom_spider\'\n    custom_settings = {\n        \'ITEM_PIPELINES\': {\n            \'__main__.PagePipeline\': 400,\n            \'__main__.ProductPipeline\': 300,\n        },\n        \'CONCURRENT_REQUESTS_PER_DOMAIN\': 2\n    }\n\nI can then split up a pipeline (or even use multiple pipelines) by adding a value to the loader/returned item that identifies which part of the spider sent items over. This way I won’t get any KeyError exceptions and I know which items should be available. \n    ...\n    def scrape_stuff(self, response):\n        pageloader = PageLoader(\n                PageItem(), response=response)\n\n        pageloader.add_xpath(\'entire_page\', \'/html//text()\')\n        pageloader.add_value(\'item_type\', \'page\')\n        yield pageloader.load_item()\n\n        productloader = ProductLoader(\n                ProductItem(), response=response)\n\n        productloader.add_xpath(\'product_name\', \'//span[contains(text(), ""Example"")]\')\n        productloader.add_value(\'item_type\', \'product\')\n        yield productloader.load_item()\n\nclass PagePipeline:\n    def process_item(self, item, spider):\n        if item[\'item_type\'] == \'product\':\n            # do product stuff\n\n        if item[\'item_type\'] == \'page\':\n            # do page stuff\n\n', ""\nI am using two pipelines, one for image download (MyImagesPipeline) and second for save data in mongodb (MongoPipeline).\nsuppose we have many spiders(spider1,spider2,...........),in my example spider1 and spider5 can not use MyImagesPipeline\nsettings.py\nITEM_PIPELINES = {'scrapycrawler.pipelines.MyImagesPipeline' : 1,'scrapycrawler.pipelines.MongoPipeline' : 2}\nIMAGES_STORE = '/var/www/scrapycrawler/dowload'\n\nAnd bellow complete code of pipeline\nimport scrapy\nimport string\nimport pymongo\nfrom scrapy.pipelines.images import ImagesPipeline\n\nclass MyImagesPipeline(ImagesPipeline):\n    def process_item(self, item, spider):\n        if spider.name not in ['spider1', 'spider5']:\n            return super(ImagesPipeline, self).process_item(item, spider)\n        else:\n           return item \n\n    def file_path(self, request, response=None, info=None):\n        image_name = string.split(request.url, '/')[-1]\n        dir1 = image_name[0]\n        dir2 = image_name[1]\n        return dir1 + '/' + dir2 + '/' +image_name\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n    collection_url='snapdeal_urls'\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'scraping')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        #self.db[self.collection_name].insert(dict(item))\n        collection_name=item.get( 'collection_name', self.collection_name )\n        self.db[collection_name].insert(dict(item))\n        data = {}\n        data['base_id'] = item['base_id']\n        self.db[self.collection_url].update({\n            'base_id': item['base_id']\n        }, {\n            '$set': {\n            'image_download': 1\n            }\n        }, upsert=False, multi=True)\n        return item\n\n"", '\nwe can use some conditions in pipeline as this\n    # -*- coding: utf-8 -*-\nfrom scrapy_app.items import x\n\nclass SaveItemPipeline(object):\n    def process_item(self, item, spider):\n        if isinstance(item, x,):\n            item.save()\n        return item\n\n', ""\nSimple but still useful solution.\nSpider code\n    def parse(self, response):\n        item = {}\n        ... do parse stuff\n        item['info'] = {'spider': 'Spider2'}\n\npipeline code\n    def process_item(self, item, spider):\n        if item['info']['spider'] == 'Spider1':\n            logging.error('Spider1 pipeline works')\n        elif item['info']['spider'] == 'Spider2':\n            logging.error('Spider2 pipeline works')\n        elif item['info']['spider'] == 'Spider3':\n            logging.error('Spider3 pipeline works')\n\nHope this save some time for somebody!\n"", ""\nOverriding 'ITEM_PIPELINES' with custom settings per spider, as others have suggested, works well. However, I found I had a few distinct groups of pipelines I wanted to use for different categories of spiders. I wanted to be able to easily define the pipeline for a particular category of spider without a lot of thought, and I wanted to be able to update a pipeline category without editing each spider in that category individually.\nSo I created a new file called pipeline_definitions.py in the same directory as settings.py. pipeline_definitions.py contains functions like this:\ndef episode_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveEpisode': 100,\n    }\n\ndef show_pipelines():\n    return {\n        'radio_scrape.pipelines.SaveShow': 100,\n    }\n\nThen in each spider I would import the specific function relevant for the spider:\nfrom radio_scrape.pipeline_definitions import episode_pipelines\n\nI then use that function in the custom settings assignment:\nclass RadioStationAEspisodesSpider(scrapy.Spider):\n    name = 'radio_station_A_episodes'        \n    custom_settings = {\n        'ITEM_PIPELINES': episode_pipelines()\n    }\n\n""]",https://stackoverflow.com/questions/8372703/how-can-i-use-different-pipelines-for-different-spiders-in-a-single-scrapy-proje,web-crawler
getting Forbidden by robots.txt: scrapy,"
while crawling website like https://www.netflix.com, getting Forbidden by robots.txt: https://www.netflix.com/>
ERROR: No response downloaded for: https://www.netflix.com/
",53k,"
            69
        ","['\nIn the new version (scrapy 1.1) launched 2016-05-11 the crawl first downloads robots.txt before crawling. To change this behavior change in your settings.py with ROBOTSTXT_OBEY\nROBOTSTXT_OBEY = False\n\nHere are the release notes\n', ""\nNetflix's Terms of Use state:\n\nYou also agree not to circumvent, remove, alter, deactivate, degrade or thwart any of the content protections in the Netflix service; use any robot, spider, scraper or other automated means to access the Netflix service;\n\nThey have their robots.txt set up to block web scrapers. If you override the setting in settings.py to ROBOTSTXT_OBEY=False then you are violating their terms of use which can result in a law suit.\n"", '\nFirst thing you need to ensure is that you change your user agent in the request, otherwise default user agent will be blocked for sure.\n']",https://stackoverflow.com/questions/37274835/getting-forbidden-by-robots-txt-scrapy,web-crawler
Python: maximum recursion depth exceeded while calling a Python object,"
I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need.
after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url.
you can see the subtrahends quite fast (a little ex' of the few first ""good IDs"") -
510000011 # +8
510000029 # +18
510000037 # +8
510000045 # +8
510000052 # +7
510000060 # +8
510000078 # +18
510000086 # +8
510000094 # +8
510000102 # +8
510000110 # etc'
510000128
510000136
510000144
510000151
510000169
510000177
510000185
510000193
510000201

after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\18\17\8 (top returning subtrahends ) etc'.
this is the function - 
def checkNextID(ID):
    global numOfRuns, curRes, lastResult
    while ID < lastResult:
        try:
            numOfRuns += 1
            if numOfRuns % 10 == 0:
                time.sleep(3) # sleep every 10 iterations
            if isValid(ID + 8):
                parseHTML(curRes)
                checkNextID(ID + 8)
                return 0
            if isValid(ID + 18):
                parseHTML(curRes)
                checkNextID(ID + 18)
                return 0
            if isValid(ID + 7):
                parseHTML(curRes)
                checkNextID(ID + 7)
                return 0
            if isValid(ID + 17):
                parseHTML(curRes)
                checkNextID(ID + 17)
                return 0
            if isValid(ID+6):
                parseHTML(curRes)
                checkNextID(ID + 6)
                return 0
            if isValid(ID + 16):
                parseHTML(curRes)
                checkNextID(ID + 16)
                return 0
            else:
                checkNextID(ID + 1)
                return 0
        except Exception, e:
            print ""somethin went wrong: "" + str(e)

what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first ""if isValid"" clause (isValid(ID + 8) will return True).
lastResult is a variable which saves the last known url id, so we'll run until numOfRuns is
isValid() is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - 'curRes', it returns False if the url doesn't contain the data I need.
parseHTML is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True.
if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url.
you can see the rest of the code here
after running the code I got about 950~ good results and suddenly an exception had raised -

""somethin went wrong: maximum recursion depth exceeded while calling a
  Python object""

I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it.
I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes.
I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old ""algorithm"", it will never end).
Thanks!
",253k,"
            65
        ","[""\nPython don't have a great support for recursion because of it's lack of TRE (Tail Recursion Elimination).\nThis means that each call to your recursive function will create a function call stack and because there is a limit of stack depth (by default is 1000) that you can check out by sys.getrecursionlimit (of course you can change it using sys.setrecursionlimit but it's not recommended) your program will end up by crashing when it hits this limit.\nAs other answer has already give you a much nicer way for how to solve this in your case (which is to replace recursion by simple loop) there is another solution if you still want to use recursion which is to use one of the many recipes of implementing TRE in python like this one.\nN.B: My answer is meant to give you more insight on why you get the error, and I'm not advising you to use the TRE as i already explained because in your case a loop will be much better and easy to read.\n"", '\nYou can increase the capacity of the stack by the following :\nimport sys\nsys.setrecursionlimit(10000)\n\n', '\nthis turns the recursion in to a loop:\ndef checkNextID(ID):\n    global numOfRuns, curRes, lastResult\n    while ID < lastResult:\n        try:\n            numOfRuns += 1\n            if numOfRuns % 10 == 0:\n                time.sleep(3) # sleep every 10 iterations\n            if isValid(ID + 8):\n                parseHTML(curRes)\n                ID = ID + 8\n            elif isValid(ID + 18):\n                parseHTML(curRes)\n                ID = ID + 18\n            elif isValid(ID + 7):\n                parseHTML(curRes)\n                ID = ID + 7\n            elif isValid(ID + 17):\n                parseHTML(curRes)\n                ID = ID + 17\n            elif isValid(ID+6):\n                parseHTML(curRes)\n                ID = ID + 6\n            elif isValid(ID + 16):\n                parseHTML(curRes)\n                ID = ID + 16\n            else:\n                ID = ID + 1\n        except Exception, e:\n            print ""somethin went wrong: "" + str(e)\n\n', '\nYou can increase the recursion depth and thread stack size.\nimport sys, threading\nsys.setrecursionlimit(10**7) # max depth of recursion\nthreading.stack_size(2**27)  # new thread will get stack of such size\n\n', '\nInstead of doing recursion, the parts of the code with checkNextID(ID + 18) and similar could be replaced with ID+=18, and then if you remove all instances of return 0, then it should do the same thing but as a simple loop. You should then put a return 0 at the end and make your variables non-global.\n', ""\n\nuse try and except but don't print your error in except just run your function again in except statement\n\n""]",https://stackoverflow.com/questions/6809402/python-maximum-recursion-depth-exceeded-while-calling-a-python-object,web-crawler
Detecting honest web crawlers,"
I would like to detect (on the server side) which requests are from bots.  I don't care about malicious bots at this point, just the ones that are playing nice.  I've seen a few approaches that mostly involve matching the user agent string against keywords like 'bot'.  But that seems awkward, incomplete, and unmaintainable.  So does anyone have any more solid approaches?  If not, do you have any resources you use to keep up to date with all the friendly user agents?
If you're curious: I'm not trying to do anything against any search engine policy.  We have a section of the site where a user is randomly presented with one of several slightly different versions of a page.  However if a web crawler is detected, we'd always give them the same version so that the index is consistent.
Also I'm using Java, but I would imagine the approach would be similar for any server-side technology.
",20k,"
            46
        ","['\nYou said matching the user agent on ‘bot’ may be awkward, but we’ve found it to be a pretty good match. Our studies have shown that it will cover about 98% of the hits you receive. We also haven’t come across any false positive matches yet either. If you want to raise this up to 99.9% you can include a few other well-known matches such as ‘crawler’, ‘baiduspider’, ‘ia_archiver’, ‘curl’ etc. We’ve tested this on our production systems over millions of hits. \nHere are a few c# solutions for you:\n1) Simplest\nIs the fastest when processing a miss. i.e. traffic from a non-bot – a normal user.\nCatches 99+% of crawlers.\nbool iscrawler = Regex.IsMatch(Request.UserAgent, @""bot|crawler|baiduspider|80legs|ia_archiver|voyager|curl|wget|yahoo! slurp|mediapartners-google"", RegexOptions.IgnoreCase);\n\n2) Medium\nIs the fastest when processing a hit. i.e. traffic from a bot. Pretty fast for misses too.\nCatches close to 100% of crawlers.\nMatches ‘bot’, ‘crawler’, ‘spider’ upfront. \nYou can add to it any other known crawlers.\nList<string> Crawlers3 = new List<string>()\n{\n    ""bot"",""crawler"",""spider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",\n    ""lwp-trivial"",""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",            \n    ""atn_worldwide"",""atomz"",""bjaaland"",""ukonline"",""calif"",""combine"",""cosmos"",""cusco"",\n    ""cyberspyder"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",\n    ""esther"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""havindex"",""hotwired"",\n    ""htdig"",""ingrid"",""informant"",""inspectorwww"",""iron33"",""teoma"",""ask jeeves"",""jeeves"",\n    ""image.kapsi.net"",""kdd-explorer"",""label-grabber"",""larbin"",""linkidator"",""linkwalker"",\n    ""lockon"",""marvin"",""mattie"",""mediafox"",""merzscope"",""nec-meshexplorer"",""udmsearch"",""moget"",\n    ""motor"",""muncher"",""muninn"",""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",\n    ""netscoop"",""newscan-online"",""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",\n    ""patric"",""pegasus"",""phpdig"",""piltdownman"",""pimptrain"",""plumtreewebaccessor"",""getterrobo-plus"",\n    ""raven"",""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",\n    ""searchprocess"",""senrigan"",""shagseeker"",""site valet"",""skymob"",""slurp"",""snooper"",""speedy"",\n    ""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",""titin"",""topiclink"",""udmsearch"",\n    ""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",""webscout"",""voyager"",""crawlpaper"",\n    ""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",""webmoose"",""pagesinventory"",""webquest"",""webreaper"",\n    ""webwalker"",""winona"",""occam"",""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""fido"",""wlm"",\n    ""wolp"",""wwwc"",""xget"",""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\nstring ua = Request.UserAgent.ToLower();\nbool iscrawler = Crawlers3.Exists(x => ua.Contains(x));\n\n3) Paranoid\nIs pretty fast, but a little slower than options 1 and 2.\nIt’s the most accurate, and allows you to maintain the lists if you want.\nYou can maintain a separate list of names with ‘bot’ in them if you are afraid of false positives in future.\nIf we get a short match we log it and check it for a false positive.\n// crawlers that have \'bot\' in their useragent\nList<string> Crawlers1 = new List<string>()\n{\n    ""googlebot"",""bingbot"",""yandexbot"",""ahrefsbot"",""msnbot"",""linkedinbot"",""exabot"",""compspybot"",\n    ""yesupbot"",""paperlibot"",""tweetmemebot"",""semrushbot"",""gigabot"",""voilabot"",""adsbot-google"",\n    ""botlink"",""alkalinebot"",""araybot"",""undrip bot"",""borg-bot"",""boxseabot"",""yodaobot"",""admedia bot"",\n    ""ezooms.bot"",""confuzzledbot"",""coolbot"",""internet cruiser robot"",""yolinkbot"",""diibot"",""musobot"",\n    ""dragonbot"",""elfinbot"",""wikiobot"",""twitterbot"",""contextad bot"",""hambot"",""iajabot"",""news bot"",\n    ""irobot"",""socialradarbot"",""ko_yappo_robot"",""skimbot"",""psbot"",""rixbot"",""seznambot"",""careerbot"",\n    ""simbot"",""solbot"",""mail.ru_bot"",""spiderbot"",""blekkobot"",""bitlybot"",""techbot"",""void-bot"",\n    ""vwbot_k"",""diffbot"",""friendfeedbot"",""archive.org_bot"",""woriobot"",""crystalsemanticsbot"",""wepbot"",\n    ""spbot"",""tweetedtimes bot"",""mj12bot"",""who.is bot"",""psbot"",""robot"",""jbot"",""bbot"",""bot""\n};\n\n// crawlers that don\'t have \'bot\' in their useragent\nList<string> Crawlers2 = new List<string>()\n{\n    ""baiduspider"",""80legs"",""baidu"",""yahoo! slurp"",""ia_archiver"",""mediapartners-google"",""lwp-trivial"",\n    ""nederland.zoek"",""ahoy"",""anthill"",""appie"",""arale"",""araneo"",""ariadne"",""atn_worldwide"",""atomz"",\n    ""bjaaland"",""ukonline"",""bspider"",""calif"",""christcrawler"",""combine"",""cosmos"",""cusco"",""cyberspyder"",\n    ""cydralspider"",""digger"",""grabber"",""downloadexpress"",""ecollector"",""ebiness"",""esculapio"",""esther"",\n    ""fastcrawler"",""felix ide"",""hamahakki"",""kit-fireball"",""fouineur"",""freecrawl"",""desertrealm"",\n    ""gammaspider"",""gcreep"",""golem"",""griffon"",""gromit"",""gulliver"",""gulper"",""whowhere"",""portalbspider"",\n    ""havindex"",""hotwired"",""htdig"",""ingrid"",""informant"",""infospiders"",""inspectorwww"",""iron33"",\n    ""jcrawler"",""teoma"",""ask jeeves"",""jeeves"",""image.kapsi.net"",""kdd-explorer"",""label-grabber"",\n    ""larbin"",""linkidator"",""linkwalker"",""lockon"",""logo_gif_crawler"",""marvin"",""mattie"",""mediafox"",\n    ""merzscope"",""nec-meshexplorer"",""mindcrawler"",""udmsearch"",""moget"",""motor"",""muncher"",""muninn"",\n    ""muscatferret"",""mwdsearch"",""sharp-info-agent"",""webmechanic"",""netscoop"",""newscan-online"",\n    ""objectssearch"",""orbsearch"",""packrat"",""pageboy"",""parasite"",""patric"",""pegasus"",""perlcrawler"",\n    ""phpdig"",""piltdownman"",""pimptrain"",""pjspider"",""plumtreewebaccessor"",""getterrobo-plus"",""raven"",\n    ""roadrunner"",""robbie"",""robocrawl"",""robofox"",""webbandit"",""scooter"",""search-au"",""searchprocess"",\n    ""senrigan"",""shagseeker"",""site valet"",""skymob"",""slcrawler"",""slurp"",""snooper"",""speedy"",\n    ""spider_monkey"",""spiderline"",""curl_image_client"",""suke"",""www.sygol.com"",""tach_bw"",""templeton"",\n    ""titin"",""topiclink"",""udmsearch"",""urlck"",""valkyrie libwww-perl"",""verticrawl"",""victoria"",\n    ""webscout"",""voyager"",""crawlpaper"",""wapspider"",""webcatcher"",""t-h-u-n-d-e-r-s-t-o-n-e"",\n    ""webmoose"",""pagesinventory"",""webquest"",""webreaper"",""webspider"",""webwalker"",""winona"",""occam"",\n    ""robi"",""fdse"",""jobo"",""rhcs"",""gazz"",""dwcp"",""yeti"",""crawler"",""fido"",""wlm"",""wolp"",""wwwc"",""xget"",\n    ""legs"",""curl"",""webs"",""wget"",""sift"",""cmc""\n};\n\nstring ua = Request.UserAgent.ToLower();\nstring match = null;\n\nif (ua.Contains(""bot"")) match = Crawlers1.FirstOrDefault(x => ua.Contains(x));\nelse match = Crawlers2.FirstOrDefault(x => ua.Contains(x));\n\nif (match != null && match.Length < 5) Log(""Possible new crawler found: "", ua);\n\nbool iscrawler = match != null;\n\nNotes:\n\nIt’s tempting to just keep adding names to the regex option 1. But if you do this it will become slower. If you want a more complete list then linq with lambda is faster.\nMake sure .ToLower() is outside of your linq method – remember the method is a loop and you would be modifying the string during each iteration.\nAlways put the heaviest bots at the start of the list, so they match sooner.\nPut the lists into a static class so that they are not rebuilt on every pageview.\n\nHoneypots\nThe only real alternative to this is to create a ‘honeypot’ link on your site that only a bot will reach. You then log the user agent strings that hit the honeypot page to a database. You can then use those logged strings to classify crawlers.\nPostives: It will match some unknown crawlers that aren’t declaring themselves.\nNegatives: Not all crawlers dig deep enough to hit every link on your site, and so they may not reach your honeypot.\n', '\nYou can find a very thorough database of data on known ""good"" web crawlers in the robotstxt.org Robots Database.  Utilizing this data would be far more effective than just matching bot in the user-agent.\n', '\nOne suggestion is to create an empty anchor on your page that only a bot would follow.  Normal users wouldn\'t see the link, leaving spiders and bots to follow.  For example, an empty anchor tag that points to a subfolder would record a get request in your logs...\n<a href=""dontfollowme.aspx""></a>\n\nMany people use this method while running a HoneyPot to catch malicious bots that aren\'t following the robots.txt file.  I use the empty anchor method in an ASP.NET honeypot solution I wrote to trap and block those creepy crawlers...\n', '\nAny visitor whose entry page is /robots.txt is probably a bot.\n', '\nSomething quick and dirty like this might be a good start:\nreturn if request.user_agent =~ /googlebot|msnbot|baidu|curl|wget|Mediapartners-Google|slurp|ia_archiver|Gigabot|libwww-perl|lwp-trivial/i\n\nNote: rails code, but regex is generally applicable.\n', ""\nI'm pretty sure a large proportion of bots don't use robots.txt, however that was my first thought.\nIt seems to me that the best way to detect a bot is with time between requests, if the time between requests is consistently fast then its a bot.\n"", '\nvoid CheckBrowserCaps()\n    {\n        String labelText = """";\n        System.Web.HttpBrowserCapabilities myBrowserCaps = Request.Browser;\n        if (((System.Web.Configuration.HttpCapabilitiesBase)myBrowserCaps).Crawler)\n        {\n            labelText = ""Browser is a search engine."";\n        }\n        else\n        {\n            labelText = ""Browser is not a search engine."";\n        }\n\n        Label1.Text = labelText;\n    }\n\nHttpCapabilitiesBase.Crawler Property\n']",https://stackoverflow.com/questions/544450/detecting-honest-web-crawlers,web-crawler
How to programmatically fill input elements built with React?,"
I'm tasked with crawling website built with React. I'm trying to fill in input fields and submitting the form using javascript injects to the page (either selenium or webview in mobile). This works like a charm on every other site + technology but React seems to be a real pain.
so here is a sample code 
var email = document.getElementById( 'email' );
email.value = 'example@mail.com';

I the value changes on the DOM input element, but the React does not trigger the change event.
I've been trying plethora of different ways to get the React to update the state.
var event = new Event('change', { bubbles: true });
email.dispatchEvent( event );

no avail
var event = new Event('input', { bubbles: true });
email.dispatchEvent( event );

not working
email.onChange( event );

not working
I cannot believe interacting with React has been made so difficult. I would greatly appreciate any help. 
Thank you
",24k,"
            37
        ","[""\nThis accepted solution appears not to work in React > 15.6 (including React 16) as a result of changes to de-dupe input and change events.\nYou can see the React discussion here: https://github.com/facebook/react/issues/10135\nAnd the suggested workaround here:\nhttps://github.com/facebook/react/issues/10135#issuecomment-314441175\nReproduced here for convenience:\nInstead of\ninput.value = 'foo';\ninput.dispatchEvent(new Event('input', {bubbles: true}));\n\nYou would use\nfunction setNativeValue(element, value) {\n  const valueSetter = Object.getOwnPropertyDescriptor(element, 'value').set;\n  const prototype = Object.getPrototypeOf(element);\n  const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value').set;\n\n  if (valueSetter && valueSetter !== prototypeValueSetter) {\n    prototypeValueSetter.call(element, value);\n  } else {\n    valueSetter.call(element, value);\n  }\n}\n\nand then\nsetNativeValue(input, 'foo');\ninput.dispatchEvent(new Event('input', { bubbles: true }));\n\n"", '\nReact is listening for the input event of text fields.\nYou can change the value and manually trigger an input event, and react\'s onChange handler will trigger:\n\n\nclass Form extends React.Component {\r\n  constructor(props) {\r\n    super(props)\r\n    this.state = {value: \'\'}\r\n  }\r\n  \r\n  handleChange(e) {\r\n    this.setState({value: e.target.value})\r\n    console.log(\'State updated to \', e.target.value);\r\n  }\r\n  \r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          id=\'textfield\'\r\n          value={this.state.value}\r\n          onChange={this.handleChange.bind(this)}\r\n        />\r\n        <p>{this.state.value}</p>\r\n      </div>      \r\n    )\r\n  }\r\n}\r\n\r\nReactDOM.render(\r\n  <Form />,\r\n  document.getElementById(\'app\')\r\n)\r\n\r\ndocument.getElementById(\'textfield\').value = \'foo\'\r\nconst event = new Event(\'input\', { bubbles: true })\r\ndocument.getElementById(\'textfield\').dispatchEvent(event)\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js""></script>\r\n<script src=""https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js""></script>\r\n\r\n<div id=\'app\'></div>\n\n\n', ""\nHere is the cleanest possible solution for inputs, selects, checkboxes, etc. (works not only for react inputs)\n/**\n * See [Modify React Component's State using jQuery/Plain Javascript from Chrome Extension](https://stackoverflow.com/q/41166005)\n * See https://github.com/facebook/react/issues/11488#issuecomment-347775628\n * See [How to programmatically fill input elements built with React?](https://stackoverflow.com/q/40894637)\n * See https://github.com/facebook/react/issues/10135#issuecomment-401496776\n *\n * @param {HTMLInputElement | HTMLSelectElement} el\n * @param {string} value\n */\nfunction setNativeValue(el, value) {\n  const previousValue = el.value;\n\n  if (el.type === 'checkbox' || el.type === 'radio') {\n    if ((!!value && !el.checked) || (!!!value && el.checked)) {\n      el.click();\n    }\n  } else el.value = value;\n\n  const tracker = el._valueTracker;\n  if (tracker) {\n    tracker.setValue(previousValue);\n  }\n\n  // 'change' instead of 'input', see https://github.com/facebook/react/issues/11488#issuecomment-381590324\n  el.dispatchEvent(new Event('change', { bubbles: true }));\n}\n\nUsage:\nsetNativeValue(document.getElementById('name'), 'Your name');\ndocument.getElementById('radio').click(); // or setNativeValue(document.getElementById('radio'), true)\ndocument.getElementById('checkbox').click(); // or setNativeValue(document.getElementById('checkbox'), true)\n\n"", '\nI noticed the input element had some property with a name along the lines of __reactEventHandlers$..., which had some functions including an onChange.\nThis worked for finding that function and triggering it\nlet getReactEventHandlers = (element) => {\n    // the name of the attribute changes, so we find it using a match.\n    // It\'s something like `element.__reactEventHandlers$...`\n    let reactEventHandlersName = Object.keys(element)\n       .filter(key => key.match(\'reactEventHandler\'));\n    return element[reactEventHandlersName];\n}\n\nlet triggerReactOnChangeEvent = (element) => {\n    let ev = new Event(\'change\');\n    // workaround to set the event target, because `ev.target = element` doesn\'t work\n    Object.defineProperty(ev, \'target\', {writable: false, value: element});\n    getReactEventHandlers(element).onChange(ev);\n}\n\ninput.value = ""some value"";\ntriggerReactOnChangeEvent(input);\n\n', '\nWithout element ids:\nexport default function SomeComponent() {\n    const inputRef = useRef();\n    const [address, setAddress] = useState("""");\n    const onAddressChange = (e) => {\n        setAddress(e.target.value);\n    }\n    const setAddressProgrammatically = (newValue) => {\n        const event = new Event(\'change\', { bubbles: true });\n        const input = inputRef.current;\n        if (input) {\n            setAddress(newValue);\n            input.value = newValue;\n            input.dispatchEvent(event);\n        }\n    }\n    return (\n        ...\n        <input ref={inputRef} type=""text"" value={address} onChange={onAddressChange}/>\n        ...\n    );\n}\n\n', '\nReact 17 works with fibers:\nfunction findReact(dom) {\n    let key = Object.keys(dom).find(key => key.startsWith(""__reactFiber$""));\n    let internalInstance = dom[key];\n    if (internalInstance == null) return ""internalInstance is null: "" + key;\n\n    if (internalInstance.return) { // react 16+\n        return internalInstance._debugOwner\n            ? internalInstance._debugOwner.stateNode\n           : internalInstance.return.stateNode;\n    } else { // react <16\n        return internalInstance._currentElement._owner._instance;\n   }\n}\n\nthen:\nfindReact(domElement).onChangeWrapper(""New value"");\n\nthe domElement in this is the tr with the data-param-name of the field you are trying to change:\nvar domElement = ?.querySelectorAll(\'tr[data-param-name=""<my field name>""]\')\n\n']",https://stackoverflow.com/questions/40894637/how-to-programmatically-fill-input-elements-built-with-react,web-crawler
How to identify web-crawler?,"
How can I filter out hits from webcrawlers etc. Hits which not is human..
I use maxmind.com to request the city from the IP.. It is not quite cheap if I have to pay for ALL hits including webcrawlers, robots etc.
",27k,"
            36
        ","['\nThere are two general ways to detect robots and I would call them ""Polite/Passive"" and ""Aggressive"". Basically, you have to give your web site a psychological disorder.\nPolite\nThese are ways to politely tell crawlers that they shouldn\'t crawl your site and to limit how often you are crawled. Politeness is ensured through robots.txt file in which you specify which bots, if any, should be allowed to crawl your website and how often your website can be crawled. This assumes that the robot you\'re dealing with is polite.\nAggressive\nAnother way to keep bots off your site is to get aggressive. \nUser Agent\nSome aggressive behavior includes (as previously mentioned by other users) the filtering of user-agent strings. This is probably the simplest, but also the least reliable way to detect if it\'s a user or not. A lot of bots tend to spoof user agents and some do it for legitimate reasons (i.e. they only want to crawl mobile content), while others simply don\'t want to be identified as bots. Even worse, some bots spoof legitimate/polite bot agents, such as the user agents of google, microsoft, lycos and other crawlers which are generally considered polite. Relying on the user agent can be helpful, but not by itself.\nThere are more aggressive ways to deal with robots that spoof user agents AND don\'t abide by your robots.txt file:\nBot Trap\nI like to think of this as a ""Venus Fly Trap,"" and it basically punishes any bot that wants to play tricks with you. \nA bot trap is probably the most effective way to find bots that don\'t adhere to your robots.txt file without actually impairing the usability of your website. Creating a bot trap ensures that only bots are captured and not real users. The basic way to do it is to setup a directory which you specifically mark as off limits in your robots.txt file, so any robot that is polite will not fall into the trap. The second thing you do is to place a ""hidden"" link from your website to the bot trap directory (this ensures that real users will never go there, since real users never click on invisible links). Finally, you ban any IP address that goes to the bot trap directory. \nHere are some instructions on how to achieve this:\nCreate a bot trap (or in your case: a PHP bot trap).\nNote: of course, some bots are smart enough to read your robots.txt file, see all the directories which you\'ve marked as ""off limits"" and STILL ignore your politeness settings (such as crawl rate and allowed bots). Those bots will probably not fall into your bot trap despite the fact that they are not polite.\nViolent\nI think this is actually too aggressive for the general audience (and general use), so if there are any kids under the age of 18, then please take them to another room!\nYou can make the bot trap ""violent"" by simply not specifying a robots.txt file. In this situation ANY BOT that crawls the hidden links will probably end up in the bot trap and you can ban all bots, period! \nThe reason this is not recommended is that you may actually want some bots to crawl your website (such as Google, Microsoft or other bots for site indexing). Allowing your website to be politely crawled by the bots from Google, Microsoft, Lycos, etc. will ensure that your site gets indexed and it shows up when people search for it on their favorite search engine.\nSelf Destructive\nYet another way to limits what bots can crawl on your website, is to serve CAPTCHAs or other challenges which a bot cannot solve. This comes at an expense of your users and I would think that anything which makes your website less usable (such as a CAPTCHA) is ""self destructive."" This, of course, will not actually block the bot from repeatedly trying to crawl your website, it will simply make your website very uninteresting to them. There are ways to ""get around"" the CAPTCHAs, but they\'re difficult to implement so I\'m not going to delve into this too much.\nConclusion\nFor your purposes, probably the best way to deal with bots is to employ a combination of the above mentioned strategies:\n\nFilter user agents.\nSetup a bot trap (the violent one).\n\nCatch all the bots that go into the violent bot trap and simply black-list their IPs (but don\'t block them). This way you will still get the ""benefits"" of being crawled by bots, but you will not have to pay to check the IP addresses that are black-listed due to going to your bot trap.\n', ""\nYou can check USER_AGENT, something like:\nfunction crawlerDetect($USER_AGENT)\n{\n    $crawlers = array(\n    array('Google', 'Google'),\n    array('msnbot', 'MSN'),\n    array('Rambler', 'Rambler'),\n    array('Yahoo', 'Yahoo'),\n    array('AbachoBOT', 'AbachoBOT'),\n    array('accoona', 'Accoona'),\n    array('AcoiRobot', 'AcoiRobot'),\n    array('ASPSeek', 'ASPSeek'),\n    array('CrocCrawler', 'CrocCrawler'),\n    array('Dumbot', 'Dumbot'),\n    array('FAST-WebCrawler', 'FAST-WebCrawler'),\n    array('GeonaBot', 'GeonaBot'),\n    array('Gigabot', 'Gigabot'),\n    array('Lycos', 'Lycos spider'),\n    array('MSRBOT', 'MSRBOT'),\n    array('Scooter', 'Altavista robot'),\n    array('AltaVista', 'Altavista robot'),\n    array('IDBot', 'ID-Search Bot'),\n    array('eStyle', 'eStyle Bot'),\n    array('Scrubby', 'Scrubby robot')\n    );\n\n    foreach ($crawlers as $c)\n    {\n        if (stristr($USER_AGENT, $c[0]))\n        {\n            return($c[1]);\n        }\n    }\n\n    return false;\n}\n\n// example\n\n$crawler = crawlerDetect($_SERVER['HTTP_USER_AGENT']);\n\n"", ""\nThe user agent ($_SERVER['HTTP_USER_AGENT']) often identifies whether the connecting agent is a browser or a robot. Review logs/analytics for the user agents of crawlers that visit your site. Filter accordingly.\nTake note that the user agent is a header supplied by the client application. As such it can be pretty much anything and shouldn't be trusted 100%. Plan accordingly.\n"", ""\nChecking the User-Agent will protect you from legitimate bots like Google and Yahoo.\nHowever, if you're also being hit with spam bots, then chances are User-Agent comparison won't protect you since those bots typically forge a common User-Agent string anyway.  In that instance, you would need to imploy more sophisticated measures.  If user input is required, a simple image verification scheme like ReCaptcha will work.\nIf you're looking to filter out all page hits from a bot, unfortunately, there's no 100% reliable way to do this if the bot is forging its credentials.  This is just an annoying fact of life on the internet that web admins have to put up with.\n"", ""\nI found this package, it's actively being developed and I'm quite liking it so far:\nhttps://github.com/JayBizzle/Crawler-Detect\nIt's simple as this:\nuse Jaybizzle\\CrawlerDetect\\CrawlerDetect;\n\n$CrawlerDetect = new CrawlerDetect;\n\n// Check the user agent of the current 'visitor'\nif($CrawlerDetect->isCrawler()) {\n    // true if crawler user agent detected\n}\n\n// Pass a user agent as a string\nif($CrawlerDetect->isCrawler('Mozilla/5.0 (compatible; Sosospider/2.0; +http://help.soso.com/webspider.htm)')) {\n    // true if crawler user agent detected\n}\n\n// Output the name of the bot that matched (if any)\necho $CrawlerDetect->getMatches();\n\n"", '\nuseragentstring.com is serving a lilst that you can use to analyze the userstring:\n$api_request=""http://www.useragentstring.com/?uas="".urlencode($_SERVER[\'HTTP_USER_AGENT\']).""&getJSON=all"";\n$ua=json_decode(file_get_contents($api_request), true);\nif($ua[""agent_type""]==""Crawler"") die();\n\n']",https://stackoverflow.com/questions/8404775/how-to-identify-web-crawler,web-crawler
Send Post Request in Scrapy,"
I am trying to crawl the latest reviews from google play store and to get that I need to make a post request.
With the Postman, it works and I get desired response.

but a post request in terminal gives me a server error
For ex: this page https://play.google.com/store/apps/details?id=com.supercell.boombeach
curl -H ""Content-Type: application/json"" -X POST -d '{""id"": ""com.supercell.boombeach"", ""reviewType"": '0', ""reviewSortOrder"": '0', ""pageNum"":'0'}' https://play.google.com/store/getreviews

gives a server error and
Scrapy just ignores this line:
frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": 0, ""reviewSortOrder"": 0, ""pageNum"":0}
        url = ""https://play.google.com/store/getreviews""
        yield Request(url, callback=self.parse, method=""POST"", body=urllib.urlencode(frmdata))

",63k,"
            31
        ","[""\nThe answer above do not really solved the problem. They are sending the data as paramters instead of JSON data as the body of the request.\nFrom http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json:\nmy_data = {'field1': 'value1', 'field2': 'value2'}\nrequest = scrapy.Request( url, method='POST', \n                          body=json.dumps(my_data), \n                          headers={'Content-Type':'application/json'} )\n\n"", '\nMake sure that each element in your formdata is of type string/unicode\nfrmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\nurl = ""https://play.google.com/store/getreviews""\nyield FormRequest(url, callback=self.parse, formdata=frmdata)\n\nI think this will do\nIn [1]: from scrapy.http import FormRequest\n\nIn [2]: frmdata = {""id"": ""com.supercell.boombeach"", ""reviewType"": \'0\', ""reviewSortOrder"": \'0\', ""pageNum"":\'0\'}\n\nIn [3]: url = ""https://play.google.com/store/getreviews""\n\nIn [4]: r = FormRequest(url, formdata=frmdata)\n\nIn [5]: fetch(r)\n 2015-05-20 14:40:09+0530 [default] DEBUG: Crawled (200) <POST      https://play.google.com/store/getreviews> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3ea4258890>\n[s]   item       {}\n[s]   r          <POST https://play.google.com/store/getreviews>\n[s]   request    <POST https://play.google.com/store/getreviews>\n[s]   response   <200 https://play.google.com/store/getreviews>\n[s]   settings   <scrapy.settings.Settings object at 0x7f3eaa205450>\n[s]   spider     <Spider \'default\' at 0x7f3ea3449cd0>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n', '\nSample Page Traversing using Post in Scrapy:\ndef directory_page(self,response):\n    if response:\n        profiles = response.xpath(""//div[@class=\'heading-h\']/h3/a/@href"").extract()\n        for profile in profiles:\n            yield Request(urljoin(response.url,profile),callback=self.profile_collector)\n\n        page = response.meta[\'page\'] + 1\n        if page :\n            yield FormRequest(\'https://rotmanconnect.com/AlumniDirectory/getmorerecentjoineduser\',\n                                        formdata={\'isSortByName\':\'false\',\'pageNumber\':str(page)},\n                                        callback= self.directory_page,\n                                        meta={\'page\':page})\n    else:\n         print ""No more page available""\n\n']",https://stackoverflow.com/questions/30342243/send-post-request-in-scrapy,web-crawler
how do web crawlers handle javascript,"
Today a lot of content on Internet is generated using JavaScript (specifically by background AJAX calls). I was wondering how web crawlers like Google handle them. Are they aware of JavaScript? Do they have a built-in JavaScript engine? Or do they simple ignore all JavaScript generated content in the page (I guess quite unlikely). Do people use specific techniques for getting their content indexed which would otherwise be available through background AJAX requests to a normal Internet user? 
",11k,"
            22
        ","[""\nJavaScript is handled by both Bing and Google crawlers. Yahoo uses the Bing crawler data, so it should be handled as well. I didn't look into other search engines, so if you care about them, you should look them up.\nBing published guidance in March 2014 as to how to create JavaScript-based websites that work with their crawler (mostly related to pushState) that are good practices in general:\n\nAvoid creating broken links with pushState\nAvoid creating two different links that link to the same content with pushState\nAvoid cloaking. (Here's an article Bing published about their cloaking detection in 2007)\nSupport browsers (and crawlers) that can't handle pushState.\n\nGoogle later published guidance in May 2014 as to how to create JavaScript-based websites that work with their crawler, and their recommendations are also recommended:\n\nDon't block the JavaScript (and CSS) in the robots.txt file.\nMake sure you can handle the load of the crawlers.\nIt's a good idea to support browsers and crawlers that can't handle (or users and organizations that won't allow) JavaScript\nTricky JavaScript that relies on arcane or specific features of the language might not work with the crawlers.\nIf your JavaScript removes content from the page, it might not get indexed.\naround.\n\n"", ""\nMost of them don't handle Javascript in any way. (At least, all the major search engines' crawlers don't.)\nThis is why it's still important to have your site gracefully handle navigation without Javascript.\n"", ""\nI have tested this by putting pages on my site only reachable by Javascript and then observing their presence in search indexes.\nPages on my site which were reachable only by Javascript were subsequently indexed by Google.\nThe content was reached through Javascript with a 'classic' technique or constructing a URL and setting the window.location accordingly.\n"", ""\nPrecisely what Ben S said. And anyone accessing your site with Lynx won't execute JavaScript either. If your site is intended for general public use, it should generally be usable without JavaScript.\nAlso, related: if there are pages that you would want a search engine to find, and which would normally arise only from JavaScript, you might consider generating static versions of them, reachable by a crawlable site map, where these static pages use JavaScript to load the current version when hit by a JavaScript-enabled browser (in case a human with a browser follows your site map). The search engine will see the static form of the page, and can index it.\n"", '\nCrawlers doesn\'t parse Javascript to find out what it does.\nThey may be built to recognise some classic snippets like  onchange=""window.location.href=this.options[this.selectedIndex].value;"" or onclick=""window.location.href=\'blah.html\';"", but they don\'t bother with things like content fetched using AJAX. At least not yet, and content fetched like that will always be secondary anyway.\nSo, Javascript should be used only for additional functionality. The main content taht you want the crawlers to find should still be plain text in the page and regular links that the crawlers easily can follow.\n', ""\ncrawlers can handle javascript or ajax calls if they are using some kind of frameworks like 'htmlunit' or 'selenium'\n""]",https://stackoverflow.com/questions/1785083/how-do-web-crawlers-handle-javascript,web-crawler
HTTPWebResponse + StreamReader Very Slow,"
I'm trying to implement a limited web crawler in C# (for a few hundred sites only)
using HttpWebResponse.GetResponse() and Streamreader.ReadToEnd() , also tried using StreamReader.Read() and a loop to build my HTML string.
I'm only downloading pages which are about 5-10K. 
It's all very slow! For example, the average GetResponse() time is about half a second, while the average StreamREader.ReadToEnd() time is about 5 seconds!
All sites should be very fast, as they are very close to my location, and have fast servers. (in Explorer takes practically nothing to D/L) and I am not using any proxy.
My Crawler has about 20 threads reading simultaneously from the same site. Could this be causing a problem?
How do I reduce StreamReader.ReadToEnd times DRASTICALLY?
",24k,"
            21
        ","['\nHttpWebRequest may be taking a while to detect your proxy settings. Try adding this to your application config:\n<system.net>\n  <defaultProxy enabled=""false"">\n    <proxy/>\n    <bypasslist/>\n    <module/>\n  </defaultProxy>\n</system.net>\n\nYou might also see a slight performance gain from buffering your reads to reduce the number of calls made to the underlying operating system socket:\nusing (BufferedStream buffer = new BufferedStream(stream))\n{\n  using (StreamReader reader = new StreamReader(buffer))\n  {\n    pageContent = reader.ReadToEnd();\n  }\n}\n\n', '\nWebClient\'s DownloadString is a simple wrapper for HttpWebRequest, could you try using that temporarily and see if the speed improves? If things get much faster, could you share your code so we can have a look at what may be wrong with it?\nEDIT:\nIt seems HttpWebRequest observes IE\'s \'max concurrent connections\' setting, are these URLs on the same domain? You could try increasing the connections limit to see if that helps? I found this article about the problem:\n\nBy default, you can\'t perform more\n  than 2-3 async HttpWebRequest (depends\n  on the OS). In order to override it\n  (the easiest way, IMHO) don\'t forget\n  to add this under \n  section in the application\'s config\n  file:\n\n<system.net>\n  <connectionManagement>\n     <add address=""*"" maxconnection=""65000"" />\n  </connectionManagement>\n</system.net>\n\n', ""\nI had the same problem, but when I sat the HttpWebRequest's Proxy parameter to null, it solved the problem.\nUriBuilder ub = new UriBuilder(url);\nHttpWebRequest request = (HttpWebRequest)WebRequest.Create( ub.Uri );\nrequest.Proxy = null;\nHttpWebResponse response = (HttpWebResponse)request.GetResponse();\n\n"", '\nHave you tried ServicePointManager.maxConnections?  I usually set it to 200 for things similar to this.\n', '\nI had problem the same problem but worst.\nresponse = (HttpWebResponse)webRequest.GetResponse(); in my code\ndelayed about 10 seconds before running more code and after this the download saturated my connection.\nkurt\'s answer defaultProxy enabled=""false"" \nsolved the problem. now the response is almost instantly and i can download any http file at my connections maximum speed :)\nsorry for bad english\n', '\nI found the Application Config method did not work, but the problem was still due to the proxy settings.  My simple request used to take up to 30 seconds, now it takes 1.\npublic string GetWebData()\n{\n            string DestAddr = ""http://mydestination.com"";\n            System.Net.WebClient myWebClient = new System.Net.WebClient();\n            WebProxy myProxy = new WebProxy();\n            myProxy.IsBypassed(new Uri(DestAddr));\n            myWebClient.Proxy = myProxy;\n            return myWebClient.DownloadString(DestAddr);\n}\n\n', ""\nThank you all for answers, they've helped me to dig in proper direction. I've faced with the same performance issue, though proposed solution to change application config file (as I understood that solution is for web applications) doesn't fit my needs, my solution is shown below:\nHttpWebRequest webRequest;\n\nwebRequest = (HttpWebRequest)System.Net.WebRequest.Create(fullUrl);\nwebRequest.Method = WebRequestMethods.Http.Post;\n\nif (useDefaultProxy)\n{\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n    webRequest.Credentials = CredentialCache.DefaultCredentials;\n}\nelse\n{\n    System.Net.WebRequest.DefaultWebProxy = null;\n    webRequest.Proxy = System.Net.WebRequest.DefaultWebProxy;\n}\n\n"", ""\nWhy wouldn't multithreading solve this issue? Multithreading would minimize the network wait times, and since you'd be storing the contents of the buffer in system memory (RAM), there would be no IO bottleneck from dealing with a filesystem. Thus, your 82 pages that take 82 seconds to download and parse, should take like 15 seconds (assuming a 4x processor). Correct me if I'm missing something. \n____ DOWNLOAD THREAD_____*\nDownload Contents\nForm Stream\nRead Contents\n_________________________*\n"", '\nTry to add cookie(AspxAutoDetectCookieSupport=1) to your request like this\nrequest.CookieContainer = new CookieContainer();         \nrequest.CookieContainer.Add(new Cookie(""AspxAutoDetectCookieSupport"", ""1"") { Domain = target.Host });\n\n']",https://stackoverflow.com/questions/901323/httpwebresponse-streamreader-very-slow,web-crawler
Web crawler that can interpret JavaScript [closed],"






Closed. This question needs to be more focused. It is not currently accepting answers.
                        
                    










Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 6 years ago.







                        Improve this question
                    



I want to write a web crawler that can interpret JavaScript. Basically its a program in Java or PHP that takes a URL as input and outputs the DOM tree which is similar to the output in Firebug HTML window. The best example is Kayak.com where you can not see the resulting DOM displayed on the browser when you 'view source' but can save the resulting HTML though Firebug. 
How would I go about doing this? What tools exist that would help me?
",21k,"
            18
        ","['\nRuby\'s Capybara is an integration test library, but it can also be used to write stand-alone web-crawlers. Given that it uses backends like Selenium or headless WebKit, it interprets javascript out-of-the-box:\nrequire \'capybara/dsl\'\nrequire \'capybara-webkit\'\n\ninclude Capybara::DSL\nCapybara.current_driver = :webkit\nCapybara.app_host = ""http://www.google.com""\npage.visit(""/"")\nputs(page.html)\n\n', ""\nI've been using HtmlUnit (Java). This was originally designed for unit testing pages. It's not perfect javascript, but it hasn't failed me in my limited usage. According to the site, it can run the following JS frameworks to a reasonable degree:\n\njQuery 1.2.6\nMochiKit 1.4.1\nGWT 2.0.0\nSarissa 0.9.9.3\nMooTools 1.2.1\nPrototype 1.6.0\nExt JS 2.2\nDojo 1.0.2\nYUI 2.3.0\n\n"", ""\nYou are more likely to have success in Java than in PHP.  There is a pre-existing Javascript interpreter for Java called Rhino.  It's a reference implementation, and well-documented.\nRhino is used in lots of existing Java apps to provide Javascript scripting ability within the app.  I have also heard of it used to assist with performing automated tests in Javascript.\nI also know that Java includes code that can parse and render HTML, though someone who knows more about Java than me can probably advise more on that.  I am not denying it would be very difficult to achieve something like this; you'd essentially be re-implementing a lot of what a browser does.\n"", ""\nYou could use Mozilla's rendering engine Gecko:\nhttps://developer.mozilla.org/en/Gecko\n"", '\nGive a look here: http://snippets.scrapy.org/snippets/22/\nit\'s a python screen scraping and web crawling framework used with webdrivers that open a page, render all the things you need and gives you the possibilities to ""capture"" anything you want in the page via \n']",https://stackoverflow.com/questions/2670082/web-crawler-that-can-interpret-javascript,web-crawler
Submit form with no submit button in rvest,"
I'm trying write a crawler to download some information, similar to this Stack Overflow post.  The answer is useful for creating the filled-in form, but I'm struggling to find a way to submit the form when a submit button is not part of the form.  Here is an example:
session <- html_session(""www.chase.com"")
form <- html_form(session)[[3]]

filledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)
session <- submit_form(session, filledform)

At this point, I receive this error:
Error in names(submits)[[1]] : subscript out of bounds

How can I make this form submit?
",3k,"
            8
        ","['\nHere\'s a dirty hack that works for me: After studying the submit_form source code, I figured that I could work around the problem by injecting a fake submit button into my code version of the form, and then the submit_form function would call that. It works, except that it gives a warning that often lists an inappropriate input object (not in the example below, though). However, despite the warning, the code works for me:\nsession <- html_session(""www.chase.com"")\nform <- html_form(session)[[3]]\n\n# Form on home page has no submit button,\n# so inject a fake submit button or else rvest cannot submit it.\n# When I do this, rvest gives a warning ""Submitting with \'___\'"", where ""___"" is\n# often an irrelevant field item.\n# This warning might be an rvest (version 0.3.2) bug, but the code works.\nfake_submit_button <- list(name = NULL,\n                           type = ""submit"",\n                           value = NULL,\n                           checked = NULL,\n                           disabled = NULL,\n                           readonly = NULL,\n                           required = FALSE)\nattr(fake_submit_button, ""class"") <- ""input""\nform[[""fields""]][[""submit""]] <- fake_submit_button\n\nuser_name <- ""user""\nusr_password <- ""password""\n\nfilledform <- set_values(form, `user_name` = user_name, `usr_password` = usr_password)\nsession <- submit_form(session, filledform)\n\nThe successful result displays the following warning, which I simply ignore:\n> Submitting with \'submit\'\n\n']",https://stackoverflow.com/questions/33885629/submit-form-with-no-submit-button-in-rvest,web-crawler
HtmlUnit Only Displays Host HTML Page for GWT App,"
I am using HtmlUnit API to add crawler support to my GWT app as follows:
PrintWriter out = null;
try {
    resp.setCharacterEncoding(CHAR_ENCODING);
    resp.setContentType(""text/html"");

    url = buildUrl(req);
    out = resp.getWriter();

    WebClient webClient = webClientProvider.get();

    // set options
    WebClientOptions options = webClient.getOptions();
    options.setCssEnabled(false);
    options.setThrowExceptionOnScriptError(false);
    options.setThrowExceptionOnFailingStatusCode(false);
    options.setRedirectEnabled(true);
    options.setJavaScriptEnabled(true);

    // set timeouts
    webClient.setJavaScriptTimeout(0);
    webClient.waitForBackgroundJavaScript(20000);

    // ajax controller
    webClient.setAjaxController(new NicelyResynchronizingAjaxController());

    // render page
    HtmlPage page = webClient.getPage(url);

    webClient.getJavaScriptEngine().pumpEventLoop(timeoutMillis);

    out.println(page.asXml());

    webClient.closeAllWindows();
}
...

However; only the bare HTML host page for my GWT app is produced and sent to the client.

UPDATE: Here is the output from Chrome DevTools:
Request URL:http://127.0.0.1:8888/MyApp.html?gwt.codesvr=127.0.0.1:9997&_escaped_fragment_=myobject%3Bid%3D507ac730e4b0e3b7a73b1b81
Request Method:GET
Status Code:200 OK
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Charset:ISO-8859-1,utf-8;q=0.7,*;q=0.3
Accept-Encoding:gzip,deflate,sdch
Accept-Language:en-GB,en-US;q=0.8,en;q=0.6
Cache-Control:max-age=0
Connection:keep-alive
Cookie:__utma=96992031.428505342.1351707614.1351707614.1356355174.2; __utmb=96992031.1.10.1356355174; __utmc=96992031; __utmz=96992031.1351707614.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)
Host:127.0.0.1:8888
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11
Query String Parametersview URL encoded
gwt.codesvr:127.0.0.1:9997
_escaped_fragment_:myobject;id=507ac730e4b0e3b7a73b1b81
Response Headersview source
Content-Type:text/html; charset=utf-8
Server:Jetty(6.1.x)
Transfer-Encoding:chunked

Why isn't the GWT code being executed?
",3k,"
            5
        ","['\nI had to try many variants before I finally got it to work.  One key is to leave enough time for the javascript to fully run.  But there were a few other subtleties I don\'t recall -- you can find below my filter version that seems to work for me, look at the parameters I set, some were keys to get this thing to work.  Other than the timer parameters that depend upon what the code to execute (and server ability to run it quickly too), it is pretty generic, so I don\'t understand why Google does not package such a function once and for all!\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT1 = ""_escaped_fragment_="";\nprivate static final int ESCAPED_FRAGMENT_LENGTH1 = ESCAPED_FRAGMENT_FORMAT1.length();\n/**\n * Special URL token that gets passed from the crawler to the servlet filter.\n * This token is used in case there are not already existing query parameters.\n */\nprivate static final String ESCAPED_FRAGMENT_FORMAT2 = ""&""+ESCAPED_FRAGMENT_FORMAT1;\nprivate static final int ESCAPED_FRAGMENT_LENGTH2 = ESCAPED_FRAGMENT_FORMAT2.length();\n\nprivate class SyncAllAjaxController extends NicelyResynchronizingAjaxController\n{\n  private static final long serialVersionUID = 1L;\n  @Override\n  public boolean processSynchron(HtmlPage page, WebRequest request, boolean async)\n  {\n      return true;\n  }\n}\n\nprivate WebClient webClient = null;\n\nprivate static final long _pumpEventLoopTimeoutMillis = 200;\nprivate static final long _jsTimeoutMillis = 200;\nprivate static final long _pageWaitMillis = 100;\nfinal int _maxLoopChecks = 2;\n\npublic void destroy()\n{\n  if (webClient != null)\n    webClient.closeAllWindows();\n}\n\npublic void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain)\n    throws IOException, ServletException\n{\n  // Grab the request uri and query strings.\n  final HttpServletRequest httpRequest = (HttpServletRequest) request;\n  final String requestURI = httpRequest.getRequestURI();\n  final String queryString = httpRequest.getQueryString();\n  final HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n  if ((queryString != null) && (queryString.contains(ESCAPED_FRAGMENT_FORMAT1)))\n  {\n    // This is a Googlebot crawler request, let\'s return a static indexable html page\n    // post javascript execution, as rendered in the browser.\n\n    final String domain = httpRequest.getServerName();\n    final int port = httpRequest.getServerPort();\n\n    // Rewrite the URL back to the original #! version\n    //   -- basically remove _escaped_fragment_ from the query. \n    // Unescape any %XX characters as need be.\n    final String urlStringWithHashFragment = requestURI + rewriteQueryString(queryString);\n    final String protocol = httpRequest.getProtocol();\n    final URL urlWithHashFragment = new URL(protocol, domain, port, urlStringWithHashFragment);\n    final WebRequest webRequest = new WebRequest(urlWithHashFragment);\n\n    // Use the headless browser to obtain an HTML snapshot.\n    webClient = new WebClient(BrowserVersion.FIREFOX_3_6);\n    webClient.getCache().clear();\n    webClient.setJavaScriptEnabled(true);\n    webClient.setThrowExceptionOnScriptError(false);\n    webClient.setRedirectEnabled(false);\n    webClient.setAjaxController(new SyncAllAjaxController());\n    webClient.setCssErrorHandler(new SilentCssErrorHandler());\n\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit starting webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n    final HtmlPage page = webClient.getPage(webRequest);\n\n    // Important!  Give the headless browser enough time to execute JavaScript\n    // The exact time to wait may depend on your application.\n\n    webClient.getJavaScriptEngine().pumpEventLoop(_pumpEventLoopTimeoutMillis);\n\n    int waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n    int loopCount = 0;\n    while (waitForBackgroundJavaScript > 0 && loopCount < _maxLoopChecks)\n    {\n      ++loopCount;\n      waitForBackgroundJavaScript = webClient.waitForBackgroundJavaScript(_jsTimeoutMillis);\n      if (waitForBackgroundJavaScript == 0)\n      {\n        if (_logger.isTraceEnabled())\n          _logger.trace(""HtmlUnit exits background javascript at loop counter "" + loopCount);\n        break;\n      }\n      synchronized (page) \n      {\n        if (_logger.isTraceEnabled())\n            _logger.trace(""HtmlUnit waits for background javascript at loop counter "" + loopCount);\n        try\n        {\n          page.wait(_pageWaitMillis);\n        }\n        catch (InterruptedException e)\n        {\n          _logger.error(""HtmlUnit ERROR on page.wait at loop counter "" + loopCount);\n          e.printStackTrace();\n        }\n      }\n    }\n    webClient.getAjaxController().processSynchron(page, webRequest, false);\n    if (webClient.getJavaScriptEngine().isScriptRunning())\n    {\n      _logger.warn(""HtmlUnit webClient.getJavaScriptEngine().shutdownJavaScriptExecutor()"");\n      webClient.getJavaScriptEngine().shutdownJavaScriptExecutor();\n    }\n\n    // Return the static snapshot.\n    final String staticSnapshotHtml = page.asXml();\n    httpResponse.setContentType(""text/html;charset=UTF-8"");\n    final PrintWriter out = httpResponse.getWriter();\n    out.println(""<hr />"");\n    out.println(""<center><h3>Page non-interactive pour le crawler."");\n    out.println(""La page interactive est: <a href=\\""""\n        + urlWithHashFragment\n        + ""\\"">""\n        + urlWithHashFragment + ""</a></h3></center>"");\n    out.println(""<hr />"");\n    out.println(staticSnapshotHtml);\n    // Close web client.\n    webClient.closeAllWindows();\n    out.println("""");\n    out.flush();\n    out.close();\n    if (_logger.isInfoEnabled())\n      _logger.info(""HtmlUnit completed webClient.getPage(webRequest) where webRequest = "" + webRequest.toString());\n  }\n  else\n  {\n    if (requestURI.contains("".nocache.""))\n    {\n      // Ensure the gwt nocache bootstrapping file is never cached.\n      // References:\n      //   http://stackoverflow.com/questions/4274053/how-to-clear-cache-in-gwt\n      //   http://seewah.blogspot.com/2009/02/gwt-tips-2-nocachejs-getting-cached-in.html\n      // \n      final Date now = new Date();\n      httpResponse.setDateHeader(""Date"", now.getTime());\n      httpResponse.setDateHeader(""Expires"", now.getTime() - 86400000L); // One day old.\n      httpResponse.setHeader(""Pragma"", ""no-cache"");\n      httpResponse.setHeader(""Cache-control"", ""no-cache, no-store, must-revalidate"");\n    }\n\n    filterChain.doFilter(request, response);\n  }\n}\n\n/**\n * Maps from the query string that contains _escaped_fragment_ to one that\n * doesn\'t, but is instead followed by a hash fragment. It also unescapes any\n * characters that were escaped by the crawler. If the query string does not\n * contain _escaped_fragment_, it is not modified.\n * \n * @param queryString\n * @return A modified query string followed by a hash fragment if applicable.\n *         The non-modified query string otherwise.\n * @throws UnsupportedEncodingException\n */\nprivate static String rewriteQueryString(String queryString)\n    throws UnsupportedEncodingException\n{\n  // Seek the escaped fragment.\n  int index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT2);\n  int length = ESCAPED_FRAGMENT_LENGTH2;\n  if (index == -1)\n  {\n    index = queryString.indexOf(ESCAPED_FRAGMENT_FORMAT1);\n    length = ESCAPED_FRAGMENT_LENGTH1;\n  }\n  if (index != -1)\n  {\n    // Found the escaped fragment, so build back the original decoded one.\n    final StringBuilder queryStringSb = new StringBuilder();\n    // Add url parameters if any.\n    if (index > 0)\n    {\n      queryStringSb.append(""?"");\n      queryStringSb.append(queryString.substring(0, index));\n    }\n    // Add the hash fragment as a replacement for the escaped fragment.\n    queryStringSb.append(""#!"");\n    // Add the decoded token.\n    final String token2Decode = queryString.substring(index + length, queryString.length());\n    final String tokenDecoded = URLDecoder.decode(token2Decode, ""UTF-8"");\n    queryStringSb.append(tokenDecoded);\n    return queryStringSb.toString();\n  }\n  return queryString;\n}\n\n']",https://stackoverflow.com/questions/13997424/htmlunit-only-displays-host-html-page-for-gwt-app,web-crawler
selenium implicitly wait doesn't work,"
This is the first time I use selenium and headless browser as I want to crawl some web page using ajax tech.
The effect is great, but for some case it takes too much time to load the whole page(especially when some resource is unavailable),so I have to set a time out for the selenium.
First of all I tried set_page_load_timeout() and set_script_timeout(),but when I set these timeouts, I won't get any page source if the page doesn't load completely, as the codes below:
driver = webdriver.Chrome(chrome_options=options)
driver.set_page_load_timeout(5)
driver.set_script_timeout(5)
try:
    driver.get(url)
except Exception:
    driver.execute_script('window.stop()')

print driver.page_source.encode('utf-8')  # raise TimeoutException this line.

so I try to using Implicitly Wait and Conditional Wait, like this:
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
wait = WebDriverWait(driver, timeout=10)
driver.implicitly_wait(2)
start = time.time()
driver.get(url)
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')

This time I got the content that I want.However,it takes a very long time(40+ seconds),that means the timeout I set for 2 seconds doesn't work at all.
In my view, it seems like the driver.get() call ends until the browser stop loading the page, only after that the codes below can work, and you can not kill the get() call or you'll get nothing.
But this is very different from the selenium docs, I REALLY wonder where is the mistake.
environment: OSX 10.12, selenium 3.0.9 with FireFox & GoogleChrome Headless(both latest version.)
--- update ----
Thanks for help.I change the code as below, using WebDriverWait() alone, but there still exist cases that the call last for a very long time, far more than the timeout that I set.
Wonder if I can stop the page load immediately as the time is out?
driver = webdriver.Firefox(firefox_options=options, executable_path=path)
print(""Firefox Headless Browser Invoked"")
start = time.time()
driver.get('url')
end = time.time()
print 'time used: %s s' % str(end - start)
try:
    WebDriverWait(driver, 2, 0.5).until(expected.presence_of_element_located((By.TAG_NAME, 'body')))
    print driver.find_element_by_tag_name('body').text
except Exception:
    driver.execute_script('window.stop()')
driver.quit()

Here is a terminal output in test:
Firefox Headless Browser Invoked
time used: 44.6049938202 s

according to the code this means the driver.get() call takes 44 seconds to finish call, which is unexpected,I wonder if I misunderstood the behavior of the headless browsers?
",3k,"
            2
        ","['\nAs you mentioned in your question it takes too much time to load the whole page(especially when some resource is unavailable) is pretty much possible if the Application Under Test (AUT) uses JavaScript or AJAX Calls.\n\nIn your first scenario you have induced both set_page_load_timeout(5) and set_script_timeout(5)\n\nset_page_load_timeout(time_to_wait) : Sets the amount of time to wait for a page load to complete before throwing an exception.\nset_script_timeout(time_to_wait) : Sets the amount of time that the script should wait during an execute_async_script call before throwing an exception.\n\n\nHence the Application Under Test being dependent on JavaScript or AJAX Calls in presence of both the conditions raises TimeoutException.\n\nIn your second scenario you have induced both implicitly_wait(2) and WebDriverWait(driver, 2, 0.5).\n\nimplicitly_wait(time_to_wait) : Sets the timeout to implicitly wait for an element to be found or a command to complete.\nWebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None) : Sets the timeout in-conjunction with different expected_conditions\nBut you are experiancing a very long timeout(40+ seconds) as it is clearly mentioned in the docs Do not mix implicit and explicit waits which can cause unpredictable wait times\n\n\n\nWARNING : Do not mix implicit and explicit waits. Doing so can cause unpredictable wait times. For example setting an implicit wait of 10 seconds and an explicit wait of 15 seconds, could cause a timeout to occur after 20 seconds.\n\nSolution :\nThe best solution would be to remove all the instance of implicitly_wait(time_to_wait) and replace with WebDriverWait() for a stable behavior of the Application Under Test (AUT).\n\nUpdate\nAs per your counter question, the current code block looks perfect. The measurement of time which you are seeing as time used: 44.6049938202 s is the time required for the Web Page to load completely and functionally that is the time required for the Client (i.e. the Web Browser) to return back the control to the WebDriver instance once \'document.readyState\' equals to ""complete"" is achieved. Selenium or as an user you have no control on this rendering process. However for a better performance you may follow the best practices as follows :\n\nKeep your JDK version updated currently Java SE Development Kit 8u162\nKeep your Selenium Client version updated currently selenium 3.9.0\nKeep your WebDriver version updated.\nKeep your Web Browser version updated.\nClean you Project Workspace within your IDE regularly to build your project with required dependencies only.\nUse CCleaner tool to wipe away the OS chores before and after your Test Suite execution.\nIf your Web Browser base version is too old uninstall the Web Browser through Revo Uninstaller and install a recent GA released version of the Web Browser.\nExecute your Test.\n\n']",https://stackoverflow.com/questions/48989984/selenium-implicitly-wait-doesnt-work,web-crawler
How can I scrape tooltips value from a Tableau graph embedded in a webpage,"
I am trying to figure out if there is a way and how to scrape tooltip values from a Tableau embedded graph in a webpage using python.
Here is an example of a graph with tooltips when user hovers over the bars:
https://public.tableau.com/views/NumberofCOVID-19patientsadmittedordischarged/DASHPublicpage_patientsdischarges?:embed=y&:showVizHome=no&:host_url=https%3A%2F%2Fpublic.tableau.com%2F&:embed_code_version=3&:tabs=no&:toolbar=yes&:animate_transition=yes&:display_static_image=no&:display_spinner=no&:display_overlay=yes&:display_count=yes&publish=yes&:loadOrderID=1
I grabbed this url from the original webpage that I want to scrape from:
https://covid19.colorado.gov/hospital-data
Any help is appreciated.
",2k,"
            2
        ","['\nEdit\nI\'ve made a python library to scrape tableau dashboard. The implementation is more straightforward :\nfrom tableauscraper import TableauScraper as TS\n\nurl = ""https://public.tableau.com/views/Colorado_COVID19_Data/CO_Home""\n\nts = TS()\nts.loads(url)\ndashboard = ts.getDashboard()\n\nfor t in dashboard.worksheets:\n    #show worksheet name\n    print(f""WORKSHEET NAME : {t.name}"")\n    #show dataframe for this worksheet\n    print(t.data)\n\nrun this on repl.it\n\nOld answer\nThe graphic seems to be generated in JS from the result of an API which looks like :\nPOST https://public.tableau.com/TITLE/bootstrapSession/sessions/SESSION_ID \n\nThe SESSION_ID parameter is located (among other things) in tsConfigContainer textarea in the URL used to build the iframe.\nStarting from https://covid19.colorado.gov/hospital-data :\n\ncheck element with class tableauPlaceholder\nget the param element with attribute name\nit gives you the url : https://public.tableau.com/views/{urlPath}\nthe previous link gives you a textarea with id tsConfigContainer with a bunch of json values\nextract the session_id and root path (vizql_root)\nmake a POST on https://public.tableau.com/ROOT_PATH/bootstrapSession/sessions/SESSION_ID with the sheetId as form data\nextract the json from the result (result is not json)\n\nCode :\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\nr = requests.get(""https://covid19.colorado.gov/hospital-data"")\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\n# get the second tableau link\ntableauContainer = soup.findAll(""div"", { ""class"": ""tableauPlaceholder""})[1]\nurlPath = tableauContainer.find(""param"", { ""name"": ""name""})[""value""]\n\nr = requests.get(\n    f""https://public.tableau.com/views/{urlPath}"",\n    params= {\n        "":showVizHome"":""no"",\n    }\n)\nsoup = BeautifulSoup(r.text, ""html.parser"")\n\ntableauData = json.loads(soup.find(""textarea"",{""id"": ""tsConfigContainer""}).text)\n\ndataUrl = f\'https://public.tableau.com{tableauData[""vizql_root""]}/bootstrapSession/sessions/{tableauData[""sessionid""]}\'\n\nr = requests.post(dataUrl, data= {\n    ""sheet_id"": tableauData[""sheetId""],\n})\n\ndataReg = re.search(\'\\d+;({.*})\\d+;({.*})\', r.text, re.MULTILINE)\ninfo = json.loads(dataReg.group(1))\ndata = json.loads(dataReg.group(2))\n\nprint(data[""secondaryInfo""][""presModelMap""][""dataDictionary""][""presModelHolder""][""genDataDictionaryPresModel""][""dataSegments""][""0""][""dataColumns""])\n\nFrom there you have all the data. You will need to look for the way the data is splitted as it seems all the data is dumped through a single list. Probably looking at the other fields in the JSON object would be useful for that.\n']",https://stackoverflow.com/questions/61962611/how-can-i-scrape-tooltips-value-from-a-tableau-graph-embedded-in-a-webpage,web-crawler
Finding the layers and layer sizes for each Docker image,"
For research purposes I'm trying to crawl the public Docker registry ( https://registry.hub.docker.com/ ) and find out 1) how many layers an average image has and 2) the sizes of these layers to get an idea of the distribution.
However I studied the API and public libraries as well as the details on the github but I cant find any method to:

retrieve all the public repositories/images (even if those are thousands I still need a starting list to iterate through)
find all the layers of an image
find the size for a layer (so not an image but for the individual layer).

Can anyone help me find a way to retrieve this information?
Thank you!
EDIT: is anyone able to verify that searching for '*' in Docker registry is returning all the repositories and not just anything that mentions '*' anywhere? https://registry.hub.docker.com/search?q=*
",187k,"
            205
        ","['\nCheck out dive written in golang. \n\nAwesome tool!\n', ""\nYou can first find the image ID using:\n$ docker images -a\n\nThen find the image's layers and their sizes:\n$ docker history --no-trunc <Image ID>\n\nNote: I'm using Docker version 1.13.1\n$ docker -v\nDocker version 1.13.1, build 092cba3\n\n"", '\nYou can find the layers of the images in the folder /var/lib/docker/aufs/layers; provide if you configured for storage-driver as aufs (default option) \nExample:\n docker ps -a\n CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n 0ca502fa6aae        ubuntu              ""/bin/bash""         44 minutes ago      Exited (0) 44 seconds ago                       DockerTest\n\nNow to view the layers of the containers that were created with the image ""Ubuntu""; go to /var/lib/docker/aufs/layers directory and cat the file starts with the container ID (here it is 0ca502fa6aae*)\n root@viswesn-vm2:/var/lib/docker/aufs/layers# cat    0ca502fa6aaefc89f690736609b54b2f0fdebfe8452902ca383020e3b0d266f9-init \n d2a0ecffe6fa4ef3de9646a75cc629bbd9da7eead7f767cb810f9808d6b3ecb6\n 29460ac934423a55802fcad24856827050697b4a9f33550bd93c82762fb6db8f\n b670fb0c7ecd3d2c401fbfd1fa4d7a872fbada0a4b8c2516d0be18911c6b25d6\n 83e4dde6b9cfddf46b75a07ec8d65ad87a748b98cf27de7d5b3298c1f3455ae4\n\nThis will show the result of same by running \nroot@viswesn-vm2:/var/lib/docker/aufs/layers# docker history ubuntu\nIMAGE               CREATED             CREATED BY                                         SIZE                COMMENT\nd2a0ecffe6fa        13 days ago         /bin/sh -c #(nop) CMD [""/bin/bash""]             0 B                 \n29460ac93442        13 days ago         /bin/sh -c sed -i \'s/^#\\s*\\   (deb.*universe\\)$/   1.895 kB            \nb670fb0c7ecd        13 days ago         /bin/sh -c echo \'#!/bin/sh\' > /usr/sbin/polic   194.5 kB            \n83e4dde6b9cf        13 days ago         /bin/sh -c #(nop) ADD file:c8f078961a543cdefa   188.2 MB \n\nTo view the full layer ID; run with --no-trunc option as part of history command.\ndocker history --no-trunc ubuntu\n\n', '\nIn my opinion, docker history <image> is sufficient. This returns the size of each layer:\n$ docker history jenkinsci-jnlp-slave:2019-1-9c\nIMAGE        CREATED    CREATED BY                                    SIZE  COMMENT\n93f48953d298 42 min ago /bin/sh -c #(nop)  USER jenkins               0B\n6305b07d4650 42 min ago /bin/sh -c chown jenkins:jenkins -R /home/je… 1.45GB\n\n', '\nThey have a very good answer here:\nhttps://stackoverflow.com/a/32455275/165865\nJust run below images:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz images -t\n\n', ""\nThis will inspect the docker image and print the layers:\n$ docker image inspect nginx -f '{{.RootFS.Layers}}'\n[sha256:d626a8ad97a1f9c1f2c4db3814751ada64f60aed927764a3f994fcd88363b659 sha256:82b81d779f8352b20e52295afc6d0eab7e61c0ec7af96d85b8cda7800285d97d sha256:7ab428981537aa7d0c79bc1acbf208c71e57d9678f7deca4267cc03fba26b9c8]\n\n"", '\none more tool : https://github.com/CenturyLinkLabs/dockerfile-from-image\nGUI using ImageLayers.io \n', ""\n\nhttps://hub.docker.com/search?q=* shows all the images in the entire Docker hub, it's not possible to get this via the search command as it doesnt accept wildcards.\nAs of v1.10 you can find all the layers in an image by pulling it and using these commands:\ndocker pull ubuntu\nID=$(sudo docker inspect -f {{.Id}} ubuntu)\njq .rootfs.diff_ids /var/lib/docker/image/aufs/imagedb/content/$(echo $ID|tr ':' '/')\n\n\n3) The size can be found in /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/size although LAYERID != the diff_ids found with the previous command. For this you need to look at /var/lib/docker/image/aufs/layerdb/sha256/{LAYERID}/diff and compare with the previous command output to properly match the correct diff_id and size.\n"", ""\nIt's indeed doable to query the manifest or blob info from docker registry server without pulling the image to local disk.\nYou can refer to the Registry v2 API to fetch the manifest of image.\nGET /v2/<name>/manifests/<reference>\n\nNote, you have to handle different manifest version. For v2 you can directly get the size of layer and digest of blob. For v1 manifest, you can HEAD the blob download url to get the actual layer size.\nThere is a simple script for handling above cases that will be continuously maintained. \n"", '\nTo find all the layers of an image and to find the size for a layer, you can display the manifest from the docker hub registry via the ""manifest"" experimental feature:\ndocker manifest inspect ubuntu\n\nThe result is a JSON file (only the first lines are shown here):\n{\n   ""schemaVersion"": 2,\n   ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"",\n   ""manifests"": [\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:10cbddb6cf8568f56584ccb6c866203e68ab8e621bb87038e254f6f27f955bbe"",\n         ""platform"": {\n            ""architecture"": ""amd64"",\n            ""os"": ""linux""\n         }\n      },\n      {\n         ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",\n         ""size"": 529,\n         ""digest"": ""sha256:dd375524d7eda25a69f9f9790cd3e28855be7908e04162360dd462794035ebf7"",\n         ""platform"": {\n            ""architecture"": ""arm"",\n            ""os"": ""linux"",\n            ""variant"": ""v7""\n\n', '\nNot exactly the original question but to find the sum total of all the images without double-counting shared layers, the following is useful (ubuntu 18):\nsudo du -h -d1  /var/lib/docker/overlay2 | sort -h\n\n', ""\nI've solved this problem by using the search function on Docker's website where '*' is a valid search that returns 200k repositories and then I crawled each invididual page. HTML parsing allows me to extract all the image names on each page.\n""]",https://stackoverflow.com/questions/29696656/finding-the-layers-and-layer-sizes-for-each-docker-image,web-crawler
How to write a crawler?,"
I have had thoughts of trying to write a simple crawler that might crawl and produce a list of its findings for our NPO's websites and content.
Does anybody have any thoughts on how to do this? Where do you point the crawler to get started? How does it send back its findings and still keep crawling? How does it know what it finds, etc,etc.
",60k,"
            64
        ","[""\nYou'll be reinventing the wheel, to be sure. But here's the basics:\n\nA list of unvisited URLs - seed this with one or more starting pages\nA list of visited URLs - so you don't go around in circles\nA set of rules for URLs you're not interested in - so you don't index the whole Internet\n\nPut these in persistent storage, so you can stop and start the crawler without losing state.\nAlgorithm is:\nwhile(list of unvisited URLs is not empty) {\n    take URL from list\n    remove it from the unvisited list and add it to the visited list\n    fetch content\n    record whatever it is you want to about the content\n    if content is HTML {\n        parse out URLs from links\n        foreach URL {\n           if it matches your rules\n              and it's not already in either the visited or unvisited list\n              add it to the unvisited list\n        }\n    }\n}\n\n"", '\nThe complicated part of a crawler is if you want to scale it to a huge number of websites/requests.\nIn this situation you will have to deal with some issues like:\n\nImpossibility to keep info all in one database.\nNot enough RAM to deal with huge index(s)\nMultithread performance and concurrency\nCrawler traps (infinite loop created by changing urls, calendars, sessions ids...) and duplicated content.\nCrawl from more than one computer\nMalformed HTML codes\nConstant http errors from servers\nDatabases without compression, wich make your need for space about 8x bigger.\nRecrawl routines and priorities.\nUse requests with compression (Deflate/gzip) (good for any kind of crawler).\n\nAnd some important things\n\nRespect robots.txt\nAnd a crawler delay on each request to dont suffocate web servers.\n\n', ""\nMultithreaded Web Crawler\nIf you want to crawl large sized website then you should write a multi-threaded crawler.\nconnecting,fetching and writing crawled information in files/database - these are the three steps of crawling but if you use a single threaded than your CPU and network utilization will be pour.\nA multi threaded web crawler needs two data structures- linksVisited(this should be implemented as a hashmap or trai) and linksToBeVisited(this is a queue). \nWeb crawler uses BFS to traverse world wide web.\nAlgorithm of a basic web crawler:-\n\nAdd one or more seed urls to linksToBeVisited. The method to add a url to linksToBeVisited must be synchronized.\nPop an element from linksToBeVisited and add this to linksVisited. This pop method to pop url from linksToBeVisited must be synchronized.\nFetch the page from internet.\nParse the file and add any till now not visited link found in the page to linksToBeVisited. URL's can be filtered if needed. The user can give a set of rules to filter which url's to be scanned.\nThe necessary information found on the page is saved in database or file.\nrepeat step 2 to 5 until queue is linksToBeVisited empty.\nHere is a code snippet on how to synchronize the threads....\n public void add(String site) {\n   synchronized (this) {\n   if (!linksVisited.contains(site)) {\n     linksToBeVisited.add(site);\n     }\n   }\n }\n\n public String next() {\n    if (linksToBeVisited.size() == 0) {\n    return null;\n    }\n       synchronized (this) {\n        // Need to check again if size has changed\n       if (linksToBeVisited.size() > 0) {\n          String s = linksToBeVisited.get(0);\n          linksToBeVisited.remove(0);\n          linksVisited.add(s);\n          return s;\n       }\n     return null;\n     }\n  }\n\n\n\n"", ""\nCrawlers are simple in concept.\nYou get a root page via a HTTP GET, parse it to find URLs and put them on a queue unless they've been parsed already (so you need a global record of pages you have already parsed).\nYou can use the Content-type header to find out what the type of content is, and limit your crawler to only parsing the HTML types.\nYou can strip out the HTML tags to get the plain text, which you can do text analysis on (to get tags, etc, the meat of the page). You could even do that on the alt/title tags for images if you got that advanced.\nAnd in the background you can have a pool of threads eating URLs from the Queue and doing the same. You want to limit the number of threads of course.\n"", ""\nIf your NPO's sites are relatively big or complex (having dynamic pages that'll effectively create a 'black hole' like a calendar with a 'next day' link) you'd be better using a real web crawler, like Heritrix.\nIf the sites total a few number of pages you can get away with just using curl or wget or your own. Just remember if they start to get big or you start making your script more complex to just use a real crawler or at least look at its source to see what are they doing and why.\nSome issues (there are more):\n\nBlack holes (as described)\nRetries (what if you get a 500?)\nRedirects\nFlow control (else you can be a burden on the sites)\nrobots.txt implementation\n\n"", '\nWikipedia has a good article about web crawlers, covering many of the algorithms and considerations.\nHowever, I wouldn\'t bother writing my own crawler.  It\'s a lot of work, and since you only need a ""simple crawler"", I\'m thinking all you really need is an off-the-shelf crawler.  There are a lot of free and open-source crawlers that will likely do everything you need, with very little work on your part.\n', '\nYou could make a list of words and make a thread for each word searched at google. Then each thread will create a new thread for each link it find in the page. Each thread should write what it finds in a database. When each thread finishes reading the page, it terminates.And there you have a very big database of links in your database.\n', '\nUse wget, do a recursive web suck, which will dump all the files onto your harddrive, then write another script to go through all the downloaded files and analyze them.\nEdit: or maybe curl instead of wget, but I am not familiar with curl, I do not know if it does recursive downloads like wget.\n', ""\nI'm using Open search server for my company internal search, try this : http://open-search-server.com its also open soruce.\n"", '\ni did a simple web crawler using reactive extension in .net.\nhttps://github.com/Misterhex/WebCrawler\npublic class Crawler\n    {\n    class ReceivingCrawledUri : ObservableBase<Uri>\n    {\n        public int _numberOfLinksLeft = 0;\n\n        private ReplaySubject<Uri> _subject = new ReplaySubject<Uri>();\n        private Uri _rootUri;\n        private IEnumerable<IUriFilter> _filters;\n\n        public ReceivingCrawledUri(Uri uri)\n            : this(uri, Enumerable.Empty<IUriFilter>().ToArray())\n        { }\n\n        public ReceivingCrawledUri(Uri uri, params IUriFilter[] filters)\n        {\n            _filters = filters;\n\n            CrawlAsync(uri).Start();\n        }\n\n        protected override IDisposable SubscribeCore(IObserver<Uri> observer)\n        {\n            return _subject.Subscribe(observer);\n        }\n\n        private async Task CrawlAsync(Uri uri)\n        {\n            using (HttpClient client = new HttpClient() { Timeout = TimeSpan.FromMinutes(1) })\n            {\n                IEnumerable<Uri> result = new List<Uri>();\n\n                try\n                {\n                    string html = await client.GetStringAsync(uri);\n                    result = CQ.Create(html)[""a""].Select(i => i.Attributes[""href""]).SafeSelect(i => new Uri(i));\n                    result = Filter(result, _filters.ToArray());\n\n                    result.ToList().ForEach(async i =>\n                    {\n                        Interlocked.Increment(ref _numberOfLinksLeft);\n                        _subject.OnNext(i);\n                        await CrawlAsync(i);\n                    });\n                }\n                catch\n                { }\n\n                if (Interlocked.Decrement(ref _numberOfLinksLeft) == 0)\n                    _subject.OnCompleted();\n            }\n        }\n\n        private static List<Uri> Filter(IEnumerable<Uri> uris, params IUriFilter[] filters)\n        {\n            var filtered = uris.ToList();\n            foreach (var filter in filters.ToList())\n            {\n                filtered = filter.Filter(filtered);\n            }\n            return filtered;\n        }\n    }\n\n    public IObservable<Uri> Crawl(Uri uri)\n    {\n        return new ReceivingCrawledUri(uri, new ExcludeRootUriFilter(uri), new ExternalUriFilter(uri), new AlreadyVisitedUriFilter());\n    }\n\n    public IObservable<Uri> Crawl(Uri uri, params IUriFilter[] filters)\n    {\n        return new ReceivingCrawledUri(uri, filters);\n    }\n}\n\nand you can use it as follows:\nCrawler crawler = new Crawler();\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n']",https://stackoverflow.com/questions/102631/how-to-write-a-crawler,web-crawler
Python: Disable images in Selenium Google ChromeDriver,"
I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.
1- The answer in Disable images in Selenium Google ChromeDriver works in Java. So we should do the same thing in Python:
opt = webdriver.ChromeOptions()
opt.add_extension(""Block-image_v1.1.crx"")
browser = webdriver.Chrome(chrome_options=opt)

2- But downloading ""Block-image_v1.1.crx"" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj
you can go to http://chrome-extension-downloader.com/
and paste the extension url there to be able to download the extension file.
3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded.
",65k,"
            58
        ","['\nHere is another way to disable images:\nfrom selenium import webdriver\n\nchrome_options = webdriver.ChromeOptions()\nprefs = {""profile.managed_default_content_settings.images"": 2}\nchrome_options.add_experimental_option(""prefs"", prefs)\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n\nI found it below:\nhttp://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option\n', '\nJava:\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.\n    chromeOptions = new ChromeOptions();\n    HashMap<String, Object> images = new HashMap<String, Object>();\n    images.put(""images"", 2);\n    HashMap<String, Object> prefs = new HashMap<String, Object>();\n    prefs.put(""profile.default_content_setting_values"", images);\n    chromeOptions.setExperimentalOption(""prefs"", prefs);\n    driver=new ChromeDriver(chromeOptions);\n\n    firefoxOpt = new FirefoxOptions();\n    FirefoxProfile profile = new FirefoxProfile();\n    profile.setPreference(""permissions.default.image"", 2);\n    firefoxOpt.setProfile(profile);\n\n', '\nThere is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  \ndriver = webdriver.Chrome()\n\n\ndef expand_shadow_element(element):\n  shadow_root = driver.execute_script(\'return arguments[0].shadowRoot\', element)\n  return shadow_root\n\ndriver.get(""chrome://settings"")\nroot1 = driver.find_element_by_tag_name(\'settings-ui\')\nshadow_root1 = expand_shadow_element(root1)\n\nroot2 = shadow_root1.find_element_by_css_selector(\'[page-name=""Settings""]\')\nshadow_root2 = expand_shadow_element(root2)\n\nroot3 = shadow_root2.find_element_by_id(\'search\')\nshadow_root3 = expand_shadow_element(root3)\n\nsearch_button = shadow_root3.find_element_by_id(""searchTerm"")\nsearch_button.click()\n\ntext_area = shadow_root3.find_element_by_id(\'searchInput\')\ntext_area.send_keys(""content settings"")\n\nroot0 = shadow_root1.find_element_by_id(\'main\')\nshadow_root0_s = expand_shadow_element(root0)\n\n\nroot1_p = shadow_root0_s.find_element_by_css_selector(\'settings-basic-page\')\nshadow_root1_p = expand_shadow_element(root1_p)\n\n\nroot1_s = shadow_root1_p.find_element_by_css_selector(\'settings-privacy-page\')\nshadow_root1_s = expand_shadow_element(root1_s)\n\ncontent_settings_div = shadow_root1_s.find_element_by_css_selector(\'#site-settings-subpage-trigger\')\ncontent_settings = content_settings_div.find_element_by_css_selector(""button"")\ncontent_settings.click()\n\n']",https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver,web-crawler
I need a Powerful Web Scraper library [closed],"









                        As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    


Closed 10 years ago.



I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my preferred database. I have searched but i didn't find any good solution for this. I need a good suggestion from experts. Please help me out.
",67k,"
            29
        ","[""\nScraping is easy really, you just have to parse the content you are downloading and get all the associated links.\nThe most important piece though is the part that processes the HTML.  Because most browsers don't require the cleanest (or standards-compliant) HTML in order to be rendered, you need an HTML parser that is going to be able to make sense of HTML that is not always well-formed.\nI recommend you use the HTML Agility Pack for this purpose.  It does very well at handling non-well-formed HTML, and provides an easy interface for you to use XPath queries to get nodes in the resulting document.\nBeyond that, you just need to pick a data store to hold your processed data (you can use any database technology for that) and a way to download content from the web, which .NET provides two high-level mechanisms for, the WebClient and HttpWebRequest/HttpWebResponse classes.\n"", '\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace SoftCircuits.Parsing\n{\n    public class HtmlTag\n    {\n        /// <summary>\n        /// Name of this tag\n        /// </summary>\n        public string Name { get; set; }\n\n        /// <summary>\n        /// Collection of attribute names and values for this tag\n        /// </summary>\n        public Dictionary<string, string> Attributes { get; set; }\n\n        /// <summary>\n        /// True if this tag contained a trailing forward slash\n        /// </summary>\n        public bool TrailingSlash { get; set; }\n\n        /// <summary>\n        /// Indicates if this tag contains the specified attribute. Note that\n        /// true is returned when this tag contains the attribute even when the\n        /// attribute has no value\n        /// </summary>\n        /// <param name=""name"">Name of attribute to check</param>\n        /// <returns>True if tag contains attribute or false otherwise</returns>\n        public bool HasAttribute(string name)\n        {\n            return Attributes.ContainsKey(name);\n        }\n    };\n\n    public class HtmlParser : TextParser\n    {\n        public HtmlParser()\n        {\n        }\n\n        public HtmlParser(string html) : base(html)\n        {\n        }\n\n        /// <summary>\n        /// Parses the next tag that matches the specified tag name\n        /// </summary>\n        /// <param name=""name"">Name of the tags to parse (""*"" = parse all tags)</param>\n        /// <param name=""tag"">Returns information on the next occurrence of the specified tag or null if none found</param>\n        /// <returns>True if a tag was parsed or false if the end of the document was reached</returns>\n        public bool ParseNext(string name, out HtmlTag tag)\n        {\n            // Must always set out parameter\n            tag = null;\n\n            // Nothing to do if no tag specified\n            if (String.IsNullOrEmpty(name))\n                return false;\n\n            // Loop until match is found or no more tags\n            MoveTo(\'<\');\n            while (!EndOfText)\n            {\n                // Skip over opening \'<\'\n                MoveAhead();\n\n                // Examine first tag character\n                char c = Peek();\n                if (c == \'!\' && Peek(1) == \'-\' && Peek(2) == \'-\')\n                {\n                    // Skip over comments\n                    const string endComment = ""-->"";\n                    MoveTo(endComment);\n                    MoveAhead(endComment.Length);\n                }\n                else if (c == \'/\')\n                {\n                    // Skip over closing tags\n                    MoveTo(\'>\');\n                    MoveAhead();\n                }\n                else\n                {\n                    bool result, inScript;\n\n                    // Parse tag\n                    result = ParseTag(name, ref tag, out inScript);\n                    // Because scripts may contain tag characters, we have special\n                    // handling to skip over script contents\n                    if (inScript)\n                        MovePastScript();\n                    // Return true if requested tag was found\n                    if (result)\n                        return true;\n                }\n                // Find next tag\n                MoveTo(\'<\');\n            }\n            // No more matching tags found\n            return false;\n        }\n\n        /// <summary>\n        /// Parses the contents of an HTML tag. The current position should be at the first\n        /// character following the tag\'s opening less-than character.\n        /// \n        /// Note: We parse to the end of the tag even if this tag was not requested by the\n        /// caller. This ensures subsequent parsing takes place after this tag\n        /// </summary>\n        /// <param name=""reqName"">Name of the tag the caller is requesting, or ""*"" if caller\n        /// is requesting all tags</param>\n        /// <param name=""tag"">Returns information on this tag if it\'s one the caller is\n        /// requesting</param>\n        /// <param name=""inScript"">Returns true if tag began, and did not end, and script\n        /// block</param>\n        /// <returns>True if data is being returned for a tag requested by the caller\n        /// or false otherwise</returns>\n        protected bool ParseTag(string reqName, ref HtmlTag tag, out bool inScript)\n        {\n            bool doctype, requested;\n            doctype = inScript = requested = false;\n\n            // Get name of this tag\n            string name = ParseTagName();\n\n            // Special handling\n            if (String.Compare(name, ""!DOCTYPE"", true) == 0)\n                doctype = true;\n            else if (String.Compare(name, ""script"", true) == 0)\n                inScript = true;\n\n            // Is this a tag requested by caller?\n            if (reqName == ""*"" || String.Compare(name, reqName, true) == 0)\n            {\n                // Yes\n                requested = true;\n                // Create new tag object\n                tag = new HtmlTag();\n                tag.Name = name;\n                tag.Attributes = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n            }\n\n            // Parse attributes\n            MovePastWhitespace();\n            while (Peek() != \'>\' && Peek() != NullChar)\n            {\n                if (Peek() == \'/\')\n                {\n                    // Handle trailing forward slash\n                    if (requested)\n                        tag.TrailingSlash = true;\n                    MoveAhead();\n                    MovePastWhitespace();\n                    // If this is a script tag, it was closed\n                    inScript = false;\n                }\n                else\n                {\n                    // Parse attribute name\n                    name = (!doctype) ? ParseAttributeName() : ParseAttributeValue();\n                    MovePastWhitespace();\n                    // Parse attribute value\n                    string value = String.Empty;\n                    if (Peek() == \'=\')\n                    {\n                        MoveAhead();\n                        MovePastWhitespace();\n                        value = ParseAttributeValue();\n                        MovePastWhitespace();\n                    }\n                    // Add attribute to collection if requested tag\n                    if (requested)\n                    {\n                        // This tag replaces existing tags with same name\n                        if (tag.Attributes.ContainsKey(name))\n                            tag.Attributes.Remove(name);\n                        tag.Attributes.Add(name, value);\n                    }\n                }\n            }\n            // Skip over closing \'>\'\n            MoveAhead();\n\n            return requested;\n        }\n\n        /// <summary>\n        /// Parses a tag name. The current position should be the first character of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseTagName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute name. The current position should be the first character\n        /// of the name\n        /// </summary>\n        /// <returns>Returns the parsed name string</returns>\n        protected string ParseAttributeName()\n        {\n            int start = Position;\n            while (!EndOfText && !Char.IsWhiteSpace(Peek()) && Peek() != \'>\' && Peek() != \'=\')\n                MoveAhead();\n            return Substring(start, Position);\n        }\n\n        /// <summary>\n        /// Parses an attribute value. The current position should be the first non-whitespace\n        /// character following the equal sign.\n        /// \n        /// Note: We terminate the name or value if we encounter a new line. This seems to\n        /// be the best way of handling errors such as values missing closing quotes, etc.\n        /// </summary>\n        /// <returns>Returns the parsed value string</returns>\n        protected string ParseAttributeValue()\n        {\n            int start, end;\n            char c = Peek();\n            if (c == \'""\' || c == \'\\\'\')\n            {\n                // Move past opening quote\n                MoveAhead();\n                // Parse quoted value\n                start = Position;\n                MoveTo(new char[] { c, \'\\r\', \'\\n\' });\n                end = Position;\n                // Move past closing quote\n                if (Peek() == c)\n                    MoveAhead();\n            }\n            else\n            {\n                // Parse unquoted value\n                start = Position;\n                while (!EndOfText && !Char.IsWhiteSpace(c) && c != \'>\')\n                {\n                    MoveAhead();\n                    c = Peek();\n                }\n                end = Position;\n            }\n            return Substring(start, end);\n        }\n\n        /// <summary>\n        /// Locates the end of the current script and moves past the closing tag\n        /// </summary>\n        protected void MovePastScript()\n        {\n            const string endScript = ""</script"";\n\n            while (!EndOfText)\n            {\n                MoveTo(endScript, true);\n                MoveAhead(endScript.Length);\n                if (Peek() == \'>\' || Char.IsWhiteSpace(Peek()))\n                {\n                    MoveTo(\'>\');\n                    MoveAhead();\n                    break;\n                }\n            }\n        }\n    }\n}\n\n', '\nFor simple websites ( = plain html only), Mechanize works really well and fast. For sites that use Javascript, AJAX or even Flash, you need a real browser solution such as iMacros.\n', ""\nMy Advice:\nYou could look around for a HTML Parser and then use it to parse out information from sites. (Like here). Then all you would need to do is save that data into your database however you see fit.\nI've made my own scraper a few times, it's pretty easy and allow you to customize the data that is saved. \nData Mining Tools\nIf you really just want to get a tool to do this then you should have no problem finding some. \n""]",https://stackoverflow.com/questions/4377355/i-need-a-powerful-web-scraper-library,web-crawler
scrapy- how to stop Redirect (302),"
I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist. 
Redirecting (302) to <GET http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197> from <GET http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx>

The problem is http://www.shop.inonit.in/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/pid-1275197.aspx exists, but http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-Cushions/Shor-Sharaba/Andaz-Apna-Apna-Cushion-Cover/1275197 doesn't, so the crawler cant find this. I've crawled many other websites as well but didn't have this problem anywhere else. Is there a way I can stop this redirect?
Any help would be much appreciated. Thanks.
Update: This is my spider class
class Inon_Spider(BaseSpider):
name = 'Inon'
allowed_domains = ['www.shop.inonit.in']

start_urls = ['http://www.shop.inonit.in/Products/Inonit-Gadget-Accessories-Mobile-Covers/-The-Red-Tag/Samsung-Note-2-Dead-Mau/pid-2656465.aspx']

def parse(self, response):

    item = DealspiderItem()
    hxs = HtmlXPathSelector(response)

    title = hxs.select('//div[@class=""aboutproduct""]/div[@class=""container9""]/div[@class=""ctl_aboutbrand""]/h1/text()').extract()
    price = hxs.select('//span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_spnWebPrice""]/span[@class=""offer""]/span[@id=""ctl00_ContentPlaceHolder1_Price_ctl00_lblOfferPrice""]/text()').extract()
    prc = price[0].replace(""Rs.  "","""")
    description = []

    item['price'] = prc
    item['title'] = title
    item['description'] = description
    item['url'] = response.url

    return item

",29k,"
            25
        ","['\nyes you can do this simply by adding meta values like\nmeta={\'dont_redirect\': True}\n\nalso you can stop redirected for a particular response code like\nmeta={\'dont_redirect\': True,""handle_httpstatus_list"": [302]}\n\nit will stop redirecting only 302 response codes. you can add as many http status code you want to avoid redirecting them.\nexample\nyield Request(\'some url\',\n    meta = {\n        \'dont_redirect\': True,\n        \'handle_httpstatus_list\': [302]\n    },\n    callback= self.some_call_back)\n\n', ""\nAfter looking at the documentation and looking through the relevant source, I was able to figure it out. If you look in the source for start_requests, you'll see that it calls make_requests_from_url for all URLs.\nInstead of modifying start_requests, I modified make_requests_from_url\ndef make_requests_from_url(self, url):\n    return Request(url, dont_filter=True, meta = {\n        'dont_redirect': True,\n        'handle_httpstatus_list': [301, 302]\n    })\n\nAnd added this as part of my spider, right above parse().\n"", '\nBy default, Scrapy use RedirectMiddleware to handle redirection. You can set REDIRECT_ENABLED to False to disable redirection.\nSee documentation. \n', ""\nAs explained here: Scrapy docs\nUse Request Meta\nrequest =  scrapy.Request(link.url, callback=self.parse2)\nrequest.meta['dont_redirect'] = True\nyield request\n\n""]",https://stackoverflow.com/questions/15476587/scrapy-how-to-stop-redirect-302,web-crawler
Writing items to a MySQL database in Scrapy,"
I am new to Scrapy, I had the spider code
class Example_spider(BaseSpider):
   name = ""example""
   allowed_domains = [""www.example.com""]

   def start_requests(self):
       yield self.make_requests_from_url(""http://www.example.com/bookstore/new"")

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       urls = hxs.select('//div[@class=""bookListingBookTitle""]/a/@href').extract()
       for i in urls:
           yield Request(urljoin(""http://www.example.com/"", i[1:]), callback=self.parse_url)

   def parse_url(self, response):
           hxs = HtmlXPathSelector(response)
           main =   hxs.select('//div[@id=""bookshelf-bg""]')
           items = []
           for i in main:
           item = Exampleitem()
           item['book_name'] = i.select('div[@class=""slickwrap full""]/div[@id=""bookstore_detail""]/div[@class=""book_listing clearfix""]/div[@class=""bookstore_right""]/div[@class=""title_and_byline""]/p[@class=""book_title""]/text()')[0].extract()
           item['price'] = i.select('div[@id=""book-sidebar-modules""]/div[@class=""add_to_cart_wrapper slickshadow""]/div[@class=""panes""]/div[@class=""pane clearfix""]/div[@class=""inner""]/div[@class=""add_to_cart 0""]/form/div[@class=""line-item""]/div[@class=""line-item-price""]/text()').extract()
           items.append(item)
       return items

And pipeline code is:
class examplePipeline(object):

    def __init__(self):               
        self.dbpool = adbapi.ConnectionPool('MySQLdb',
                db='blurb',
                user='root',
                passwd='redhat',
                cursorclass=MySQLdb.cursors.DictCursor,
                charset='utf8',
                use_unicode=True
            )
def process_item(self, spider, item):
    # run db query in thread pool
    assert isinstance(item, Exampleitem)
    query = self.dbpool.runInteraction(self._conditional_insert, item)
    query.addErrback(self.handle_error)
    return item
def _conditional_insert(self, tx, item):
    print ""db connected-=========>""
    # create record if doesn't exist. 
    tx.execute(""select * from example_book_store where book_name = %s"", (item['book_name']) )
    result = tx.fetchone()
    if result:
        log.msg(""Item already stored in db: %s"" % item, level=log.DEBUG)
    else:
        tx.execute(""""""INSERT INTO example_book_store (book_name,price)
                    VALUES (%s,%s)"""""",   
                            (item['book_name'],item['price'])
                    )
        log.msg(""Item stored in db: %s"" % item, level=log.DEBUG)            

def handle_error(self, e):
    log.err(e)          

After running this I am getting the following error 
exceptions.NameError: global name 'Exampleitem' is not defined

I got the above error when I added the below code in process_item method
assert isinstance(item, Exampleitem)

and without adding this line I am getting 
**exceptions.TypeError: 'Example_spider' object is not subscriptable

Can anyone make this code run and make sure that all the items saved into database?
",40k,"
            21
        ","['\nTry the following code in your pipeline\nimport sys\nimport MySQLdb\nimport hashlib\nfrom scrapy.exceptions import DropItem\nfrom scrapy.http import Request\n\nclass MySQLStorePipeline(object):\n    def __init__(self):\n        self.conn = MySQLdb.connect(\'host\', \'user\', \'passwd\', \n                                    \'dbname\', charset=""utf8"",\n                                    use_unicode=True)\n        self.cursor = self.conn.cursor()\n\n    def process_item(self, item, spider):    \n        try:\n            self.cursor.execute(""""""INSERT INTO example_book_store (book_name, price)  \n                        VALUES (%s, %s)"""""", \n                       (item[\'book_name\'].encode(\'utf-8\'), \n                        item[\'price\'].encode(\'utf-8\')))            \n            self.conn.commit()            \n        except MySQLdb.Error, e:\n            print ""Error %d: %s"" % (e.args[0], e.args[1])\n        return item\n\n', ""\nYour process_item method should be declared as: def process_item(self, item, spider): instead of def process_item(self, spider, item): -> you switched the arguments around.\nThis exception: exceptions.NameError: global name 'Exampleitem' is not defined indicates you didn't import the Exampleitem in your pipeline.\nTry adding: from myspiders.myitems import Exampleitem (with correct names/paths ofcourse).\n"", '\nI think this way is better and more concise:\n#Item\nclass pictureItem(scrapy.Item):\n    topic_id=scrapy.Field()\n    url=scrapy.Field()\n\n#SQL\nself.save_picture=""insert into picture(`url`,`id`) values(%(url)s,%(id)s);""\n\n#usage\ncur.execute(self.save_picture,dict(item))\n\nIt\'s just like\ncur.execute(""insert into picture(`url`,`id`) values(%(url)s,%(id)s)"" % {""url"":someurl,""id"":1})\n\nCause (you can read more about Items in Scrapy)\n\nThe Field class is just an alias to the built-in dict class and doesn’t provide any extra functionality or attributes. In other words, Field objects are plain-old Python dicts. \n\n']",https://stackoverflow.com/questions/10845839/writing-items-to-a-mysql-database-in-scrapy,web-crawler
Locally run all of the spiders in Scrapy,"
Is there a way to run all of the spiders in a Scrapy project without using the Scrapy daemon? There used to be a way to run multiple spiders with scrapy crawl, but that syntax was removed and Scrapy's code changed quite a bit.
I tried creating my own command:
from scrapy.command import ScrapyCommand
from scrapy.utils.misc import load_object
from scrapy.conf import settings

class Command(ScrapyCommand):
    requires_project = True

    def syntax(self):
        return '[options]'

    def short_desc(self):
        return 'Runs all of the spiders'

    def run(self, args, opts):
        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])
        spiders = spman_cls.from_settings(settings)

        for spider_name in spiders.list():
            spider = self.crawler.spiders.create(spider_name)
            self.crawler.crawl(spider)

        self.crawler.start()

But once a spider is registered with self.crawler.crawl(), I get assertion errors for all of the other spiders:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/scrapy/cmdline.py"", line 138, in _run_command
    cmd.run(args, opts)
  File ""/home/blender/Projects/scrapers/store_crawler/store_crawler/commands/crawlall.py"", line 22, in run
    self.crawler.crawl(spider)
  File ""/usr/lib/python2.7/site-packages/scrapy/crawler.py"", line 47, in crawl
    return self.engine.open_spider(spider, requests)
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1214, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File ""/usr/lib/python2.7/site-packages/twisted/internet/defer.py"", line 1071, in _inlineCallbacks
    result = g.send(result)
  File ""/usr/lib/python2.7/site-packages/scrapy/core/engine.py"", line 215, in open_spider
    spider.name
exceptions.AssertionError: No free spider slots when opening 'spidername'

Is there any way to do this? I'd rather not start subclassing core Scrapy components just to run all of my spiders like this.
",11k,"
            18
        ","[""\nWhy didn't you just use something like:\nscrapy list|xargs -n 1 scrapy crawl\n\n?\n"", ""\nHere is an example that does not run inside a custom command, but runs the Reactor manually and creates a new Crawler for each spider:\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\n# scrapy.conf.settings singlton was deprecated last year\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy import log\n\ndef setup_crawler(spider_name):\n    crawler = Crawler(settings)\n    crawler.configure()\n    spider = crawler.spiders.create(spider_name)\n    crawler.crawl(spider)\n    crawler.start()\n\nlog.start()\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.configure()\n\nfor spider_name in crawler.spiders.list():\n    setup_crawler(spider_name)\n\nreactor.run()\n\nYou will have to design some signal system to stop the reactor when all spiders are finished.\nEDIT: And here is how you can run multiple spiders in a custom command:\nfrom scrapy.command import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import Crawler\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n\n        for spider_name in self.crawler.spiders.list():\n            crawler = Crawler(settings)\n            crawler.configure()\n            spider = crawler.spiders.create(spider_name)\n            crawler.crawl(spider)\n            crawler.start()\n\n        self.crawler.start()\n\n"", ""\nthe answer of @Steven Almeroth will be failed in Scrapy 1.0, and you should edit the script like this:\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n    excludes = ['spider1']\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n        crawler_process = CrawlerProcess(settings) \n\n        for spider_name in crawler_process.spider_loader.list():\n            if spider_name in self.excludes:\n                continue\n            spider_cls = crawler_process.spider_loader.load(spider_name) \n            crawler_process.crawl(spider_cls)\n        crawler_process.start()\n\n"", '\nthis code is works on My scrapy version is 1.3.3 (save it in same directory in scrapy.cfg):\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\nfor scrapy 1.5.x (so you don\'t get the deprecation warning)\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spider_loader.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nLinux script\n#!/bin/bash\nfor spider in $(scrapy list)\ndo\nscrapy crawl ""$spider"" -o ""$spider"".json\ndone\n\n', ""\nRunning all spiders in project using python\n# Run all spiders in project implemented using Scrapy 2.7.0\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\n\ndef main():\n    settings = get_project_settings()\n    process = CrawlerProcess(settings)\n    spiders_names = process.spider_loader.list()\n    for s in spiders_names:\n        process.crawl(s)\n    process.start()\n\n\nif __name__ == '__main__':\n    main()\n\n""]",https://stackoverflow.com/questions/15564844/locally-run-all-of-the-spiders-in-scrapy,web-crawler
Creating a generic scrapy spider,"
My question is really how to do the same thing as a previous question, but in Scrapy 0.14.
Using one Scrapy spider for several websites
Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance.
Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it.
class MySpider(CrawlSpider):

name = 'MySpider'
allowed_domains = ['somedomain.com', 'sub.somedomain.com']
start_urls = ['http://www.somedomain.com']

rules = (
    Rule(SgmlLinkExtractor(allow=('/pages/', ), deny=('', ))),

    Rule(SgmlLinkExtractor(allow=('/2012/03/')), callback='parse_item'),
)

def parse_item(self, response):
    contentTags = []

    soup = BeautifulSoup(response.body)

    contentTags = soup.findAll('p', itemprop=""myProp"")

    for contentTag in contentTags:
        matchedResult = re.search('Keyword1|Keyword2', contentTag.text)
        if matchedResult:
            print('URL Found: ' + response.url)

    pass

",7k,"
            17
        ","['\nYou could create a run-time spider which is evaluated by the interpreter. This code piece could be evaluated at runtime like so:\na = open(""test.py"")\nfrom compiler import compile\nd = compile(a.read(), \'spider.py\', \'exec\')\neval(d)\n\nMySpider\n<class \'__main__.MySpider\'>\nprint MySpider.start_urls\n[\'http://www.somedomain.com\']\n\n', '\nI use the Scrapy Extensions approach to extend the Spider class to a class named Masterspider that includes a generic parser.\nBelow is the very ""short"" version of my generic extended parser. Note that you\'ll need to implement a renderer with a Javascript engine (such as Selenium or BeautifulSoup) a as soon as you start working on pages using AJAX. And a lot of additional code to manage differences between sites (scrap based on column title, handle relative vs long URL, manage different kind of data containers, etc...).\nWhat is interresting with the Scrapy Extension approach is that you can still override the generic parser method if something does not fit but I never had to. The Masterspider class checks if some methods have been created (eg. parser_start, next_url_parser...) under the site specific spider class to allow the management of specificies: send a form, construct the next_url request from elements in the page, etc.\nAs I\'m scraping very different sites, there\'s always specificities to manage. That\'s why I prefer to keep a class for each scraped site so that I can write some specific methods to handle it (pre-/post-processing except PipeLines, Request generators...).\nmasterspider/sitespider/settings.py\nEXTENSIONS = {\n    \'masterspider.masterspider.MasterSpider\': 500\n}\n\nmasterspider/masterspdier/masterspider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\n\nclass MasterSpider(Spider):\n\n    def start_requests(self):\n        if hasattr(self,\'parse_start\'): # First page requiring a specific parser\n            fcallback = self.parse_start\n        else:\n            fcallback = self.parse\n        return [ Request(self.spd[\'start_url\'],\n                     callback=fcallback,\n                     meta={\'itemfields\': {}}) ]\n\n    def parse(self, response):\n        sel = Selector(response)\n        lines = sel.xpath(self.spd[\'xlines\'])\n        # ...\n        for line in lines:\n            item = genspiderItem(response.meta[\'itemfields\'])               \n            # ...\n            # Get request_url of detailed page and scrap basic item info\n            # ... \n            yield  Request(request_url,\n                   callback=self.parse_item,\n                   meta={\'item\':item, \'itemfields\':response.meta[\'itemfields\']})\n\n        for next_url in sel.xpath(self.spd[\'xnext_url\']).extract():\n            if hasattr(self,\'next_url_parser\'): # Need to process the next page URL before?\n                yield self.next_url_parser(next_url, response)\n            else:\n                yield Request(\n                    request_url,\n                    callback=self.parse,\n                    meta=response.meta)\n\n    def parse_item(self, response):\n        sel = Selector(response)\n        item = response.meta[\'item\']\n        for itemname, xitemname in self.spd[\'x_ondetailpage\'].iteritems():\n            item[itemname] = ""\\n"".join(sel.xpath(xitemname).extract())\n        return item\n\nmasterspider/sitespider/spiders/somesite_spider.py\n# -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\nfrom masterspider.masterspider import MasterSpider\n\nclass targetsiteSpider(MasterSpider):\n    name = ""targetsite""\n    allowed_domains = [""www.targetsite.com""]\n    spd = {\n        \'start_url\' : ""http://www.targetsite.com/startpage"", # Start page\n        \'xlines\' : ""//td[something...]"",\n        \'xnext_url\' : ""//a[contains(@href,\'something?page=\')]/@href"", # Next pages\n        \'x_ondetailpage\' : {\n            ""itemprop123"" :      u""id(\'someid\')//text()""\n            }\n    }\n\n#     def next_url_parser(self, next_url, response): # OPTIONAL next_url regexp pre-processor\n#          ...\n\n', ""\nInstead of having the variables name,allowed_domains, start_urls and rules attached to the class, you should write a MySpider.__init__, call CrawlSpider.__init__ from that passing the necessary arguments, and setting name, allowed_domains etc. per object. \nMyProp and keywords also should be set within your __init__. So in the end you should have something like below. You don't have to add name to the arguments, as name is set by BaseSpider itself from kwargs: \nclass MySpider(CrawlSpider):\n\n    def __init__(self, allowed_domains=[], start_urls=[], \n            rules=[], findtag='', finditemprop='', keywords='', **kwargs):\n        CrawlSpider.__init__(self, **kwargs)\n        self.allowed_domains = allowed_domains\n        self.start_urls = start_urls\n        self.rules = rules\n        self.findtag = findtag\n        self.finditemprop = finditemprop\n        self.keywords = keywords\n\n    def parse_item(self, response):\n        contentTags = []\n\n        soup = BeautifulSoup(response.body)\n\n        contentTags = soup.findAll(self.findtag, itemprop=self.finditemprop)\n\n        for contentTag in contentTags:\n            matchedResult = re.search(self.keywords, contentTag.text)\n            if matchedResult:\n                print('URL Found: ' + response.url)\n\n"", '\nI am not sure which way is preferred, but I will tell you what I have done in the past. I am in no way sure that this is the best (or correct) way of doing this and I would be interested to learn what other people think.\nI usually just override the parent class (CrawlSpider) and either pass in arguments and then initialize the parent class via super(MySpider, self).__init__() from within my own init-function or I pull in that data from a database where I have saved a list of links to be appended to start_urls earlier.\n', '\nAs far as crawling specific domains passed as arguments goes, I just override Spider.__init__:\nclass MySpider(scrapy.Spider):\n    """"""\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    """"""\n    def __init__(self, name=None, **kwargs):\n        if \'start_urls\' in kwargs:\n            self.start_urls = kwargs.pop(\'start_urls\').split(\',\')\n        super(Spider, self).__init__(name, **kwargs)\n\n']",https://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider,web-crawler
How do I stop all spiders and the engine immediately after a condition in a pipeline is met?,"
We have a system written with scrapy to crawl a few websites. There are several spiders, and a few cascaded pipelines for all items passed by all crawlers.
One of the pipeline components queries the google servers for geocoding addresses.
Google imposes a limit of 2500 requests per day per IP address, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'.
Hence I want to know about any mechanism which I can invoke from within the pipeline that will completely and immediately stop all further crawling/processing of all spiders and also the main engine.
I have checked other similar questions and their answers have not worked:

Force my scrapy spider to stop crawling


from scrapy.project import crawler
crawler._signal_shutdown(9,0) #Run this if the cnxn fails.


this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address)

import sys
sys.exit(""SHUT DOWN EVERYTHING!"")


this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -> exceptions.SystemExit raised (to no effect)

How can I make scrapy crawl break and exit when encountering the first exception?


crawler.engine.close_spider(self, 'log message')


this one has the same problem as the first case mentioned above.
I tried:

scrapy.project.crawler.engine.stop()


To no avail
EDIT:
If I do in the pipeline:

from scrapy.contrib.closespider import CloseSpider

what should I pass as the 'crawler' argument to the CloseSpider's init() from the scope of my pipeline?
",9k,"
            13
        ","['\nYou can raise a CloseSpider exception to close down a spider.\nHowever, I don\'t think this will work from a pipeline. \nEDIT: avaleske notes in the comments to this answer that he was able to raise a CloseSpider exception from a pipeline. Most wise would be to use this.\nA similar situation has been described on the Scrapy Users group, in this thread.\nI quote:\n\nTo close an spider for any part of your code you should use \n  engine.close_spider  method. See this extension for an usage\n  example: \n  https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61\n\nYou could write your own extension, whilst looking at closespider.py as an example, which will shut down a spider if a certain condition has been met.\nAnother ""hack"" would be to set a flag on the spider in the pipeline. For example:\npipeline:\ndef process_item(self, item, spider):\n    if some_flag:\n        spider.close_down = True\n\nspider:\ndef parse(self, response):\n    if self.close_down:\n        raise CloseSpider(reason=\'API usage exceeded\')\n\n']",https://stackoverflow.com/questions/9699049/how-do-i-stop-all-spiders-and-the-engine-immediately-after-a-condition-in-a-pipe,web-crawler
How to print html source to console with phantomjs,"
I just downloaed and installed phantomjs on my machine. I copy and pasted the following script into a file called hello.js:
var page = require('webpage').create();
var url = 'https://www.google.com'

page.onLoadStarted = function () {
    console.log('Start loading...');
};

page.onLoadFinished = function (status) {
    console.log('Loading finished.');
phantom.exit();
};

page.open(url);

I'd like to print the complete html source (in this case from the google page) to a file or to the console. How do I do this?
",28k,"
            12
        ","[""\nSpent some time to read the documentation, it should be obvious afterwards.\nvar page = require('webpage').create();\npage.open('http://google.com', function () {\n    console.log(page.content);\n    phantom.exit();\n});\n\n""]",https://stackoverflow.com/questions/12450868/how-to-print-html-source-to-console-with-phantomjs,web-crawler
How to get a web page's source code from Java [duplicate],"






This question already has answers here:
                        
                    



How do you Programmatically Download a Webpage in Java

                                (11 answers)
                            

Closed 7 years ago.



I just want to retrieve any web page's source code from Java. I found lots of solutions so far, but I couldn't find any code that works for all the links below: 

http://www.cumhuriyet.com.tr?hn=298710
http://www.fotomac.com.tr/Yazarlar/Olcay%20%C3%87ak%C4%B1r/2011/11/23/hesap-makinesi 
http://www.sabah.com.tr/Gundem/2011/12/23/basbakan-konferansta-konusuyor#

The main problem for me is that some codes retrieve web page source code, but with missing ones. For example the code below does not work for the first link.
InputStream is = fURL.openStream(); //fURL can be one of the links above
BufferedReader buffer = null;
buffer = new BufferedReader(new InputStreamReader(is, ""iso-8859-9""));

int byteRead;
while ((byteRead = buffer.read()) != -1) {
    builder.append((char) byteRead);
}
buffer.close();
System.out.println(builder.toString());

",110k,"
            11
        ","['\nTry the following code with an added request property:\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class SocketConnection\n{\n    public static String getURLSource(String url) throws IOException\n    {\n        URL urlObject = new URL(url);\n        URLConnection urlConnection = urlObject.openConnection();\n        urlConnection.setRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.95 Safari/537.11"");\n\n        return toString(urlConnection.getInputStream());\n    }\n\n    private static String toString(InputStream inputStream) throws IOException\n    {\n        try (BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream, ""UTF-8"")))\n        {\n            String inputLine;\n            StringBuilder stringBuilder = new StringBuilder();\n            while ((inputLine = bufferedReader.readLine()) != null)\n            {\n                stringBuilder.append(inputLine);\n            }\n\n            return stringBuilder.toString();\n        }\n    }\n}\n\n', '\nURL yahoo = new URL(""http://www.yahoo.com/"");\nBufferedReader in = new BufferedReader(\n            new InputStreamReader(\n            yahoo.openStream()));\n\nString inputLine;\n\nwhile ((inputLine = in.readLine()) != null)\n    System.out.println(inputLine);\n\nin.close();\n\n', '\nI am sure that you have found a solution somewhere over the past 2 years but the following is a solution that works for your requested site\npackage javasandbox;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.URL;\n\n/**\n*\n* @author Ryan.Oglesby\n*/\npublic class JavaSandbox {\n\nprivate static String sURL;\n\n/**\n * @param args the command line arguments\n */\npublic static void main(String[] args) throws MalformedURLException, IOException {\n    sURL = ""http://www.cumhuriyet.com.tr/?hn=298710"";\n    System.out.println(sURL);\n    URL url = new URL(sURL);\n    HttpURLConnection httpCon = (HttpURLConnection) url.openConnection();\n    //set http request headers\n            httpCon.addRequestProperty(""Host"", ""www.cumhuriyet.com.tr"");\n            httpCon.addRequestProperty(""Connection"", ""keep-alive"");\n            httpCon.addRequestProperty(""Cache-Control"", ""max-age=0"");\n            httpCon.addRequestProperty(""Accept"", ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"");\n            httpCon.addRequestProperty(""User-Agent"", ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36"");\n            httpCon.addRequestProperty(""Accept-Encoding"", ""gzip,deflate,sdch"");\n            httpCon.addRequestProperty(""Accept-Language"", ""en-US,en;q=0.8"");\n            //httpCon.addRequestProperty(""Cookie"", ""JSESSIONID=EC0F373FCC023CD3B8B9C1E2E2F7606C; lang=tr; __utma=169322547.1217782332.1386173665.1386173665.1386173665.1; __utmb=169322547.1.10.1386173665; __utmc=169322547; __utmz=169322547.1386173665.1.1.utmcsr=stackoverflow.com|utmccn=(referral)|utmcmd=referral|utmcct=/questions/8616781/how-to-get-a-web-pages-source-code-from-java; __gads=ID=3ab4e50d8713e391:T=1386173664:S=ALNI_Mb8N_wW0xS_wRa68vhR0gTRl8MwFA; scrElm=body"");\n            HttpURLConnection.setFollowRedirects(false);\n            httpCon.setInstanceFollowRedirects(false);\n            httpCon.setDoOutput(true);\n            httpCon.setUseCaches(true);\n\n            httpCon.setRequestMethod(""GET"");\n\n            BufferedReader in = new BufferedReader(new InputStreamReader(httpCon.getInputStream(), ""UTF-8""));\n            String inputLine;\n            StringBuilder a = new StringBuilder();\n            while ((inputLine = in.readLine()) != null)\n                a.append(inputLine);\n            in.close();\n\n            System.out.println(a.toString());\n\n            httpCon.disconnect();\n}\n}\n\n']",https://stackoverflow.com/questions/8616781/how-to-get-a-web-pages-source-code-from-java,web-crawler
Crawling the Google Play store,"
I want to crawl the Google Play store to download the web pages of all the android application (All the webpages with the following base url: https://play.google.com/store/apps/). I checked the robots.txt file of the play store and it disallows crawling these URLs. 
Also, when I browse the Google Play store I can only see top applications up to 3 pages for each of the categories. How can I get the other application pages?
If anyone has tried crawling the Google Play please let me know the following things:
a) Were you successful in crawling the play store. If yes, please let me know how you did that.
b) How to crawl the hidden application pages not visible in top apps for each of the categories?
c) Is there a techniques to download the applications also and not just the webpages?
I already searched around and found the following links:
a) https://code.google.com/p/android-market-api/ 
b) https://code.google.com/p/android-marketplace-crawler/source/checkout 
c) http://mohsin-junaid.blogspot.co.uk/2012/12/how-to-install-android-marketplace.html 
d) http://mohsin-junaid.blogspot.in/2012/12/how-to-download-multiple-android-apks.html

Thanks!
",15k,"
            11
        ","['\nFirst of all, Google Play\'s robots.txt does NOT disallow the pages with base ""/store/apps"".\nIf you want to crawl Google Play you would need to develop your own web crawler, parse the HTML page and extract the app meta-data you need (e.g. title, descriptions, price, etc). This topic has been covered in this other question. There are libraries helping with that, for instance:\n\nJava: https://jsoup.org\nPython: https://scrapy.org\n\nThe harder part is to ""find"" the app-pages to crawl. You could use 1) the Google Play Sitemap or 2) follow the app-links you find in every page you crawl as explained in the Link Extractor documentation (in case you plan to use Scrapy).\nAnother option is to use an open-source library based on ProtoBuf to fetch meta-data about an app, here the link to the project: https://code.google.com/archive/p/android-market-api.\nThis library fetches app meta-data from Google Play on behalf of a valid Google account, but also in this case you need a crawler to ""find"" which apps are available and schedule their meta-data retrieval. This other open-source project can help you with that: https://code.google.com/archive/p/android-marketplace-crawler.\nIf you don\'t want to implement all this by yourself, you could use a third-party managed service to access Android apps meta-data through a JSON-based API. For instance, 42matters.com (the company I work for) offers an API for both Android and iOS to retrieve apps\' meta-data, here more details:\nhttps://42matters.com/app-market-data\nIn order to get the Title, Icon, Description, Downloads for an app you can use the ""lookup"" endpoint as documented here:\nhttps://42matters.com/docs/app-market-data/android/apps/lookup\nThis is an example of the JSON response for the ""Angry Birds Space Premium"" app:\n{\n    ""package_name"": ""com.rovio.angrybirdsspace.premium"",\n    ""title"": ""Angry Birds Space Premium"",\n    ""description"": ""Play over 300 interstellar levels across 10 planets..."",\n    ""short_desc"": ""The #1 mobile game of all time blasts off into space!"",\n    ""rating"": 4.3046236038208,\n    ""category"": ""Arcade"",\n    ""cat_key"": ""GAME_ARCADE"",\n    ""cat_keys"": [\n        ""GAME_ARCADE"",\n        ""GAME"",\n        ""FAMILY_EDUCATION"",\n        ""FAMILY""\n    ],\n    ""price"": ""$1.15"",\n    ""downloads"": ""1,000,000 - 5,000,000"",\n    ""version"": ""2.2.1"",\n    ""content_rating"": ""Everyone"",\n    ""promo_video"": ""https://www.youtube.com/embed/g6AL9YqRHaI?ps=play&vq=large&rel=0&autohide=1&showinfo=0&autoplay=1"",\n    ""market_update"": ""2015-07-03T00:00:00+00:00"",\n    ""screenshots"": [\n        ""https://lh3.googleusercontent.com/ZmuBQzIy1G74coPrQ1R7fCeKdJmjTdpJhNrIHBOaFyM0N2EYdUPwZaQjnQUtiUDGmac=h310"",\n        ""https://lh3.googleusercontent.com/Xg2Aq70ZH0SnNhtSKH7xg9jCfisWgmmq3C7xQbx6YMhTVAIRqlRJeH8GYtjxapb_qR4=h310"",\n        ""https://lh3.googleusercontent.com/T4o5-2_UP82sj4fSSegbjrGmslNHlfvtEYuZacXMSOC55-7eyiKySw05lNF1QQGO2FeU=h310"",\n        ""https://lh3.googleusercontent.com/f2ennaLdivFu5cQQaVPKsRcWxB8FS5T4Bkoy3l0iPW9-GDDnTVRhvR5kz6l4m8FL1c8=h310"",\n        ""https://lh3.googleusercontent.com/H-9M03_-O9Df1nHr2-rUdjtk2aeBY3bAxnqSX3m2zh_aV8-K1t0qU1DxLXnK0GrDAw=h310""\n    ],\n    ""created"": ""2012-03-22T08:24:00+00:00"",\n    ""developer"": ""Rovio Entertainment Ltd."",\n    ""number_ratings"": 20812,\n    ""price_currency"": ""$"",\n    ""icon"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w300"",\n    ""icon_72"": ""https://lh3.ggpht.com/aQaIEGrmba1ENSEgUtArdm3yhJUug7BRWlu_WaspoJusZyHv1rjlWtYqe_qRjE_Kmh1E=w72"",\n    ""market_url"": ""https://play.google.com/store/apps/details?id=com.rovio.angrybirdsspace.premium&referrer=utm_source%3D42matters.com%26utm_medium%3Dapi""\n}\n\nI hope this helps, otherwise feel free to get in touch with me. I know this topic quite well and can point you in the right direction.\nRegards,\nAndrea\n', '\nI have did the job in Python before, what you need is a web auto test lib called selenium, it can execute Javascript code and return the result to Python, with Javascript, you can click the ""show more"" button by the program itself. And when you get all links for a single category page, you can get some info for the app. The simple demo here. Hope helpful.\n', '\nGoogle doesn\'t disallow crawling of /store/apps pages. \nThere is no mention about ""/store/apps"" in the robot.txt\nSee https://play.google.com/robots.txt\n']",https://stackoverflow.com/questions/17002181/crawling-the-google-play-store,web-crawler
crawling a html page using php?,"
This website lists over 250 courses in one list. I want to get the name of each course and insert that into my mysql database using php. The courses are listed like this:
<td> computer science</td>
<td> media studeies</td>
…

Is there a way to do that in PHP, instead of me  having a mad data entry nightmare?
",4k,"
            4
        ","['\nRegular expressions work well.\n$page = // get the page\n$page = preg_split(""/\\n/"", $page);\nfor ($text in $page) {\n    $matches = array();\n    preg_match(""/^<td>(.*)<\\/td>$/"", $text, $matches);\n    // insert $matches[1] into the database\n}\n\nSee the documentation for preg_match.\n', ""\nHow to parse HTML has been asked and answered countless times before. While (for your specific UseCase) Regular Expressions will work, it is - in general - better and more reliable to use a proper parser for this task. Below is how to do it with DOM:\n$dom = new DOMDocument;\n$dom->loadHTMLFile('http://courses.westminster.ac.uk/CourseList.aspx');\nforeach($dom->getElementsByTagName('td') as $title) {\n    echo $title->nodeValue;\n}\n\nFor inserting the data into MySql, you should use the mysqli extension. Examples are plentiful on StackOverflow. so please use the search function.\n"", '\nYou can use this HTML parsing php library to achieve this :http://simplehtmldom.sourceforge.net/\n', '\nI encountered the same problem.\nHere is a good class library called the html dom\nhttp://simplehtmldom.sourceforge.net/.\nThis like jquery\n', '\nJust for fun, here\'s a quick shell script to do the same thing.\ncurl http://courses.westminster.ac.uk/CourseList.aspx \\\n| sed \'/<td>\\(.*\\)<\\/td>/ { s/.*"">\\(.*\\)<\\/a>.*/\\1/; b }; d;\' \\\n| uniq > courses.txt\n\n']",https://stackoverflow.com/questions/3946506/crawling-a-html-page-using-php,web-crawler
Scrapy Linkextractor duplicating(?),"
I have the crawler implemented as below.
It is working and it would go through sites regulated under the link extractor.
Basically what I am trying to do is to extract information from different places in the page:
- href and text() under the class 'news' ( if exists)
- image url under the class 'think block' ( if exists)
I have three problems for my scrapy:
1) duplicating linkextractor
It seems that it will duplicate processed page.  ( I check against the export file and found that the same ~.img appeared many times while it is hardly possible)
And the fact is , for every page in the website, there are hyperlinks at the bottom that facilitate users to direct to the topic they are interested in, while my objective is to extract information from the topic's page ( here listed several passages's title under the same topic ) and the images found within a passage's page( you can arrive to the passage's page by clicking on the passage's title found at topic page).
I suspect link extractor would loop the same page over again in this case.
( maybe solve with depth_limit?)
2) Improving parse_item
I think it is quite not efficient for parse_item. How could I improve it? I need to extract information from different places in the web ( for sure it only extracts if  it exists).Beside, it looks like that the parse_item could only progress HkejImage but not HkejItem (again I checked with the output file). How should I tackle this?
3) I need the spiders to be able to read Chinese.
I am crawling a site in HK and it would be essential to be capable to read Chinese.
The site:

http://www1.hkej.com/dailynews/headline/article/1105148/IMF%E5%82%B3%E4%BF%83%E4%B8%AD%E5%9C%8B%E9%80%80%E5%87%BA%E6%95%91%E5%B8%82

As long as it belongs to 'dailynews', that's the thing I want.
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.http import Request, FormRequest
from scrapy.contrib.linkextractors import LinkExtractor
import items


class EconjournalSpider(CrawlSpider):
    name = ""econJournal""
    allowed_domains = [""hkej.com""]
    login_page = 'http://www.hkej.com/template/registration/jsp/login.jsp'
    start_urls =  'http://www.hkej.com/dailynews'

    rules=(Rule(LinkExtractor(allow=('dailynews', ),unique=True), callback='parse_item', follow =True),
           )


    def start_requests(self):
         yield Request(
         url=self.login_page,
         callback=self.login,
         dont_filter=True
         )
# name column
    def login(self, response):
        return FormRequest.from_response(response,
                    formdata={'name': 'users', 'password': 'my password'},
                    callback=self.check_login_response)

    def check_login_response(self, response):
        """"""Check the response returned by a login request to see if we are
        successfully logged in.
        """"""
        if ""username"" in response.body:       
            self.log(""\n\n\nSuccessfully logged in. Let's start crawling!\n\n\n"")
            return Request(url=self.start_urls)
        else:
            self.log(""\n\n\nYou are not logged in.\n\n\n"")
            # Something went wrong, we couldn't log in, so nothing happens

    def parse_item(self, response):
        hxs = Selector(response)
        news=hxs.xpath(""//div[@class='news']"")
        images=hxs.xpath('//p')

        for image in images:
            allimages=items.HKejImage()
            allimages['image'] = image.xpath('a/img[not(@data-original)]/@src').extract()
            yield allimages

        for new in news:
            allnews = items.HKejItem()
            allnews['news_title']=new.xpath('h2/@text()').extract()
            allnews['news_url'] = new.xpath('h2/@href').extract()
            yield allnews

Thank you very much and I would appreciate any help!
",2k,"
            3
        ","[""\nFirst, to set settings, make it on the settings.py file or you can specify the custom_settings parameter on the spider, like:\ncustom_settings = {\n    'DEPTH_LIMIT': 3,\n}\n\nThen, you have to make sure the spider is reaching the parse_item method (which I think it doesn't, haven't tested yet). And also you can't specify the callback and follow parameters on a rule, because they don't work together.\nFirst remove the follow on your rule, or add another rule, to check which links to follow, and which links to return as items.\nSecond on your parse_item method, you are getting incorrect xpath, to get all the images, maybe you could use something like:\nimages=hxs.xpath('//img')\n\nand then to get the image url:\nallimages['image'] = image.xpath('./@src').extract()\n\nfor the news, it looks like this could work:\nallnews['news_title']=new.xpath('.//a/text()').extract()\nallnews['news_url'] = new.xpath('.//a/@href').extract()\n\nNow, as and understand your problem, this isn't a Linkextractor duplicating error, but only poor rules specifications, also make sure you have valid xpath, because your question didn't indicate you needed xpath correction.\n""]",https://stackoverflow.com/questions/31630771/scrapy-linkextractor-duplicating,web-crawler
HtmlAgilityPack HtmlWeb.Load returning empty Document,"
I have been using HtmlAgilityPack for the last 2 months in a Web Crawler Application with no issues loading a webpage.
Now when I try to load a this particular webpage, the document OuterHtml is empty, so this test fails
var url = ""http://www.prettygreen.com/"";
var htmlWeb = new HtmlWeb();
var htmlDoc = htmlWeb.Load(url);
var outerHtml = htmlDoc.DocumentNode.OuterHtml;
Assert.AreNotEqual("""", pageHtml);

I can load another page from the site with no problems, such as setting
url = ""http://www.prettygreen.com/news/"";

In the past I once had an issue with encodings, I played around with htmlWeb.OverrideEncoding and htmlWeb.AutoDetectEncoding with no luck.  I have no idea what could be the issue here with this webpage.
",12k,"
            1
        ","['\nIt seems this website requires cookies to be enabled. So creating a cookie container for your web request should solve the issue:\nvar url = ""http://www.prettygreen.com/"";\nvar htmlWeb = new HtmlWeb();\nhtmlWeb.PreRequest += request =>\n    {\n        request.CookieContainer = new System.Net.CookieContainer();\n        return true;\n    };\nvar htmlDoc = htmlWeb.Load(url);\nvar outerHtml = htmlDoc.DocumentNode.OuterHtml;\nAssert.AreNotEqual("""", outerHtml);\n\n']",https://stackoverflow.com/questions/13400493/htmlagilitypack-htmlweb-load-returning-empty-document,web-crawler
Hide Email Address from Bots - Keep mailto:,"
tl;dr
Hide email address from bots without using scripts and maintain mailto: functionality. Method must also support screen-readers.

Summary

Email obfuscation without using scripts or contact forms

Email address needs to be completely visible to human viewers and maintain mailto: functionality

Email Address must not be in image form.

Email address must be ""completely"" hidden from spam-crawlers and spam-bots and any other harvester type



Desired Effect:

No scripts, please. There are no scripts used in the project and I'd like to keep it that way.

Email address is either displayed on the page or can be easily displayed after some sort of user interaction, like opening a modal.

The user can click on on the email address which in turn would trigger the mailto: functionality.

Clicking the email will open the user's email application.
In other words, mailto: functionality must work.

The email address in not visible or not identified as an email address to bots (This includes the page source)

I don't have an inbox that's full of spam



What does NOT Work

Adding a contact form - or anything similar - instead of the email address

I hate contact forms. I rarely fill up a contact form. If there's no email address, I look for a phone number, and if that's not there, I start looking for an alternative service. I would only fill up a contact form if I absolutely have to.

Replacing the address with an image of the address

This creates a HUGE disadvantage to someone using a screenreader (please remember the visually impaired in your future projects)
It also removes the mailto: functionality unless you make the image clickable and then add the mailto: functionality as the href for the link, but that defeats the purpose and now the email is visible to bots.

What might work:

Clever usage of pseudo-elements in CSS

Solutions that make use of base64 encoding

Breaking up the email address and spreading the parts across the document then putting them back together in a modal when the user clicks a button (This will probably involve multiple CSS classes and the usage of anchor tags)

Alterting html attributes via CSS


@MortezaAsadi gracefully brought up the possibility in the comments below. This is the link to the full - The article is from 2012:
What if We Could Use CSS to Alter HTML Attributes?

Other creative solutions that are beyond my scope of knowledge.


Similar Questions / Fixes

JavaScript: Protect your email address by Joe Maller

(This a great fix suggested by Joe Maller, it works well but it's script based. Here's what it looks like;


<SCRIPT TYPE=""text/javascript"">

  emailE = 'example.com'

  emailE = ('yourname' + '@' + emailE)

  document.write('<A href=""mailto:' + emailE + '"">' + emailE + '</a>')

</script>

<NOSCRIPT>

  Email address protected by JavaScript

</NOSCRIPT>



Looking for a PHP only email address obfuscator function
(A Clever solution using both PHP and CSS to first reverse the email using PHP then reverse it back with CSS) A very promising solution that Works great! But it's too easy to solve.

Is it worth obfuscating email addresses on the web these days?


(JavaScript fix)

Best way to obfuscate an e-mail address on a website?

The selected answer works. It actually works really well. It involves encoding the email as html entities. Can it be improved?
Here's what it looks like;


<A HREF=""mailto:

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;"">

&#121;&#111;&#117;&#114;&#110;&#097;&#109;&#101;&#064;&#100;&#111;&#109;&#097;&#105;&#110;&#046;&#099;&#111;&#109;

</A>



Does e-mail address obfuscation actually work?

(The selected answer to this SuperUser question is great and it presents a study of the amount of spam received by using different obfuscation methods.
It seems that manipulating the email address with CSS to make it rtl does work. This is the same method used in the first question I linked to in this section.
I am uncertain what effects adding mailto: functionality to the fix would have on the results.

There are also many other questions on SO which all have similar answers. I have not found anything that fits my desired effect


The Question:
Would it be possible to increase the efficiency (ie as little spam as possible) of the email obfuscation methods above by combining two or more of the fixes (or even adding new fixes) while:
A- Maintaining mailto: functionality; and
B- Supporting screen-readers

Many of the answers and comments below pose a very good question while indicating the impossibility of doing this without some sort of js
The question that's asked/implied is:

Why not use js?

The answer is that I am allergic to js
Joking aside though,
The three main reasons I asked this question are:

Contact forms are becoming more and more accepted as a replacement
for providing an email address - which they should not.

If it can be done without scripting then it should be done without
scripting.

Curiosity: (as I am in fact using one of the js fixes currently) I wanted to see if discussing the matter would lead to a better way of doing it.


",66k,"
            105
        ","['\nThe issue with your request is specifically the ""Supporting screen-readers"", as by definition screen readers are a ""bot"" of some sort. If a screen-reader needs to be able to interpret the email address, then a page-crawler would be able to interpret it as well.\nAlso, the point of the mailto attribute is to be the standard of how to do email addresses on the web. Asking if there is a second way to do that is sort of asking if there is a second standard.\nDoing it through scripts will still have the same issue as once the page is loaded, the script would have been run and the email address rendered in the DOM (unless you populate the email address on click or something). Either way, screen readers will still have issues with this since it\'s not already loaded.\nHonestly, just get an email service with a half decent spam filter and specify a default subject line that is easy for you to sort in your inbox.\n<a href=""mailto:no-one@example.com?subject=Something to filter on"">Email me</a>\n\nWhat you\'re asking for is if the standard has two ways to do something, one for bots and the other for non-bots. The answer is it doesn\'t, and you have to just fight the bots as best you can.\n', '\nDefeating email bots is a tough one. You may want to check out the Email Address Harvesting countermeasures section on Wikipedia.\nMy back-story is that I\'ve written a search bot. It crawled 105,000+ URLs during it\'s initial run many years ago. From what I\'ve learned from doing that is that web crawling bots literally see EVERYTHING that is text, which appears on a web page. Bots read everything except images.\nSpam can\'t be easily stopped via code for these reasons:\n\nCSS & JS are irrelevant when using the mailto: tag. Bots specifically look at HTML pages for that ""mailto:"" keyword. Everything from that colon to the next single quote or double quote (whichever comes first) is seen as an email address. HTML entity email addresses - like the example above - can be quickly translated using a reverse ASCII method/function. Running the JavaScript code snippet above, quickly turns the string which starts with: &#121;&#111;&#117;&#114;... into... yourname@example.com. (My search bot threw away hrefs with mailto:email addresses, as I wanted URLs for web pages & not email addresses.)\n\nIf a page crashes a bot, the bot author will tune the bot to fix the crash with that page in mind, so that the bot won\'t crash at that page again in the future. Thus making their bot smarter.\n\nBot authors can write bots, which generate all known variations of email addresses... without crawling pages & never using any starter email addresses. While it may not be feasible to do that, it\'s not inconceivable with today\'s high-core count CPUs (which are hyper-threaded & run at 4+ GHz), plus the availability of using distributed cloud-based computing & even super computers. It\'s conceivable that someone can now create a bot-farm to spam everyone, without knowing anyone\'s email address. 20 years ago, that would have been incomprehensible.\n\nFree email providers have had a history of selling their free user accounts to their advertisers. In the past, simply signing up for a free email account automatically guaranteed them a green light to start delivering spam to that email address... without ever using that email address online. I\'ve seen that happen multiple times, with famous company names. (I won\'t mention any names.)\n\nThe mailto: keyword is part of this IETF RFC, where browsers are built to automatically launch the default email clients, from links with that keyword in them. JavaScript has to be used to interrupt that application launching process, when it happens.\n\n\nI don\'t think it\'s possible to stop 100% of spam while using traditional email servers, without using filters on the email server and possibly using images.\nThere is one alternative... You can also build a chat-like email client, which runs internally on a website. It would be like Facebook\'s chat client. It\'s ""kind of like email"", but not really email. It\'s simply 1-to-1 instant messaging with an archiving feature... that auto-loads upon login. Since it has document attachment + link features, it works kind of like email... but without the spam. As long as you don\'t build an externally accessible API, then it\'s a closed system where people can\'t send spam into it.\nIf you\'re planning to stick with strictly traditional email, then your best bet may be to run something like Apache\'s SpamAssassin on a company\'s email server.\nYou can also try combining multiple strategies as you\'ve listed above, to make it harder for email harvesters to glean email addresses from your web pages. They won\'t stop 100% of the spam, 100% of the time... while also allowing 100% of the screen readers to work for blind visitors.\nYou\'ve created a really good starting look at what\'s wrong with traditional email! Kudos to you for that!\nA good screen reader is JAWS from Freedom Scientific. I\'ve used that before to listen to how my webpages are read by blind users. (If you hear a male voice reading both actions [like clicking on a link] & text, try changing 1 voice to female so that 1 voice reads actions & another reads text. That makes it easier to hear how the web page is read for the visually impared.)\nGood luck with your Email Address Harvesting countermeasure endeavours!\n', '\nHere is an approach that does make use of JavaScript, but with a rather small foot-print. It\'s also very ""ghetto"", and generally I would not recommend an approach with inline JS in the HTML except you have an extreme reluctance to use JS, at all.\n\n\n<a\n  href=""#""\n  data-contact=""bGUtZW1haWxAdGhlLWRvbWFpbi5jb20=""\n  data-subj=""QW4gQW1hemluZyBTdWJqZWN0""\n  onfocus=""this.href = \'mailto:\' + atob(this.dataset.contact) + \'?subject=\' + atob(this.dataset.subj || \'\')""\n  >\n  Send an email\n</a>\n\n\ndata-contact is the base64 encoded email address. And, data-subj is an optional base64 encoded subject.\nThe main challenge with doing this without JS is that CSS can\'t alter HTML attributes. (The article you linked is a ""pie-in-the-sky"" musing and does not have any bearing on what is possible today or in the near future.)\nThe HTML entities approach you mentioned, or some variation of it, is likely the simplest option that will have some efficacy. Additionally, the iframe approach is clever and the server redirect approach is pretty awesome. But, all three are vulnerable to bots:\n\nThe HTML entities just need to be converted (and detecting that is simple)\nThe document referenced by the iframe might simply be followed\nThe server redirect might simply be followed, as well\n\nWith the approach outlined above, the use of a base64 encoded email address in a data-contact attribute is very ""one-off"" – as long as the scraper is not specifically designed for your site, it should work.\n', '\nSimple + Lot of @ + Editable without tools\n\n\n<a href=""mailto:user@domain@@com""\r\n   onmouseover=""this.href=this.href.replace(\'@@\',\'.\')"">\r\n   Send email\r\n</a>\n\n\n', '\nHave you considered using google\'s recaptcha mailhide?\nhttps://www.google.com/recaptcha/admin#mailhide\nThe idea is that when a user clicks the checkbox (see nocaptcha below), the full e-mail address is displayed.\nWhile recaptcha is traditionally not only hard for screen readers but also humans as well, with the roleout of google\'s nocaptcha recaptcha which you can read about\nhere as they relate to accessibility tests. It appears to show promise with to screen readers as it renders as a traditional checkbox from their view.\n\nExample #1 - Not secure but for easy illustration of the idea\nHere is some code as an example without using mailhide but implementing something using recaptcha yourself: https://jsfiddle.net/43fad8pf/36/\n<div class=""container"">\n    <div id=""recaptcha""></div>\n</div>\n<div id=""email"">\n    Verify captcha to get e-mail\n</div>\n\nfunction createRecaptcha() {\n    grecaptcha.render(""recaptcha"", {sitekey: ""6LcgSAMTAAAAACc2C7rc6HB9ZmEX4SyB0bbAJvTG"", theme: ""light"", callback: showEmail});\n}\n createRecaptcha();\n\nfunction showEmail() {\n    // ideally you would do server side verification of the captcha and then the server would return the e-mail\n  document.getElementById(""email"").innerHTML = ""email@example.com"";\n}\n\nNote: In my example I have the e-mail in a JavaScript function. Ideally you would have the recaptcha validated on the server end, and return the e-mail, otherwise the bot can simply get it in the code.\nExample #2 - Server side validation and returning of e-mail\nIf we use an example more like this, we get additional security: https://designracy.com/recaptcha-using-ajax-php-and-jquery/\nfunction showEmail() {\n    /* Check if the captcha is complete */\n    if ($(""#g-recaptcha-response"").val()) {\n        $.ajax({\n            type: ‘POST’,\n            url: ""verify.php"", // The file we’re making the request to\n            dataType: ‘html’,\n            async: true,\n            data: {\n                captchaResponse: $(""#g-recaptcha-response"").val() // The generated response from the widget sent as a POST parameter\n        },\n        success: function (data) {\n            alert(""everything looks ok. Here is where we would take \'data\' which contains the e-mail and put it somewhere in the document"");\n        },\n        error: function (XMLHttpRequest, textStatus, errorThrown) {\n            alert(""You’re a bot"");\n        }\n    });\n} else {\n    alert(""Please fill the captcha!"");\n}\n});\n\nWhere verify.php is:\n$captcha = filter_input(INPUT_POST, ‘captchaResponse’); // get the captchaResponse parameter sent from our ajax\n\n/* Check if captcha is filled */\nif (!$captcha) {\n    http_response_code(401); // Return error code if there is no captcha\n}\n$response =     file_get_contents(""https://www.google.com/recaptcha/api/siteverify?secret=YOUR-SECRET-KEY-HERE&amp;amp;response="" . $captcha);\nif ($response . success == false) {\necho ‘SPAM’;\nhttp_response_code(401); // It’s SPAM! RETURN SOME KIND OF ERROR\n} else {\n// Everything is ok, should output this in json or something better, but this is an example\n    echo \'email@example.com\';\n}\n\n', '\nPeople who write scrapers want to make their scrapers as efficient as possible. Therefore, they won\'t download styles, scripts, and other external resources. There\'s no method that I know of to set a mailto link using CSS. In addition, you specifically said you didn\'t want to set the link using Javascript.\nIf you think about what other types of resources there are, there\'s also external documents (i.e. HTML documents using iframes). Almost no scrapers would bother downloading the contents of iframes. Therefore, you can simply do:\nindex.html:\n<iframe src=""frame.html"" style=""height: 1em; width: 100%; border: 0;""></iframe>\n\nframe.html:\nMy email is <a href=""mailto:me@example.com"" target=""_top"">me@example.com</a>\n\nTo human users, the iframe looks just like normal text. Iframes are inline and transparent by default, so we just need set its border and dimensions. You can\'t make the size of the iframe match its content\'s size without using Javascript, so the best we can do is giving it predefined dimensions.\n', ""\nFirst, I don't think doing anything with CSS will work. All bots (except Google's crawler) simply ignore all styling on websites. Any solution has to work with JS or server-side.\nA server-side solution could be making an <a> that links to a new tab, which simply redirects to the desired mailto:\nThat's all my ideas for now. Hope it helps.\n"", ""\nShort answer to fulfill all your requirements is that it's impossible\nSome of the script-based options answered here may work for certain bots, but you wanted no-script, so, no, you can't. \n"", '\nbased on the code of MaanooAk, here is my version:\n\n\n<a href=""mailto: Mike Myers""\nonclick=""this.href=this.href.replace(\' Mike \',\'MikeMy\'); this.href=this.href.replace(\'Myers\',\'ers@vwx.yz\')"">&#9993; Send Email</a>\n\n\nThe difference to MaanookAks version is, that on hover you don\'t see mailto: and a broken email adress but mailto: and the name of contact. And when you click on it, the name is replaced by the email adress.\nIn the code the email adress is splitted into two parts. Nowhere in the code the email adress is visible complete.\n', ""\nHere is my new solution for this. I first build the email adress string by addition of small pieces and then use this string also as title:\n\n\nadress = 'mailt' + 'o:MikeM' + 'yers@v' + 'wx.yz';\ndocument.getElementsByClassName('Email')[0].title = adress;\nfunction mail(){window.location.href = adress;}\n<a class='Email' onclick='mail()'>&#9993; Send Email</a>\n\n\nI use this in a footer of a website. Many pages with all the same footer.\n"", '\nPHP solution\nfunction printEmail($email){\n    $email = \'<a href=""mailto:\'.$email.\'"">\'.$email.\'</a>\';\n    $a = str_split($email);\n    return ""<script>document.write(\'"".implode(""\'+\'"",$a).""\');</script>"";\n}\n\nUse\necho printEmail(\'test@example.com\');\n\nResult\n<script>document.write(\'<\'+\'a\'+\' \'+\'h\'+\'r\'+\'e\'+\'f\'+\'=\'+\'""\'+\'m\'+\'a\'+\'i\'+\'l\'+\'t\'+\'o\'+\':\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'""\'+\'>\'+\'t\'+\'e\'+\'s\'+\'t\'+\'@\'+\'g\'+\'m\'+\'a\'+\'i\'+\'l\'+\'.\'+\'c\'+\'o\'+\'m\'+\'<\'+\'/\'+\'a\'+\'>\');</script>\n\nP.S. Requirement: user must have JavaScript enabled\n', '\nThe one method I found effective is using it with CSS like below:\n<a href=""mailto:myemail@ignore-domain.com"">myemail@<span style=""display:none;"">ignore-</span>example.com\nand then write a JavaScript to remove the ignoreme- word from the href=""mailto:..."" attribute with regex. This will hide email from bot as it will append ignore- word before real domain and this will work on screen reader and when user clicks on the link custom JS function will remove the ignore- word from href attribute so it will open the real email.\nThis method has been working very effectively for me till date. you can read more on this - http://techblog.tilllate.com/2008/07/20/ten-methods-to-obfuscate-e-mail-addresses-compared/\n']",https://stackoverflow.com/questions/41318987/hide-email-address-from-bots-keep-mailto,web-crawler
Designing a web crawler,"
I have come across an interview question ""If you were designing a web crawler, how would you avoid getting into infinite loops? "" and I am trying to answer it.
How does it all begin from the beginning.
Say Google started with some hub pages say hundreds of them (How these hub pages were found in the first place is a different sub-question).
As Google follows links from a page and so on, does it keep making  a hash table to make sure that it doesn't follow the earlier visited pages.
What if the same page has 2 names (URLs) say in these days when we have URL shorteners etc..
I have taken Google as an example. Though Google doesn't leak how its web crawler algorithms and page ranking etc work, but any guesses?
",46k,"
            74
        ","['\nIf you want to get a detailed answer take a look at section 3.8 this paper, which describes the URL-seen test of a modern scraper:\n\nIn the course of extracting links, any\nWeb crawler will encounter multiple\nlinks to the same document. To avoid\ndownloading and processing a document\nmultiple times, a URL-seen test must\nbe performed on each extracted link\nbefore adding it to the URL frontier.\n(An alternative design would be to\ninstead perform the URL-seen test when\nthe URL is removed from the frontier,\nbut this approach would result in a\nmuch larger frontier.)\nTo perform the\nURL-seen test, we store all of the\nURLs seen by Mercator in canonical\nform in a large table called the URL\nset. Again, there are too many entries\nfor them all to fit in memory, so like\nthe document fingerprint set, the URL\nset is stored mostly on disk.\nTo save\nspace, we do not store the textual\nrepresentation of each URL in the URL\nset, but rather a fixed-sized\nchecksum. Unlike the fingerprints\npresented to the content-seen test’s\ndocument fingerprint set, the stream\nof URLs tested against the URL set has\na non-trivial amount of locality. To\nreduce the number of operations on the\nbacking disk file, we therefore keep\nan in-memory cache of popular URLs.\nThe intuition for this cache is that\nlinks to some URLs are quite common,\nso caching the popular ones in memory\nwill lead to a high in-memory hit\nrate.\nIn fact, using an in-memory\ncache of 2^18 entries and the LRU-like\nclock replacement policy, we achieve\nan overall hit rate on the in-memory\ncache of 66.2%, and a hit rate of 9.5%\non the table of recently-added URLs,\nfor a net hit rate of 75.7%. Moreover,\nof the 24.3% of requests that miss in\nboth the cache of popular URLs and the\ntable of recently-added URLs, about\n1=3 produce hits on the buffer in our\nrandom access file implementation,\nwhich also resides in user-space. The\nnet result of all this buffering is\nthat each membership test we perform\non the URL set results in an average\nof 0.16 seek and 0.17 read kernel\ncalls (some fraction of which are\nserved out of the kernel’s file system\nbuffers). So, each URL set membership\ntest induces one-sixth as many kernel\ncalls as a membership test on the\ndocument fingerprint set. These\nsavings are purely due to the amount\nof URL locality (i.e., repetition of\npopular URLs) inherent in the stream\nof URLs encountered during a crawl.\n\nBasically they hash all of the URLs with a hashing function that guarantees unique hashes for each URL and due to the locality of URLs, it becomes very easy to find URLs. Google even open-sourced their hashing function: CityHash\nWARNING!\nThey might also be talking about bot traps!!! A bot trap is a section of a page that keeps generating new links with unique URLs and you will essentially get trapped in an ""infinite loop"" by following the links that are being served by that page. This is not exactly a loop, because a loop would be the result of visiting the same URL, but it\'s an infinite chain of URLs which you should avoid crawling.\nUpdate 12/13/2012- the day after the world was supposed to end :)\nPer Fr0zenFyr\'s comment: if one uses the AOPIC algorithm for selecting pages, then it\'s fairly easy to avoid bot-traps of the infinite loop kind. Here is a summary of how AOPIC works:\n\nGet a set of N seed pages.\nAllocate X amount of credit to each page, such that each page has X/N credit (i.e. equal amount of credit) before crawling has started.\nSelect a page P, where the P has the highest amount of credit (or if all pages have the same amount of credit, then crawl a random page).\nCrawl page P (let\'s say that P had 100 credits when it was crawled).\nExtract all the links from page P (let\'s say there are 10 of them).\nSet the credits of P to 0.\nTake a 10% ""tax"" and allocate it to a Lambda page.\nAllocate an equal amount of credits each link found on page P from P\'s original credit - the tax: so (100 (P credits) - 10 (10% tax))/10 (links) = 9 credits per each link.\nRepeat from step 3.\n\nSince the Lambda page continuously collects tax, eventually it will be the page with the largest amount of credit and we\'ll have to ""crawl"" it. I say ""crawl"" in quotes, because we don\'t actually make an HTTP request for the Lambda page, we just take its credits and distribute them equally to all of the pages in our database.\nSince bot traps only give internal links credits and they rarely get credit from the outside, they will continually leak credits (from taxation) to the Lambda page. The Lambda page will distribute that credits out to all of the pages in the database evenly and upon each cycle the bot trap page will lose more and more credits, until it has so little credits that it almost never gets crawled again. This will not happen with good pages, because they often get credits from back-links found on other pages. This also results in a dynamic page rank and what you will notice is that any time you take a snapshot of your database, order the pages by the amount of credits they have, then they will most likely be ordered roughly according to their true page rank.\nThis only avoid bot traps of the infinite-loop kind, but there are many other bot traps which you should watch out for and there are ways to get around them too.\n', ""\nWhile everybody here already suggested how to create your web crawler, here is how how Google ranks pages.\nGoogle gives each page a rank based on the number of callback links (how many links on other websites point to a specific website/page). This is called relevance score. This is based on the fact that if a page has many other pages link to it, it's probably an important page.\nEach site/page is viewed as a node in a graph. Links to other pages are directed edges. A degree of a vertex is defined as the number of incoming edges. Nodes with a higher number of incoming edges are ranked higher.\nHere's how the PageRank is determined. Suppose that page Pj has Lj links. If one of those links is to page Pi, then Pj will pass on 1/Lj of its importance to Pi. The importance ranking of Pi is then the sum of all the contributions made by pages linking to it. So if we denote the set of pages linking to Pi by Bi, then we have this formula:\nImportance(Pi)= sum( Importance(Pj)/Lj ) for all links from Pi to Bi\n\nThe ranks are placed in a matrix called hyperlink matrix: H[i,j]\nA row in this matrix is either 0, or 1/Lj if there is a link from Pi to Bi. Another property of this matrix is that if we sum all rows in a column we get 1. \nNow we need multiply this matrix by an Eigen vector, named I (with eigen value 1) such that:\nI = H*I\n\nNow we start iterating: IH, IIH, IIIH .... I^k *H until the solution converges. ie we get pretty much the same numbers in the matrix in step k and k+1.\nNow whatever is left in the I vector is the importance of each page.\nFor a simple class homework example see http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html\n\nAs for solving the duplicate issue in your interview question, do a checksum on the entire page and use either that or a bash of the checksum as your key in a map to keep track of visited pages.\n\n"", ""\nDepends on how deep their question was intended to be.  If they were just trying to avoid following the same links back and forth, then hashing the URL's would be sufficient.\nWhat about content that has literally thousands of URL's that lead to the same content?  Like a QueryString parameter that doesn't affect anything, but can have an infinite number of iterations.  I suppose you could hash the contents of the page as well and compare URL's to see if they are similar to catch content that is identified by multiple URL's.  See for example, Bot Traps mentioned in @Lirik's post.\n"", ""\nYou'd have to have some sort of hash table to store the results in, you'd just have to check it before each page load.   \n"", '\nThe problem here is not to crawl duplicated URLS, wich is resolved by a index using a hash obtained from urls.  The problem is to crawl DUPLICATED CONTENT.  Each url of a ""Crawler Trap"" is different (year, day, SessionID...).\nThere is not a ""perfect"" solution... but you can use some of this strategies:\n• Keep a field of wich level the url is inside the website. For each cicle of getting urls from a page, increase the level. It will be like a tree.  You can stop to crawl at certain level, like 10 (i think google use this). \n• You can try to create a kind of HASH wich can be compared to find similar documents, since you cant compare with each document in your database. There are SimHash from google, but i could not find any implementation to use. Then i´ve created my own. My hash count low and high frequency characters inside the html code and generate a 20bytes hash, wich is compared with a small cache of last crawled pages inside a AVLTree with an NearNeighbors search with some tolerance (about 2). You cant use any reference to characters locations in this hash. After ""recognize"" the trap, you can record the url pattern of the duplicate content and start to ignore pages with that too.\n• Like google, you can create a ranking to each website and ""trust"" more in one than others.\n', '\nThe web crawler is a computer program which used to collect/crawling following key values(HREF links, Image links, Meta Data .etc) from given website URL. It is designed like intelligent to follow different HREF links which are already fetched from the previous URL, so in this way, Crawler can jump from one website to other websites. Usually, it called as a Web spider or Web Bot. This mechanism always acts as a backbone of the Web search engine.\nPlease find the source code from my tech blog - http://www.algonuts.info/how-to-built-a-simple-web-crawler-in-php.html\n<?php\nclass webCrawler\n{\n    public $siteURL;\n    public $error;\n\n    function __construct()\n    {\n        $this->siteURL = """";\n        $this->error = """";\n    }\n\n    function parser()   \n    {\n        global $hrefTag,$hrefTagCountStart,$hrefTagCountFinal,$hrefTagLengthStart,$hrefTagLengthFinal,$hrefTagPointer;\n        global $imgTag,$imgTagCountStart,$imgTagCountFinal,$imgTagLengthStart,$imgTagLengthFinal,$imgTagPointer;\n        global $Url_Extensions,$Document_Extensions,$Image_Extensions,$crawlOptions;\n\n        $dotCount = 0;\n        $slashCount = 0;\n        $singleSlashCount = 0;\n        $doubleSlashCount = 0;\n        $parentDirectoryCount = 0;\n\n        $linkBuffer = array();\n\n        if(($url = trim($this->siteURL)) != """")\n        {\n            $crawlURL = rtrim($url,""/"");\n            if(($directoryURL = dirname($crawlURL)) == ""http:"")\n            {   $directoryURL = $crawlURL;  }\n            $urlParser = preg_split(""/\\//"",$crawlURL);\n\n            //-- Curl Start --\n            $curlObject = curl_init($crawlURL);\n            curl_setopt_array($curlObject,$crawlOptions);\n            $webPageContent = curl_exec($curlObject);\n            $errorNumber = curl_errno($curlObject);\n            curl_close($curlObject);\n            //-- Curl End --\n\n            if($errorNumber == 0)\n            {\n                $webPageCounter = 0;\n                $webPageLength = strlen($webPageContent);\n                while($webPageCounter < $webPageLength)\n                {\n                    $character = $webPageContent[$webPageCounter];\n                    if($character == """")\n                    {   \n                        $webPageCounter++;  \n                        continue;\n                    }\n                    $character = strtolower($character);\n                    //-- Href Filter Start --\n                    if($hrefTagPointer[$hrefTagLengthStart] == $character)\n                    {\n                        $hrefTagLengthStart++;\n                        if($hrefTagLengthStart == $hrefTagLengthFinal)\n                        {\n                            $hrefTagCountStart++;\n                            if($hrefTagCountStart == $hrefTagCountFinal)\n                            {\n                                if($hrefURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $hrefURL = ""http://"".$hrefURL;  }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $hrefURL = $tempString."""".$hrefURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $hrefURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$hrefURL;  }\n                                    }\n                                    $host = """";\n                                    $hrefURL = urldecode($hrefURL);\n                                    $hrefURL = rtrim($hrefURL,""/"");\n                                    if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($hrefURL);\n                                        if(isset($dump[""host""]))\n                                        {   $host = trim(strtolower($dump[""host""]));    }\n                                    }\n                                    else\n                                    {\n                                        $hrefURL = $directoryURL.""/"".$hrefURL;\n                                        if(filter_var($hrefURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($hrefURL);    \n                                            if(isset($dump[""host""]))\n                                            {   $host = trim(strtolower($dump[""host""]));    }\n                                        }\n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($hrefURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($hrefURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($hrefURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $hrefURL;   }\n                                            if($type == ""image"" && !in_array($hrefURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $hrefURL;    }\n                                            if($type == ""document"" && !in_array($hrefURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $hrefURL; }\n                                            if($type == ""unknown"" && !in_array($hrefURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $hrefURL;  }\n                                        }\n                                    }\n                                }\n                                $hrefTagCountStart = 0;\n                            }\n                            if($hrefTagCountStart == 3)\n                            {\n                                $hrefURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($hrefURL != """")\n                                            {   $hrefURL .= $character; }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $hrefURL .= $character; }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $hrefTagLengthStart = 0;\n                            $hrefTagLengthFinal = strlen($hrefTag[$hrefTagCountStart]);\n                            $hrefTagPointer =& $hrefTag[$hrefTagCountStart];\n                        }\n                    }\n                    else\n                    {   $hrefTagLengthStart = 0;    }\n                    //-- Href Filter End --\n                    //-- Image Filter Start --\n                    if($imgTagPointer[$imgTagLengthStart] == $character)\n                    {\n                        $imgTagLengthStart++;\n                        if($imgTagLengthStart == $imgTagLengthFinal)\n                        {\n                            $imgTagCountStart++;\n                            if($imgTagCountStart == $imgTagCountFinal)\n                            {\n                                if($imgURL != """")\n                                {\n                                    if($parentDirectoryCount >= 1 || $singleSlashCount >= 1 || $doubleSlashCount >= 1)\n                                    {\n                                        if($doubleSlashCount >= 1)\n                                        {   $imgURL = ""http://"".$imgURL;    }\n                                        else if($parentDirectoryCount >= 1)\n                                        {\n                                            $tempData = 0;\n                                            $tempString = """";\n                                            $tempTotal = count($urlParser) - $parentDirectoryCount;\n                                            while($tempData < $tempTotal)\n                                            {\n                                                $tempString .= $urlParser[$tempData].""/"";\n                                                $tempData++;\n                                            }\n                                            $imgURL = $tempString."""".$imgURL;\n                                        }\n                                        else if($singleSlashCount >= 1)\n                                        {   $imgURL = $urlParser[0].""/"".$urlParser[1].""/"".$urlParser[2].""/"".$imgURL;    }\n                                    }\n                                    $host = """";\n                                    $imgURL = urldecode($imgURL);\n                                    $imgURL = rtrim($imgURL,""/"");\n                                    if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                    {   \n                                        $dump = parse_url($imgURL); \n                                        $host = trim(strtolower($dump[""host""]));\n                                    }\n                                    else\n                                    {\n                                        $imgURL = $directoryURL.""/"".$imgURL;\n                                        if(filter_var($imgURL,FILTER_VALIDATE_URL) == true)\n                                        {   \n                                            $dump = parse_url($imgURL); \n                                            $host = trim(strtolower($dump[""host""]));\n                                        }   \n                                    }\n                                    if($host != """")\n                                    {\n                                        $extension = pathinfo($imgURL,PATHINFO_EXTENSION);\n                                        if($extension != """")\n                                        {\n                                            $tempBuffer ="""";\n                                            $extensionlength = strlen($extension);\n                                            for($tempData = 0; $tempData < $extensionlength; $tempData++)\n                                            {\n                                                if($extension[$tempData] != ""?"")\n                                                {   \n                                                    $tempBuffer = $tempBuffer.$extension[$tempData];\n                                                    continue;\n                                                }\n                                                else\n                                                {\n                                                    $extension = trim($tempBuffer);\n                                                    break;\n                                                }\n                                            }\n                                            if(in_array($extension,$Url_Extensions))\n                                            {   $type = ""domain"";   }\n                                            else if(in_array($extension,$Image_Extensions))\n                                            {   $type = ""image"";    }\n                                            else if(in_array($extension,$Document_Extensions))\n                                            {   $type = ""document""; }\n                                            else\n                                            {   $type = ""unknown"";  }\n                                        }\n                                        else\n                                        {   $type = ""domain"";   }\n\n                                        if($imgURL != """")\n                                        {\n                                            if($type == ""domain"" && !in_array($imgURL,$this->linkBuffer[""domain""]))\n                                            {   $this->linkBuffer[""domain""][] = $imgURL;    }\n                                            if($type == ""image"" && !in_array($imgURL,$this->linkBuffer[""image""]))\n                                            {   $this->linkBuffer[""image""][] = $imgURL; }\n                                            if($type == ""document"" && !in_array($imgURL,$this->linkBuffer[""document""]))\n                                            {   $this->linkBuffer[""document""][] = $imgURL;  }\n                                            if($type == ""unknown"" && !in_array($imgURL,$this->linkBuffer[""unknown""]))\n                                            {   $this->linkBuffer[""unknown""][] = $imgURL;   }\n                                        }\n                                    }\n                                }\n                                $imgTagCountStart = 0;\n                            }\n                            if($imgTagCountStart == 3)\n                            {\n                                $imgURL = """";\n                                $dotCount = 0;\n                                $slashCount = 0;\n                                $singleSlashCount = 0;\n                                $doubleSlashCount = 0;\n                                $parentDirectoryCount = 0;\n                                $webPageCounter++;\n                                while($webPageCounter < $webPageLength)\n                                {\n                                    $character = $webPageContent[$webPageCounter];\n                                    if($character == """")\n                                    {   \n                                        $webPageCounter++;  \n                                        continue;\n                                    }\n                                    if($character == ""\\"""" || $character == ""\'"")\n                                    {\n                                        $webPageCounter++;\n                                        while($webPageCounter < $webPageLength)\n                                        {\n                                            $character = $webPageContent[$webPageCounter];\n                                            if($character == """")\n                                            {   \n                                                $webPageCounter++;  \n                                                continue;\n                                            }\n                                            if($character == ""\\"""" || $character == ""\'"" || $character == ""#"")\n                                            {   \n                                                $webPageCounter--;  \n                                                break;  \n                                            }\n                                            else if($imgURL != """")\n                                            {   $imgURL .= $character;  }\n                                            else if($character == ""."" || $character == ""/"")\n                                            {\n                                                if($character == ""."")\n                                                {\n                                                    $dotCount++;\n                                                    $slashCount = 0;\n                                                }\n                                                else if($character == ""/"")\n                                                {\n                                                    $slashCount++;\n                                                    if($dotCount == 2 && $slashCount == 1)\n                                                    $parentDirectoryCount++;\n                                                    else if($dotCount == 0 && $slashCount == 1)\n                                                    $singleSlashCount++;\n                                                    else if($dotCount == 0 && $slashCount == 2)\n                                                    $doubleSlashCount++;\n                                                    $dotCount = 0;\n                                                }\n                                            }\n                                            else\n                                            {   $imgURL .= $character;  }\n                                            $webPageCounter++;\n                                        }\n                                        break;\n                                    }\n                                    $webPageCounter++;\n                                }\n                            }\n                            $imgTagLengthStart = 0;\n                            $imgTagLengthFinal = strlen($imgTag[$imgTagCountStart]);\n                            $imgTagPointer =& $imgTag[$imgTagCountStart];\n                        }\n                    }\n                    else\n                    {   $imgTagLengthStart = 0; }\n                    //-- Image Filter End --\n                    $webPageCounter++;\n                }\n            }\n            else\n            {   $this->error = ""Unable to proceed, permission denied"";  }\n        }\n        else\n        {   $this->error = ""Please enter url"";  }\n\n        if($this->error != """")\n        {   $this->linkBuffer[""error""] = $this->error;  }\n\n        return $this->linkBuffer;\n    }   \n}\n?>\n\n', ""\nWell the web is basically a directed graph, so you can construct a graph out of the urls and then do a BFS or DFS traversal while marking the visited nodes so you don't visit the same page twice. \n"", '\nThis is a web crawler example. Which can be used to collect mac Addresses for mac spoofing. \n#!/usr/bin/env python\n\nimport sys\nimport os\nimport urlparse\nimport urllib\nfrom bs4 import BeautifulSoup\n\ndef mac_addr_str(f_data):\nglobal fptr\nglobal mac_list\nword_array = f_data.split("" "")\n\n    for word in word_array:\n        if len(word) == 17 and \':\' in word[2] and \':\' in word[5] and \':\' in word[8] and \':\' in word[11] and \':\' in word[14]:\n            if word not in mac_list:\n                mac_list.append(word)\n                fptr.writelines(word +""\\n"")\n                print word\n\n\n\nurl = ""http://stackoverflow.com/questions/tagged/mac-address""\n\nurl_list = [url]\nvisited = [url]\npwd = os.getcwd();\npwd = pwd + ""/internet_mac.txt"";\n\nfptr = open(pwd, ""a"")\nmac_list = []\n\nwhile len(url_list) > 0:\n    try:\n        htmltext = urllib.urlopen(url_list[0]).read()\n    except:\n        url_list[0]\n    mac_addr_str(htmltext)\n    soup = BeautifulSoup(htmltext)\n    url_list.pop(0)\n    for tag in soup.findAll(\'a\',href=True):\n        tag[\'href\'] = urlparse.urljoin(url,tag[\'href\'])\n        if url in tag[\'href\'] and tag[\'href\'] not in visited:\n            url_list.append(tag[\'href\'])\n            visited.append(tag[\'href\'])\n\nChange the url to crawl more sites......good luck\n']",https://stackoverflow.com/questions/5834808/designing-a-web-crawler,web-crawler
Scrapy Python Set up User Agent,"
I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code:
[settings]
default = myproject.settings
USER_AGENT = ""Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36""


[deploy]
#url = http://localhost:6800/
project = myproject

But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one ""Scrapy/0.18.2 (+http://scrapy.org)"". 
Can any one explain what I have done wrong. 
Note:
(1). It works when I tried to override the user agent globally: 
scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=""Mozilla....""

(2). When I remove the line ""default = myproject.setting"" from the configuration file, and run scrapy crawl myproject.com, it says ""cannot find spider.."", so I feel like the default setting should not be removed in this case.
Thanks a lot for the help in advance.                            
",53k,"
            41
        ","['\nMove your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py\n', ""\nJust in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell...\n$ scrapy crawl myproject\n\nBut insted you use CrawlerProcess() or CrawlerRunner()...\nprocess = CrawlerProcess()\n\nor \nprocess = CrawlerRunner()\n\nthen the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. \nLike this...\n    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n\n""]",https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent,web-crawler
Difference between find and filter in jquery,"
I'm working on fetching data from wiki pages. I'm using a combination of php and jquery to do this. First I am using curl in php to fetch page contents and echoing the content. The filename is content.php:
$url = $_GET['url'];
$url = trim($url,"" "");
$url = urldecode($url);
$url = str_replace("" "",""%20"",$url);

echo ""<a class='urlmax'>"".$_GET['title'].""</a>"";
echo crawl($url);

Then jQuery is used to find the matched elements. 
$.get(""content.php"",{url:""http://en.wikipedia.org/w/index.php?action=render&title=""+str_replace("" "",""_"",data[x]),title:str_replace("" "",""_"",data[x])},function(hdata){
                        var imgs = $(hdata).find('a.image img');
                        var ent = $(hdata).filter('a.urlmax');


                        ent = $(ent[0]).text();


});

I was able to successfully get images but for the variable ent when I use find instead of filter, it's returning an empty array. Only filter is working. Why is this?
Edit: I know the basic difference between find and filter. Here both the a.image img and a.urlmax are descendats of the hdata. Then why find does not work on a.urlmax. Not a.urlmax alone it's not working on any other class or id
",26k,"
            29
        ","[""\n.find()\nhttp://api.jquery.com/find/\n\nGet the descendants of each element in the current set of matched elements, filtered by a selector, jQuery object, or element.\n\nFilter, on the other hand, works on the currently matched elements. That's why filter worked but find did not (you needed to look at the current element).\n"", '\nfilter will select subset of element from the selected element\nfind will select descendent/children of selected element\nTo make it more clear filter will search through all element whereas find will search only in the descendent list\n', '\n.find()\nIt will returns descendant elements of the selected element.\nExemple (jsfiddle):\n    <style>\n      .Cell{\n         margin: 15px;\n         width: 400px;\n         border: 2px solid lightgrey;\n    \n      }\n      .Cell * {\n         display: block;\n         border: 2px solid lightgrey;\n         color: lightgrey;\n         padding: 5px;\n         margin: 10px;\n    }\n    </style>\n\n    <div class=\'Cell Plus\'>div (1)\n      <div class=\'Plus\'>Child</div>\n    </div>\n    \n    <div class=\'Cell Plus\'>div (2)\n      <div class=\'Plus\'>Child</div>\n    </div>\n\nJS:\n$("".Cell"").find("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n\n.filter()\nIt will returns elements that match a certain criteria.\nUsing the same html above:\n$("".Cell"").filter("".Plus"").css({""color"": ""red"", ""border"": ""2px solid red""});\n\nResult:\n\n']",https://stackoverflow.com/questions/10378757/difference-between-find-and-filter-in-jquery,web-crawler
What are some good Ruby-based web crawlers? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I am looking at writing my own, but I am wondering if there are any good web crawlers out there which are written in Ruby.
Short of a full-blown web crawler, any gems that might be helpful in building a web crawler would be useful. I know this part of the question is touched upon in a couple of places, but a list of gems applicable to building a web crawler would be a great resource as well.
",22k,"
            21
        ","[""\nI used to write spiders, page scrapers and site analyzers for my job, and still write them periodically to scratch some itch I get.\nRuby has some excellent gems to make it easy:\n\nNokogiri is my #1 choice for the HTML parser. I used to use Hpricot, but found some sites that made it explode in flames. I switched to Nokogiri afterwards and have been very happy with it. I regularly use it for parsing HTML, RDF/RSS/Atom and XML. Ox looks interesting too, so that might be another candidate, though I find searching the DOM a lot easier than trying to walk through a big hash, such as what is returned by Ox.\n\nOpenURI is good as a simple HTTP client, but it can get in the way when you want to do more complex things or need to have multiple requests firing at once. I'd recommend looking at HTTPClient or Typhoeus with Hydra for modest to heavyweight jobs. Curb is good too, because it uses the cURL library, but the interface isn't as intuitive to me. It's worth looking at though. HTTPclient is also worth looking at, but I lean toward the previously mentioned ones.\nNote: OpenURI has some flaws and vulnerabilities that can affect unsuspecting programmers so it's fallen out of favor somewhat. RestClient is a very worthy successor.\n\nYou'll need a backing database, and some way to talk to it. This isn't a task for Rails per se, but you could use ActiveRecord, detached from Rails, to talk to the database. I've done that a couple times and it works all right. Instead, I really like Sequel for my ORM. It's very flexible in how it lets you talk to the database, from using straight SQL to using Sequel's ability to programmatically build a query, to modeling the database and using migrations. Once you have the database built, you could use Rails to act as a front-end to the data though.\n\nIf you are going to navigate sites in any way beyond simply grabbing pages and following links, you'll want to look at Mechanize. It makes it easy to fill out forms and submit pages. As an added bonus, you can grab the content of a page as a Nokogiri HTML document and parse away using Nokogiri's multitude of tricks.\n\nFor massaging/mangling URLs I really like Addressable::URI. It's more full-featured than the built-in URI module. One thing that URI does that's nice is it has the URI#extract method to scan a string for URLs. If that string happened to be the body of a web page it would be an alternate way of locating links, but its downside is you'll also get links to images, videos, ads, etc., and you'll have to filter those out, probably resulting in more work than if you use a parser and look for <a> tags exclusively. For that matter, Mechanize also has the links method which returns all the links in a page, but you'll still have to filter them to determine whether you want to follow or ignore them.\n\nIf you think you'll need to deal with Javascript manipulated pages, or pages that get their content dynamically from AJAX, you should look into using one of the WATIR variants. There are flavors for the different browsers on different OSes, such as Firewatir, Safariwatir and Operawatir, so you'll have to figure out what works for you.\n\nYou do NOT want to rely on keeping your list of URLs to visit, or visited URLs, in memory. Design a database schema and store that information there. Spend some time up front designing the schema, thinking about what things you'll want to know as you collect links on a site. SQLite3, MySQL and Postgres are all excellent choices, depending on how big you think your database needs will be. One of my site analyzers was custom designed to help us recommend SEO changes for a Fortune 50 company. It ran for over three weeks covering about twenty different sites before we had enough data and stopped it. Imagine what would have happened if we had a power-outage and all that data went in the bit-bucket.\n\n\nAfter all that you'll want to also make your code be aware of proper spidering etiquette: What are the key considerations when creating a web crawler?\n"", '\nI am building wombat, a Ruby DSL to crawl web pages and extract content. Check it out on github https://github.com/felipecsl/wombat\nIt is still in an early stage but is already functional with basic functionality. More stuff will be added really soon.\n', '\nSo you want a good Ruby-based web crawler?\nTry spider or anemone. Both have solid usage according to RubyGems download counts.\nThe other answers, so far, are detailed and helpful but they don\'t have a laser-like focus on the question, which asks for ruby libraries for web crawlers. It would seem that this distinction can get muddled: see my answer to ""Crawling vs. Web-Scraping?""\n', '\nTin Man\'s comprehensive list  is good but partly outdated for me.\nMost websites my customers deal with are heavily AJAX/Javascript dependent.\nI\'ve been using Watir / watir-webdriver / selenium for a few years too, but the overhead of having to load up a hidden web browser on the backend to render that DOM stuff just isn\'t viable, let alone that all this time they still haven\'t implemented a useable ""browser session reuse"" to let new code execution reuse an old browser in memory for this purpose, shooting down tickets that might have worked their way up the API layers eventually. (refering to https://code.google.com/p/selenium/issues/detail?id=18 ) **\nhttps://rubygems.org/gems/phantomjs\nis what we\'re migrating new projects over to now, to let the necessary data get rendered without even any sort of invisible Xvfb memory & CPU heavy web browser.\n** Alternative approaches also failed to pan out:\n\nhow to serialize an object using TCPServer inside?\nCan a watir browser object be re-used in a later Ruby process?\n\n', '\nIf you don\'t want to write your own, then use any ordinary web crawler.  There are dozens out there.\nIf you do want to write your own, then write your own. A web crawler isn\'t exactly a complicated activity, it consists of:\n\nDownloading a website.\nLocating URLs in that website, filtered however you dang well please.\nFor each URL in that website, repeat step 1.\n\nOh, and this seems to be a duplicate of ""Web crawler in ruby"".\n']",https://stackoverflow.com/questions/4981379/what-are-some-good-ruby-based-web-crawlers,web-crawler
Scrapy - how to identify already scraped urls,"
Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on SgmlLinkExtractor.
",10k,"
            15
        ","[""\nYou can actually do this quite easily with the scrapy snippet located here: http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/\nTo use it, copy the code from the link and put it into some file in your scrapy project. \nTo reference it, add a line in your settings.py to reference it:\nSPIDER_MIDDLEWARES = { 'project.middlewares.ignore.IgnoreVisitedItems': 560 }\n\nThe specifics on WHY you pick the number that you do can be read up here: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nFinally, you'll need to modify your items.py so that each item class has the following fields:\nvisit_id = Field()\nvisit_status = Field()\n\nAnd I think that's it.  The next time you run your spider it should automatically try to start avoiding the same sites.  \nGood luck!\n"", '\nThis is straight forward. Maintain all your previously crawled urls in python dict. So when you try to try them next time, see if that url is there in the dict. else crawl.\ndef load_urls(prev_urls):\n    prev = dict()\n    for url in prev_urls:\n        prev[url] = True\n    return prev\n\ndef fresh_crawl(prev_urls, new_urls):\n    for url in new_urls:\n        if url not in prev_urls:\n            crawl(url)\n    return\n\ndef main():\n    purls = load_urls(prev_urls)\n    fresh_crawl(purls, nurls)\n    return\n\nThe above code was typed in SO text editor aka browser. Might have syntax errors. You might also need to make a few changes. But the logic is there...\nNOTE: But beware that some websites constantly keep changing their content. So sometimes you might have to recrawl a particular webpage (i.e. same url) just to get the updated content.\n', ""\nI think jama22's answer is a little incomplete. \nIn the snippet if self.FILTER_VISITED in x.meta:, you can see that you require FILTER_VISITED in your Request instance in order for that request to be ignored. This is to ensure that you can differentiate between links that you want to traverse and move around and item links that well, you don't want to see again.\n"", '\nScrapy can auto-filter urls which are scraped, isn\'t it? Some different urls point to the same page will not be filtered, such as ""www.xxx.com/home/"" and ""www.xxx.com/home/index.html"".\n', ""\nFor today (2019), this post is the best answer for this problem. \nhttps://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016\nIt's a lib to handle MIDDLEWARES automatcally.\nHope to help someone. I've spent a lot of time seaching for this.\n""]",https://stackoverflow.com/questions/3871613/scrapy-how-to-identify-already-scraped-urls,web-crawler
Simple web crawler in C#,"
I have created a simple web crawler but I want to add the recursion function so that every page that is opened I can get the URLs in this page, but I have no idea how I can do that and I want also to include threads to make it faster.
Here is my code
namespace Crawler
{
    public partial class Form1 : Form
    {
        String Rstring;

        public Form1()
        {
            InitializeComponent();
        }

        private void button1_Click(object sender, EventArgs e)
        {
            
            WebRequest myWebRequest;
            WebResponse myWebResponse;
            String URL = textBox1.Text;

            myWebRequest =  WebRequest.Create(URL);
            myWebResponse = myWebRequest.GetResponse();//Returns a response from an Internet resource

            Stream streamResponse = myWebResponse.GetResponseStream();//return the data stream from the internet
                                                                       //and save it in the stream

            StreamReader sreader = new StreamReader(streamResponse);//reads the data stream
            Rstring = sreader.ReadToEnd();//reads it to the end
            String Links = GetContent(Rstring);//gets the links only
            
            textBox2.Text = Rstring;
            textBox3.Text = Links;
            streamResponse.Close();
            sreader.Close();
            myWebResponse.Close();




        }

        private String GetContent(String Rstring)
        {
            String sString="""";
            HTMLDocument d = new HTMLDocument();
            IHTMLDocument2 doc = (IHTMLDocument2)d;
            doc.write(Rstring);
            
            IHTMLElementCollection L = doc.links;
           
            foreach (IHTMLElement links in  L)
            {
                sString += links.getAttribute(""href"", 0);
                sString += ""/n"";
            }
            return sString;
        }

",69k,"
            13
        ","['\nI fixed your GetContent method as follow to get new links from crawled page:\npublic ISet<string> GetNewLinks(string content)\n{\n    Regex regexLink = new Regex(""(?<=<a\\\\s*?href=(?:\'|\\""))[^\'\\""]*?(?=(?:\'|\\""))"");\n\n    ISet<string> newLinks = new HashSet<string>();    \n    foreach (var match in regexLink.Matches(content))\n    {\n        if (!newLinks.Contains(match.ToString()))\n            newLinks.Add(match.ToString());\n    }\n\n    return newLinks;\n}\n\nUpdated\nFixed: regex should be regexLink. Thanks @shashlearner for pointing this out (my mistype).\n', '\ni have created something similar using Reactive Extension.\nhttps://github.com/Misterhex/WebCrawler\ni hope it can help you.\nCrawler crawler = new Crawler();\n\nIObservable observable = crawler.Crawl(new Uri(""http://www.codinghorror.com/""));\n\nobservable.Subscribe(onNext: Console.WriteLine, \nonCompleted: () => Console.WriteLine(""Crawling completed""));\n\n', '\nThe following includes an answer/recommendation.\nI believe you should use a dataGridView instead of a textBox as when you look at it in GUI it is easier to see the links (URLs) found.\nYou could change:\ntextBox3.Text = Links;\n\nto \n dataGridView.DataSource = Links;  \n\nNow for the question, you haven\'t included: \nusing System.  ""\'s""\n\nwhich ones were used, as it would be appreciated if I could get them as can\'t figure it out.\n', ""\nFrom a design standpoint, I've written a few webcrawlers.  Basically you want to implement a Depth First Search using a Stack data structure.   You can use Breadth First Search also, but you'll likely come into stack memory issues.  Good luck.\n""]",https://stackoverflow.com/questions/10452749/simple-web-crawler-in-c-sharp,web-crawler
Asp.net Request.Browser.Crawler - Dynamic Crawler List?,"
I learned Why Request.Browser.Crawler is Always False in C# (http://www.digcode.com/default.aspx?page=ed51cde3-d979-4daf-afae-fa6192562ea9&article=bc3a7a4f-f53e-4f88-8e9c-c9337f6c05a0).
Does anyone uses some method to dynamically update the Crawler's list, so Request.Browser.Crawler will be really useful?
",10k,"
            8
        ","[""\nI've been happy the the results supplied by Ocean's Browsercaps. It supports crawlers that Microsoft's config files has not bothered detecting. It will even parse out what version of the crawler is on your site, not that I really need that level of detail.\n"", '\nYou could check (regex) against Request.UserAgent.\nPeter Bromberg wrote a nice article about writing an ASP.NET Request Logger and Crawler Killer in ASP.NET.\nHere is the method he uses in his Logger class:\npublic static bool IsCrawler(HttpRequest request)\n{\n   // set next line to ""bool isCrawler = false; to use this to deny certain bots\n   bool isCrawler = request.Browser.Crawler;\n   // Microsoft doesn\'t properly detect several crawlers\n   if (!isCrawler)\n   {\n       // put any additional known crawlers in the Regex below\n       // you can also use this list to deny certain bots instead, if desired:\n       // just set bool isCrawler = false; for first line in method \n       // and only have the ones you want to deny in the following Regex list\n       Regex regEx = new Regex(""Slurp|slurp|ask|Ask|Teoma|teoma"");\n       isCrawler = regEx.Match(request.UserAgent).Success;\n   }\n   return isCrawler;\n}\n\n']",https://stackoverflow.com/questions/431765/asp-net-request-browser-crawler-dynamic-crawler-list,web-crawler
How do I save the origin html file with Apache Nutch,"
I'm new to search engines and web crawlers. Now I want to store all the original pages in a particular web site as html files, but with Apache Nutch I can only get the binary database files. How do I get the original html files with Nutch? 
Does Nutch support it? If not, what other tools can I use to achieve my goal.(The tools that support distributed crawling are better.)
",6k,"
            5
        ","['\nWell, nutch will write the crawled data in binary form so if if you want that to be saved in html format, you will have to modify the code. (this will be painful if you are new to nutch).\nIf you want quick and easy solution for getting html pages:\n\nIf the list of pages/urls that you intend to have is quite low, then better get it done with a script which invokes wget for each url.\nOR use HTTrack tool. \n\nEDIT:\nWriting a your own nutch plugin will be great. Your problem will get solved plus you can contribute to nutch by submitting your work !!! If you are new to nutch (in terms of code & design), then you will have to invest lot of time building a new plugin ... else its easy to do. \nFew pointers for helping your initiative:\nHere is a page which talks about writing own nutch plugin.\nStart with Fetcher.java. See lines 647-648. That is the place where you can get the fetched content on per url basis (for those pages which got fetched successfully).\npstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\nupdateStatus(content.getContent().length);\n\nYou should add code right after this to invoke your plugin. Pass content object to it. By now, you would have guessed that content.getContent() is the content for url you want. Inside the plugin code, write it to some file. Filename should be based on the url name else it will be difficult to work with that. Url can be obtained by fit.url.\n', '\nYou must do modifications in run Nutch in Eclipse.\nWhen you are able to run, open Fetcher.java and add the lines between ""content saver"" command lines. \ncase ProtocolStatus.SUCCESS:        // got a page\n            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);\n            updateStatus(content.getContent().length);\'\n\n\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n            String filename = ""savedsites//"" + content.getUrl().replace(\'/\', \'-\');  \n\n            File file = new File(filename);\n            file.getParentFile().mkdirs();\n            boolean exist = file.createNewFile();\n            if (!exist) {\n                System.out.println(""File exists."");\n            } else {\n                FileWriter fstream = new FileWriter(file);\n                BufferedWriter out = new BufferedWriter(fstream);\n                out.write(content.toString().substring(content.toString().indexOf(""<!DOCTYPE html"")));\n                out.close();\n                System.out.println(""File created successfully."");\n            }\n            //------------------------------------------- content saver ---------------------------------------------\\\\\n\n', '\nTo update this answer -\nIt is possible to post process the data from your crawldb segment folder, and read in the html (including other data nutch has stored) directly.\n    Configuration conf = NutchConfiguration.create();\n    FileSystem fs = FileSystem.get(conf);\n\n    Path file = new Path(segment, Content.DIR_NAME + ""/part-00000/data"");\n    SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\n\n    try\n    {\n            Text key = new Text();\n            Content content = new Content();\n\n            while (reader.next(key, content)) \n            {\n                    System.out.println(new String(content.GetContent()));\n            }\n    }\n    catch (Exception e)\n    {\n\n    }\n\n', '\nThe answers here are obsolete. Now, it is simply possible to get the plain HTML-files with nutch dump. Please see this answer.\n', '\nIn apache Nutch 2.3.1\nYou can save the raw HTML by edit the Nutch code firstly run the nutch in eclipse by following https://wiki.apache.org/nutch/RunNutchInEclipse\nAfter you finish ruunning nutch in eclipse edit file FetcherReducer.java , add this code to the output method, run ant eclipse again to rebuild the class\nFinally the raw html will added to reportUrl column in your database\nif (content != null) {\nByteBuffer raw = fit.page.getContent();\nif (raw != null) {\n    ByteArrayInputStream arrayInputStream = new ByteArrayInputStream(raw.array(), raw.arrayOffset() + raw.position(), raw.remaining());\n    Scanner scanner = new Scanner(arrayInputStream);\n    scanner.useDelimiter(""\\\\Z"");//To read all scanner content in one String\n    String data = """";\n    if (scanner.hasNext()) {\n        data = scanner.next();\n    }\n    fit.page.setReprUrl(StringUtil.cleanField(data));\n    scanner.close();\n}\n\n']",https://stackoverflow.com/questions/10007178/how-do-i-save-the-origin-html-file-with-apache-nutch,web-crawler
Wildcards in robots.txt,"
If in WordPress website I have categories in this order:
-Parent
--Child
---Subchild

I have permalinks set to:
%category%/%postname%
Let use an example.
I create post with post name ""Sport game"".
It's tag is sport-game.
It's full url is: domain.com/parent/child/subchild/sport-game
Why I use this kind of permalinks is exactly to block some content easier in robots.txt.
And now this is the part I have question for.
In robots.txt:
User-agent: Googlebot
Disallow: /parent/*
Disallow: /parent/*/*
Disallow: /parent/*/*/*

Disallow: /parent/* Is meaning of this rule that it's blocking domain.com/parent/child but not domain.com/parent/child/subchild and not domain.com/parent/?
Disallow: /parent/*/*/* Is meaning of this that it's blocking domain.com/parent/child/subchild/, that it's blocking only subchild, not child, not parent, and not posts under subchild?
",4k,"
            4
        ","['\nNote that the * wildcard in Disallow is not part of the original robots.txt specification. Some parsers support it, but as there is no specification, they might all handle it differently.\nAs you seem to be interested in Googlebot, have a look at Google’s robots.txt documentation.\nIn the examples it becomes clear that * means\n\nany string\n\n""Any string"" may, of course, also contain /.\nSo your first line Disallow: /parent/* should block every URL whose path starts with /parent/, including path segments separated by slashes.\nNote that this would be the same as Disallow: /parent/ in the original robots.txt specification, which also blocks any URL whose paths starts with /parent/, for example:\n\nhttp://example.com/parent/\nhttp://example.com/parent/foo\nhttp://example.com/parent/foo.html\nhttp://example.com/parent/foo/bar\nhttp://example.com/parent/foo/bar/\nhttp://example.com/parent/foo/bar/foo.html\n\n']",https://stackoverflow.com/questions/22134608/wildcards-in-robots-txt,web-crawler
YouTube Data API to crawl all comments and replies,"
I have been desperately seeking a solution to crawl all comments and corresponding replies for my research. Am having a very hard time creating a data frame that includes comment data in correct and corresponding orders.
I am gonna share my code here so you professionals can take a look and give me some insights.
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    data = {'Reply ID': [rauthor], 'Reply Time': [rtime], 'Reply Comments': [rtext], 'Reply Likes': [rlike]}
                    print(rauthor)
                    print(rtext)
            data = {'Comment':[comment],'Date':[comment2],'ID':[comment3], 'Likes':[comment4]}
            result = pd.DataFrame(data)
            result.to_csv('youtube.csv', mode='a',header=False)
            print(comment)
            print(comment2)
            print(comment3)
            print(comment4)
            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

When I do this, my crawler collects comments but doesn't collect some of the replies that are under certain comments.
How can I make it collect comments and their corresponding replies and put them in a single data frame?
Update
So, somehow I managed to pull the information I wanted at the output section of Jupyter Notebook. All I have to do now is to append the result at the data frame.
Here is my updated code:
def get_video_comments(service, **kwargs):
    comments = []
    results = service.commentThreads().list(**kwargs).execute()

    while results:
        for item in results['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comment2 = item['snippet']['topLevelComment']['snippet']['publishedAt']
            comment3 = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
            comment4 = item['snippet']['topLevelComment']['snippet']['likeCount']
            if 'replies' in item.keys():
                for reply in item['replies']['comments']:
                    rauthor = reply['snippet']['authorDisplayName']
                    rtext = reply['snippet']['textDisplay']
                    rtime = reply['snippet']['publishedAt']
                    rlike = reply['snippet']['likeCount']
                    print(rtext)
                    print(rtime)
                    print(rauthor)
                    print('Likes: ', rlike)
                    
            print(comment)
            print(comment2)
            print(comment3)
            print(""Likes: "", comment4)

            print('==============================')
            comments.append(comment)
                
        # Check if another page exists
        if 'nextPageToken' in results:
            kwargs['pageToken'] = results['nextPageToken']
            results = service.commentThreads().list(**kwargs).execute()
        else:
            break

    return comments

The result is:

As you can see, the comments grouped under ======== lines are the comment and corresponding replies underneath.
What would be a good way to append the result into the data frame?
",2k,"
            4
        ","[""\nAccording to the official doc, the property replies.comments[] of CommentThreads resource has the following specification:\n\nreplies.comments[] (list)\nA list of one or more replies to the top-level comment. Each item in the list is a comment resource.\nThe list contains a limited number of replies, and unless the number of items in the list equals the value of the snippet.totalReplyCount property, the list of replies is only a subset of the total number of replies available for the top-level comment. To retrieve all of the replies for the top-level comment, you need to call the Comments.list method and use the parentId request parameter to identify the comment for which you want to retrieve replies.\n\nConsequently, if wanting to obtain all reply entries associated to a given top-level comment, you will have to use the Comments.list API endpoint queried appropriately.\nI recommend you to read my answer to a very much related question; there are three sections:\n\nTop-Level Comments and Associated Replies,\nThe property nextPageToken and the parameter pageToken, and\nAPI Limitations Imposed by Design.\n\nFrom the get go, you'll have to acknowledge that the API (as currently implemented) does not allow to obtain all top-level comments associated to a given video when the number of those comments exceeds a certain (unspecified) upper bound.\n\nFor what concerns a Python implementation, I would suggest that you do structure the code as follows:\ndef get_video_comments(service, video_id):\n    request = service.commentThreads().list(\n        videoId = video_id,\n        part = 'id,snippet,replies',\n        maxResults = 100\n    )\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response['items']:\n            reply_count = comment['snippet'] \\\n                ['totalReplyCount']\n            replies = comment.get('replies')\n            if replies is not None and \\\n               reply_count != len(replies['comments']):\n               replies['comments'] = get_comment_replies(\n                   service, comment['id'])\n\n            # 'comment' is a 'CommentThreads Resource' that has it's\n            # 'replies.comments' an array of 'Comments Resource'\n\n            # Do fill in the 'comments' data structure \n            # to be provided by this function:\n            ...\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\n\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = 'id,snippet',\n        maxResults = 100\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response['items'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\nNote that the ellipsis dots above -- ... -- would have to be replaced with actual code that fills in the array of structures to be returned by get_video_comments to its caller.\nThe simplest way (useful for quick testing) would be to have ... replaced with comments.append(comment) and then the caller of get_video_comments to simply pretty print (using json.dump) the object obtained from that function.\n"", '\nBased on stvar\' answer and the original publication here I built this code:\nimport os\nimport pickle\nimport csv\nimport json\nimport google.oauth2.credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\n\nCLIENT_SECRETS_FILE = ""client_secret.json"" # for more information  to create your credentials json please visit https://python.gotrained.com/youtube-api-extracting-comments/\nSCOPES = [\'https://www.googleapis.com/auth/youtube.force-ssl\']\nAPI_SERVICE_NAME = \'youtube\'\nAPI_VERSION = \'v3\'\n\ndef get_authenticated_service():\n    credentials = None\n    if os.path.exists(\'token.pickle\'):\n        with open(\'token.pickle\', \'rb\') as token:\n            credentials = pickle.load(token)\n    #  Check if the credentials are invalid or do not exist\n    if not credentials or not credentials.valid:\n        # Check if the credentials have expired\n        if credentials and credentials.expired and credentials.refresh_token:\n            credentials.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                CLIENT_SECRETS_FILE, SCOPES)\n            credentials = flow.run_console()\n\n        # Save the credentials for the next run\n        with open(\'token.pickle\', \'wb\') as token:\n            pickle.dump(credentials, token)\n\n    return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)\n\ndef get_video_comments(service, **kwargs):\n    request = service.commentThreads().list(**kwargs)\n    comments = []\n\n    while request:\n        response = request.execute()\n\n        for comment in response[\'items\']:\n            reply_count = comment[\'snippet\'] \\\n                [\'totalReplyCount\']\n            replies = comment.get(\'replies\')\n            if replies is not None and \\\n               reply_count != len(replies[\'comments\']):\n               replies[\'comments\'] = get_comment_replies(\n                   service, comment[\'id\'])\n\n            # \'comment\' is a \'CommentThreads Resource\' that has it\'s\n            # \'replies.comments\' an array of \'Comments Resource\'\n\n            # Do fill in the \'comments\' data structure \n            # to be provided by this function:\n            comments.append(comment)\n\n        request = service.commentThreads().list_next(\n            request, response)\n\n    return comments\ndef get_comment_replies(service, comment_id):\n    request = service.comments().list(\n        parentId = comment_id,\n        part = \'id,snippet\',\n        maxResults = 1000\n    )\n    replies = []\n\n    while request:\n        response = request.execute()\n        replies.extend(response[\'items\'])\n        request = service.comments().list_next(\n            request, response)\n\n    return replies\n\n\nif __name__ == \'__main__\':\n    # When running locally, disable OAuthlib\'s HTTPs verification. When\n    # running in production *do not* leave this option enabled.\n    os.environ[\'OAUTHLIB_INSECURE_TRANSPORT\'] = \'1\'\n    service = get_authenticated_service()\n    videoId = input(\'Enter Video id : \') # video id here (the video id of https://www.youtube.com/watch?v=vedLpKXzZqE -> is vedLpKXzZqE)\n    comments = get_video_comments(service, videoId=videoId, part=\'id,snippet,replies\', maxResults = 1000)\n\n\nwith open(\'youtube_comments\', \'w\', encoding=\'UTF8\') as f:\n    writer = csv.writer(f, delimiter=\',\', quotechar=\'""\', quoting=csv.QUOTE_MINIMAL)\n    for row in comments:\n            # convert the tuple to a list and write to the output file\n            writer.writerow([row])\n\n\nit returns a file called youtube_comments with this format:\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'gvhv4hkH0H2OqQAHQKxzfA-K_tA\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'qpuKZcuD4FKf6BHgRlMunersEeU\', \'id\': \'UgzSgI1YEvwcuF4cPwN4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is a comment\', \'textOriginal\': \'This is a comment\', \'authorDisplayName\': \'Gabriell Magana\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLRGBvo2ZncDP1xGjlX6anfUufNYi9b3w9kYZFDl=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCKAa4FYftXsN7VKaPSlCivg\', \'authorChannelId\': {\'value\': \'UCKAa4FYftXsN7VKaPSlCivg\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 8, \'publishedAt\': \'2019-05-22T12:38:34Z\', \'updatedAt\': \'2019-05-22T12:38:34Z\'}}, \'canReply\': True, \'totalReplyCount\': 0, \'isPublic\': True}}""\n""{\'kind\': \'youtube#commentThread\', \'etag\': \'DsgDziMk7mB7xN4OoX7cmqlbDYE\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'topLevelComment\': {\'kind\': \'youtube#comment\', \'etag\': \'NYjvYM9W_umBafAfQkdg1P9apgg\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'This is another comment\', \'textOriginal\': \'This is another comment\', \'authorDisplayName\': \'Mary Montes\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTg1b1yw8BX8Af0PoTR_t5OOwP9Cfl9_qL-o1iikw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC_GP_8HxDPsqJjJ3Fju_UeA\', \'authorChannelId\': {\'value\': \'UC_GP_8HxDPsqJjJ3Fju_UeA\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 9, \'publishedAt\': \'2019-05-15T05:10:49Z\', \'updatedAt\': \'2019-05-15T05:10:49Z\'}}, \'canReply\': True, \'totalReplyCount\': 3, \'isPublic\': True}, \'replies\': {\'comments\': [{\'kind\': \'youtube#comment\', \'etag\': \'Tu41ENCZYNJ2KBpYeYz4qgre0H8\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF79DbfJ9zMKxM\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'this is first reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'JULIO EMPRESARIO\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/eYP4MBcZ4bON_pHtdbtVsyWnsKbpNKye2wTPhgkffkMYk3ZbN0FL6Aa1o22YlFjn2RVUAkSQYw=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCrpB9oZZZfmBv1aQsxrk66w\', \'authorChannelId\': {\'value\': \'UCrpB9oZZZfmBv1aQsxrk66w\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-09-15T04:06:50Z\', \'updatedAt\': \'2020-09-15T04:06:50Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'OrpbnJddwzlzwGArCgtuuBsYr94\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF795E1w8RV1DJ\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'the second replay\', \'textOriginal\': \'the second replay\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Anatolio27 Diaz\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLR1hOySIxEkvRCySExHjo3T6zGBNkvuKpPkqA=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UC04N8BM5aUwDJf-PNFxKI-g\', \'authorChannelId\': {\'value\': \'UC04N8BM5aUwDJf-PNFxKI-g\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2020-02-19T18:21:06Z\', \'updatedAt\': \'2020-02-19T18:21:06Z\'}}, {\'kind\': \'youtube#comment\', \'etag\': \'sPmIwerh3DTZshLiDVwOXn_fJx0\', \'id\': \'UgytsI51LU6BWRmYtBB4AaABAg.8uwduw6ppF78wwH6Aabh4y\', \'snippet\': {\'videoId\': \'tGTaBt4Hfd0\', \'textDisplay\': \'A third reply\', \'textOriginal\': \'A third reply\', \'parentId\': \'UgytsI51LU6BWRmYtBB4AaABAg\', \'authorDisplayName\': \'Voy detrás de mi pasión\', \'authorProfileImageUrl\': \'https://yt3.ggpht.com/ytc/AKedOLTgzZ3ZFvkmmAlMzA77ApM-2uGFfvOBnzxegYEX=s48-c-k-c0x00ffffff-no-rj\', \'authorChannelUrl\': \'http://www.youtube.com/channel/UCvv6QMokO7KcJCDpK6qZg3Q\', \'authorChannelId\': {\'value\': \'UCvv6QMokO7KcJCDpK6qZg3Q\'}, \'canRate\': True, \'viewerRating\': \'none\', \'likeCount\': 2, \'publishedAt\': \'2019-07-03T18:45:34Z\', \'updatedAt\': \'2019-07-03T18:45:34Z\'}}]}}""\n\nNow it is necessary a second step in order to information required. For this I a set of bash script toos like cut, awk and set:\ncut -d "":"" -f 10- youtube_comments | sed -e ""s/\', \'/\\n/g"" -e ""s/\'//g"" | awk \'/replies/{print ""------------------------****---------:::   Replies: ""$6""  :::---------******--------------------------------""}!/replies/{print}\' |sed \'/^textOriginal:/,/^authorDisplayName:/{/^authorDisplayName/!d}\' |sed \'/^authorProfileImageUrl:\\|^authorChannelUrl:\\|^authorChannelId:\\|^etag:\\|^updatedAt:\\|^parentId:\\|^id:/d\' |sed \'s/<[^>]*>//g\' | sed \'s/{textDisplay/{\\ntextDisplay/\' |sed \'/^snippet:/d\' | awk -F"":"" \'(NF==1){print ""========================================COMMENT===========================================""}(NF>1){a=0; print $0}\' | sed \'s/textDisplay: //g\' | sed \'s/authorDisplayName/User/g\' | sed \'s/T[0-9]\\{2\\}:[0-9]\\{2\\}:[0-9]\\{2\\}Z//g\' | sed \'s/likeCount: /Likes:/g\' | sed \'s/publishedAt: //g\' > output_file\n\nThe final result is a file called output_file with this format:\n========================================COMMENT===========================================\nThis is a comment\nUser: Robert Everest\nLikes:8, 2019-05-22\n========================================COMMENT===========================================\nThis is another comment\nUser: Anna Davis\nLikes:9, 2019-05-15\n------------------------****---------:::   Replies: 3,  :::---------******--------------------------------\nthis is first reply\nUser: John Doe\nLikes:2, 2020-09-15\nthe second replay\nUser: Caraqueno\nLikes:2, 2020-02-19\nA third reply\nUser: Rebeca\nLikes:2, 2019-07-03\n\nThe python script requires of the file token.pickle to work, it is generated the first time the python script run and when it expired, it have to be deleted and generated again.\n', '\nI had a similar issue that the OP does and managed to solve it, but someone in the community closed my question after I solved it and can\'t post there. I\'m posting it here for fidelity.\nThe YouTube API doesn\'t allow users to grab nested replies to comments. What it does allow is you to get the replies to the comments and all the comments i.e. Video --> Comments --> Comment Replies ---> Reply To Reply et al. Knowing this limitation we can write code to get all the top Comments, and then break into those comments to get the first-level replies.\nModuels\nimport os\nimport googleapiclient.discovery #required for using googleapi\nimport pandas as pd #require for data munging. We use pd.json_normalize to create the tables\nimport numpy as np #just good to have\nimport json # the requests are returned as json objects. \nfrom datetime import datetime #good to have for date modification\n\nGet All Comments Function\nFor a given vidId, this function will get the first 100 comments and place them into a df. It then use a while loop to check to see if the response api contains nextPageToken. While it does, it will continue to run to get all the comments until either all the comments are pulled or you run out of credits, whichever happens first.\ndef vidcomments(vidId):\n    # Disable OAuthlib\'s HTTPS verification when running locally.\n    # *DO NOT* leave this option enabled in production.\n    os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n    api_service_name = ""youtube""\n    api_version = ""v3""\n    DEVELOPER_KEY = ""yourapikey"" #<--- insert API key here\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=""snippet, replies"",\n        order=""time"",\n        maxResults=100,\n        textFormat=""plainText"",\n        videoId=vidId\n    )\n    \n    response = request.execute()\n    full = pd.json_normalize(response, record_path=[\'items\'])\n    while response:\n        \n        if \'nextPageToken\' in response:\n            response = youtube.commentThreads().list(\n                part=""snippet"",\n                maxResults=100,\n                textFormat=\'plainText\',\n                order=\'time\',\n                videoId=vidId,\n                pageToken=response[\'nextPageToken\']\n            ).execute()\n            \n            df2 = pd.json_normalize(response, record_path=[\'items\'])\n            full = full.append(df2)\n            \n        else:\n            break\n    return full\n\nGet All Replies To Comments Function\nFor a particular parentId, get all the first-level replies. Like the vidcomments() function noted above, it will run until all replies to all comments are pulled or you run out of credits, whichever happens first.\n    def repliesto(parentId):\n        # Disable OAuthlib\'s HTTPS verification when running locally.\n        # *DO NOT* leave this option enabled in production.\n        os.environ[""OAUTHLIB_INSECURE_TRANSPORT""] = ""1""\n\n        api_service_name = ""youtube""\n        api_version = ""v3""\n        DEVELOPER_KEY = DevKey #your dev key\n\n        youtube = googleapiclient.discovery.build(\n            api_service_name, api_version, developerKey = DEVELOPER_KEY)\n\n        request = youtube.comments().list(\n            part=""snippet"",\n            maxResults=100,\n            parentId=parentId,\n            textFormat=""plainText""\n        )\n        response = request.execute()\n\n        replies = pd.json_normalize(response, record_path=[\'items\'])\n        while response:\n\n            if \'nextPageToken\' in response:\n                response = youtube.comments().list(\n                    part=""snippet"",\n                    maxResults=100,\n                    parentId=parentId,\n                    textFormat=""plainText"",\n                    pageToken=response[\'nextPageToken\']                \n                ).execute()\n\n                df2 = pd.json_normalize(response, record_path=[\'items\'])\n                replies = pd.concat([replies, df2], sort=False)\n\n            else:\n                break\n        return replies\n\n\nPutting It Together\nFirst, run the vidcomments function to get all the comments information. Then use the code below to get all the reply information using a for loop to pull in each topLevelComment.id into a list, then use the list and another for loop to build the replies dataframe. This will create two separate Dataframes, one for Comments and another for Replies. After creating both of these Dataframes you can then join them in a way that makes sense for your purpose, either concat/union or a join/merge.\n    replyto = []\n    for reply in full[(full[\'snippet.totalReplyCount\']>0)] \n    [\'snippet.topLevelComment.id\']:\n        replyto.append(reply)\n\n    # create an empty DF to store all the replies\n    # use a for loop to place each item in our replyto list into the function defined above\n    \n    replies = pd.DataFrame()\n    for reply in replyto:\n        df = repliesto(reply)\n        replies = pd.concat([replies, df], ignore_index=True)\n\n']",https://stackoverflow.com/questions/64275331/youtube-data-api-to-crawl-all-comments-and-replies,web-crawler
How to request Google to re-crawl my website? [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 This question does not appear to be about programming within the scope defined in the help center.


Closed 7 years ago.







                        Improve this question
                    



Does someone know a way to request Google to re-crawl a website? If possible, this shouldn't last months. My site is showing an old title in Google's search results. How can I show it with the correct title and description? 
",450k,"
            240
        ","['\nThere are two options. The first (and better) one is using the Fetch as Google option in Webmaster Tools that Mike Flynn commented  about. Here are detailed instructions:\n\nGo to: https://www.google.com/webmasters/tools/ and log in\nIf you haven\'t already, add and verify the site with the ""Add a Site"" button\nClick on the site name for the one you want to manage\nClick Crawl -> Fetch as Google\nOptional: if you want to do a specific page only, type in the URL\nClick Fetch\nClick Submit to Index\nSelect either ""URL"" or ""URL and its direct links""\nClick OK and you\'re done.\n\nWith the option above, as long as every page can be reached from some link on the initial page or a page that it links to, Google should recrawl the whole thing. If you want to explicitly tell it a list of pages to crawl on the domain, you can follow the directions to submit a sitemap.\nYour second (and generally slower) option is, as seanbreeden pointed out, submitting here: http://www.google.com/addurl/\nUpdate 2019:\n\nLogin to - Google Search Console\nAdd a site and verify it with the available methods.\nAfter verification from the console, click on URL Inspection.\nIn the Search bar on top, enter your website URL or custom URLs for inspection and enter.\nAfter Inspection, it\'ll show an option to Request Indexing\nClick on it and GoogleBot will add your website in a Queue for crawling.\n\n', '\nThe usual way is to either resubmit your site in your Google Webmaster Tools or submit it here:  http://www.google.com/addurl/\n', '\nGoogle says that it is unable to control when your site is re-crawled. Regardless, you could also check this post on ""forcing rewcrawls"", I haven\'t tried it myself but it\'s worth a shot if you\'re desperate. \nOn another note, I might add that you make sure you have a sitemap.xml up as this will also help with SEO.\n', '\nAs far I know, if you resubmit a sitemap it will trigger and crawler of your site. \n', ""\nNowadays, the revisiting of a website pretty much depends on its popularity, authority and how often its content changes. Having a sitemap.xml containing all URLs is always better. You can also set the lastmod tag of each URL entries. If you don't abuse it, crawlers will take it into account.\n""]",https://stackoverflow.com/questions/9466360/how-to-request-google-to-re-crawl-my-website,web-crawler
What is the difference between web-crawling and web-scraping? [duplicate],"






This question already has answers here:
                        
                    



crawler vs scraper

                                (6 answers)
                            

Closed 4 years ago.



Is there a difference between Crawling and Web-scraping?
If there's a difference, what's the best method to use in order to collect some web data to supply a database for later use in a customised search engine?
",73k,"
            101
        ","[""\nCrawling would be essentially what Google, Yahoo, MSN, etc. do, looking for ANY information.  Scraping is generally targeted at certain websites, for specfic data, e.g. for price comparison, so are coded quite differently.\nUsually a scraper will be bespoke to the websites it is supposed to be scraping, and would be doing things a (good) crawler wouldn't do, i.e.:\n\nHave no regard for robots.txt\nIdentify itself as a browser\nSubmit forms with data \nExecute Javascript (if required to\nact like a user)\n\n"", ""\nYes, they are different. In practice, you may need to use both.\n(I have to jump in because, so far, the other answers don't get to the essence of it. They use examples but don't make the distinctions clear. Granted, they are from 2010!)\nWeb scraping, to use a minimal definition, is the process of processing a web document and extracting information out of it. You can do web scraping without doing web crawling. \nWeb crawling, to use a minimal definition, is the process of iteratively finding and fetching web links starting from a list of seed URL's. Strictly speaking, to do web crawling, you have to do some degree of web scraping (to extract the URL's.)\nTo clear up some concepts mentioned in the other answers:\n\nrobots.txt is intended to apply to any automated process that accesses a web page. So it applies to both crawlers and scrapers.\n'Proper' crawlers and scrapers, both, should identify themselves accurately.\n\nSome references:\n\nWikipedia on web scraping\nWikipedia on web crawlers\nWikipedia on robots.txt\n\n"", '\nAFAIK Web Crawling is what Google does - it goes around a website looking at links and building a database of the layout of that site and sites it links to\nWeb Scraping would be the progamatic analysis of a web page to load some data off of it, EG loading up BBC weather and ripping (scraping) the weather forcast off of it and placing it elsewhere or using it in another program.\n', ""\nThere's a fundamental difference between these two. \nFor those looking to dig deeper, I suggest you read this - \nWeb scraper, Web Crawler\nThis post goes into detail. A good summary is in this chart from the article:\n\n"", ""\nThere's definitely a difference between these two. One refers to visiting a site, the other to extracting.\n"", '\nWe crawl sites to have broad perspective how the site is structured, what are connections between pages, to estimate how much time we need to visit all pages we are interested in. Scraping is often harder to implement, but it’s an essence of data extraction. Let’s think of scraping as of covering website with sheet of paper with some rectangles cut out. We can now see only things we need, completely ignoring parts of website that are common for all pages (like navigation, footer, ads), or extraneous informations as comments or breadcrumbs.\nMore about differences between crawling and scrapping you find here: https://tarantoola.io/web-scraping-vs-web-crawling/\n']",https://stackoverflow.com/questions/4327392/what-is-the-difference-between-web-crawling-and-web-scraping,web-crawler
How to force scrapy to crawl duplicate url?,"
I am learning Scrapy a web crawling framework.
by default it does not crawl duplicate urls or urls which scrapy have already crawled.  
How to make Scrapy to crawl duplicate urls or urls which have already crawled?
I tried to find out on internet but could not find relevant help.  
I found DUPEFILTER_CLASS = RFPDupeFilter and SgmlLinkExtractor from Scrapy - Spider crawls duplicate urls but this question is opposite of what I am looking
",22k,"
            29
        ","[""\nYou're probably looking for the dont_filter=True argument on Request().\nSee http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\n"", ""\nA more elegant solution is to disable the duplicate filter altogether:\n# settings.py\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n\nThis way you don't have to clutter all your Request creation code with dont_filter=True. Another side effect: this only disables duplicate filtering and not any other filters like offsite filtering.\nIf you want to use this setting selectively for only one or some of multiple spiders in your project, you can set it via custom_settings in the spider implementation:\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    custom_settings = {\n        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n    }\n\n""]",https://stackoverflow.com/questions/23131283/how-to-force-scrapy-to-crawl-duplicate-url,web-crawler
How to generate the start_urls dynamically in crawling?,"
I am crawling a site which may contain a lot of start_urls, like:
http://www.a.com/list_1_2_3.htm

I want to populate start_urls like [list_\d+_\d+_\d+\.htm],
and extract items from URLs like [node_\d+\.htm] during crawling. 
Can I use CrawlSpider to realize this function?
And how can I generate the start_urls dynamically in crawling?
",20k,"
            27
        ","[""\nThe best way to generate URLs dynamically is to override the start_requests method of the spider:  \n\nfrom scrapy.http.request import Request\n\ndef start_requests(self):\n      with open('urls.txt', 'rb') as urls:\n          for url in urls:\n              yield Request(url, self.parse)\n\n\n"", ""\nThere are two questions:\n1)yes you can realize this functionality by using Rules e.g ,\nrules =(Rule(SgmlLinkExtractor(allow = ('node_\\d+.htm')) ,callback = 'parse'))\n\nsuggested reading\n2) yes you can generate start_urls dynamically ,  start_urls is a \n\nlist\n\ne.g >>> start_urls = ['http://www.a.com/%d_%d_%d' %(n,n+1,n+2) for n in range(0, 26)]\n>>> start_urls\n\n['http://www.a.com/0_1_2', 'http://www.a.com/1_2_3', 'http://www.a.com/2_3_4', 'http://www.a.com/3_4_5', 'http://www.a.com/4_5_6', 'http://www.a.com/5_6_7',  'http://www.a.com/6_7_8', 'http://www.a.com/7_8_9', 'http://www.a.com/8_9_10','http://www.a.com/9_10_11', 'http://www.a.com/10_11_12', 'http://www.a.com/11_12_13', 'http://www.a.com/12_13_14', 'http://www.a.com/13_14_15', 'http://www.a.com/14_15_16', 'http://www.a.com/15_16_17', 'http://www.a.com/16_17_18', 'http://www.a.com/17_18_19', 'http://www.a.com/18_19_20', 'http://www.a.com/19_20_21', 'http://www.a.com/20_21_22', 'http://www.a.com/21_22_23', 'http://www.a.com/22_23_24', 'http://www.a.com/23_24_25', 'http://www.a.com/24_25_26', 'http://www.a.com/25_26_27']\n\n""]",https://stackoverflow.com/questions/9322219/how-to-generate-the-start-urls-dynamically-in-crawling,web-crawler
"Save complete web page (incl css, images) using python/selenium","
I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:
from selenium import webdriver

URL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome'
SEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'
CHROME_WEBDRIVER_LOCATION = '/home/max/Downloads/chromedriver' # update this for your machine

# open page with selenium
# (first need to download Chrome webdriver, or a firefox webdriver, etc)
driver = webdriver.Chrome(executable_path=CHROME_WEBDRIVER_LOCATION)
driver.get(URL)
time.sleep(5)

# enter sequence into the query field and hit 'blast' button to search
seq_query_field = driver.find_element_by_id(""seq"")
seq_query_field.send_keys(SEQUENCE)

blast_button = driver.find_element_by_id(""b1"")
blast_button.click()
time.sleep(60)

At that point I have a page that I can manually click ""save as,"" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc.
content = driver.page_source
with open('webpage.html', 'w') as f:
    f.write(content)

I've also found this question/answer on SO, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out)
Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality?
UPDATE - Follow up question for James' answer
So I ran James' code to generate a page.html (and associated files) and compared it to the html file I got from manually clicking save-as. The page.html saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). 

This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script:
<p class=""helpbox ui-ncbitoggler-slave ui-ncbitoggler"" id=""hlp1"" aria-hidden=""true"">
These options control formatting of alignments in results pages. The
default is HTML, but other formats (including plain text) are available.
PSSM and PssmWithParameters are representations of Position Specific Scoring Matrices and are only available for PSI-BLAST. 
The Advanced view option allows the database descriptions to be sorted by various indices in a table.
</p>

Any idea why this is happening?
",19k,"
            25
        ","['\nAs you noted, Selenium cannot interact with the browser\'s context menu to use Save as..., so instead to do so, you could use an external automation library like pyautogui.\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\nThis code opens the Save as... window through its keyboard shortcut CTRL+S and then saves the webpage and its assets into the default downloads location by pressing enter. This code also names the file as the sequence in order to give it a unique name, though you could change this for your use case. If needed, you could additionally change the download location through some extra work with the tab and arrow keys.\nTested on Ubuntu 18.10; depending on your OS you may need to modify the key combination sent.\n\nFull code, in which I also added conditional waits to improve speed:\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.expected_conditions import visibility_of_element_located\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport pyautogui\n\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' #\'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA\'\n\n# open page with selenium\n# (first need to download Chrome webdriver, or a firefox webdriver, etc)\ndriver = webdriver.Chrome()\ndriver.get(URL)\n\n# enter sequence into the query field and hit \'blast\' button to search\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\n\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\n# wait until results are loaded\nWebDriverWait(driver, 60).until(visibility_of_element_located((By.ID, \'grView\')))\n\n# open \'Save as...\' to save html and assets\npyautogui.hotkey(\'ctrl\', \'s\')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + \'.html\')\npyautogui.hotkey(\'enter\')\n\n', '\nThis is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of ""save as full web page (complete)"" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.  \nMost of the javascript won\'t work due to cross origin request blocking.  But the content will look (mostly) the same.\nThis uses requests to save the loaded files, lxml to parse the html, and os for the path legwork.\nfrom selenium import webdriver\nimport chromedriver_binary\nfrom lxml import html\nimport requests\nimport os\n\ndriver = webdriver.Chrome()\nURL = \'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\'\nSEQUENCE = \'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA\' \nbase = \'https://blast.ncbi.nlm.nih.gov/\'\n\ndriver.get(URL)\nseq_query_field = driver.find_element_by_id(""seq"")\nseq_query_field.send_keys(SEQUENCE)\nblast_button = driver.find_element_by_id(""b1"")\nblast_button.click()\n\ncontent = driver.page_source\n# write the page content\nos.mkdir(\'page\')\nwith open(\'page/page.html\', \'w\') as fp:\n    fp.write(content)\n\n# download the referenced files to the same path as in the html\nsess = requests.Session()\nsess.get(base)            # sets cookies\n\n# parse html\nh = html.fromstring(content)\n# get css/js files loaded in the head\nfor hr in h.xpath(\'head//@href\'):\n    if not hr.startswith(\'http\'):\n        local_path = \'page/\' + hr\n        hr = base + hr\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)\n\n# get image/js files from the body.  skip anything loaded from outside sources\nfor src in h.xpath(\'//@src\'):\n    if not src or src.startswith(\'http\'):\n        continue\n    local_path = \'page/\' + src\n    print(local_path)\n    src = base + src\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, \'wb\') as fp:\n        fp.write(res.content)  \n\nYou should have a folder called page with a file called page.html in it with the content you are after.\n', '\nInspired by FThompson\'s answer above, I came up with the following tool that can download full/complete html for a given page url (see: https://github.com/markfront/SinglePageFullHtml)\nUPDATE - follow up with Max\'s suggestion, below are steps to use the tool:\n\nClone the project, then run maven to build:\n\n$> git clone https://github.com/markfront/SinglePageFullHtml.git\n\n$> cd ~/git/SinglePageFullHtml\n$> mvn clean compile package\n\n\nFind the generated jar file in target folder: SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar\n\nRun the jar in command line like:\n\n\n$> java -jar .target/SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar <page_url>\n\n\nThe result file name will have a prefix ""FP, followed by the hashcode of the page url, with file extension "".html"". It will be found in either folder ""/tmp"" (which you can get by System.getProperty(""java.io.tmp""). If not, try find it in your home dir or System.getProperty(""user.home"") in Java).\n\nThe result file will be a big fat self-contained html file that includes everything (css, javascript, images, etc.) referred to by the original html source.\n\n\n', '\nI\'ll advise u to have a try on sikulix which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(""xxxx"") or subprocess...).\n']",https://stackoverflow.com/questions/53729201/save-complete-web-page-incl-css-images-using-python-selenium,web-crawler
Does solr do web crawling?,"
I am interested to do web crawling. I was looking at solr. 
Does solr do web crawling, or what are the steps to do web crawling?
",30k,"
            18
        ","[""\nSolr 5+ DOES in fact now do web crawling!\nhttp://lucene.apache.org/solr/\nOlder Solr versions do not do web crawling alone, as historically it's a search server that provides full text search capabilities.  It builds on top of Lucene.\nIf you need to crawl web pages using another Solr project then you have a number of options including:\n\nNutch - http://lucene.apache.org/nutch/\nWebsphinx - http://www.cs.cmu.edu/~rcm/websphinx/\nJSpider - http://j-spider.sourceforge.net/\nHeritrix - http://crawler.archive.org/\n\nIf you want to make use of the search facilities provided by Lucene or SOLR you'll need to build indexes from the web crawl results.\nSee this also: \nLucene crawler (it needs to build lucene index)\n"", '\nSolr does not in of itself have a web crawling feature.\nNutch is the ""de-facto"" crawler (and then some) for Solr.\n', '\nSolr 5 started supporting simple webcrawling (Java Doc). If want search, Solr is the tool, if you want to crawl, Nutch/Scrapy is better :) \nTo get it up and running, you can take a detail look at here. However, here is how to get it up and running in one line: \njava \n-classpath <pathtosolr>/dist/solr-core-5.4.1.jar \n-Dauto=yes \n-Dc=gettingstarted     -> collection: gettingstarted\n-Ddata=web             -> web crawling and indexing\n-Drecursive=3          -> go 3 levels deep\n-Ddelay=0              -> for the impatient use 10+ for production\norg.apache.solr.util.SimplePostTool   -> SimplePostTool\nhttp://datafireball.com/      -> a testing wordpress blog\n\nThe crawler here is very ""naive"" where you can find all the code from this Apache Solr\'s github repo.\nHere is how the response looks like: \nSimplePostTool version 5.0.0\nPosting web pages to Solr url http://localhost:8983/solr/gettingstarted/update/extract\nEntering auto mode. Indexing pages with content-types corresponding to file endings xml,json,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\nSimplePostTool: WARNING: Never crawl an external web site faster than every 10 seconds, your IP will probably be blocked\nEntering recursive mode, depth=3, delay=0s\nEntering crawl at level 0 (1 links total, 1 new)\nPOSTed web resource http://datafireball.com (depth: 0)\nEntering crawl at level 1 (52 links total, 51 new)\nPOSTed web resource http://datafireball.com/2015/06 (depth: 1)\n...\nEntering crawl at level 2 (266 links total, 215 new)\n...\nPOSTed web resource http://datafireball.com/2015/08/18/a-few-functions-about-python-path (depth: 2)\n...\nEntering crawl at level 3 (846 links total, 656 new)\nPOSTed web resource http://datafireball.com/2014/09/06/node-js-web-scraping-using-cheerio (depth: 3)\nSimplePostTool: WARNING: The URL http://datafireball.com/2014/09/06/r-lattice-trellis-another-framework-for-data-visualization/?share=twitter returned a HTTP result status of 302\n423 web pages indexed.\nCOMMITting Solr index changes to http://localhost:8983/solr/gettingstarted/update/extract...\nTime spent: 0:05:55.059\n\nIn the end, you can see all the data are indexed properly. \n \n', '\nYou might also want to take a look at \nhttp://www.crawl-anywhere.com/\nVery powerful crawler that is compatible with Solr. \n', ""\nI have been using Nutch with Solr on my latest project and it seems to work quite nicely.\nIf you are using a Windows machine then I would strongly recommend following the 'No cygwin' instructions given by Jason Riffel too!\n"", '\nYes, I agree with the other posts here, use Apache Nutch\n\nbin/nutch crawl urls -solr http://localhost:8983/solr/ -depth 3 -topN 5\n\nAlthough your solr version has the match the correct version of Nutch, because older versions of solr stores the indices in a different format\nIts tutorial:\nhttp://wiki.apache.org/nutch/NutchTutorial\n', ""\nI know it's been a while, but in case someone else is searching for a Solr crawler like me, there is a new open-source crawler called Norconex HTTP Collector \n"", ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nIn order to use Solr, you can use a web crawler that is capable of storing documents in Solr.\nFor instance, The Norconex HTTP Collector is a flexible and powerful open-source web crawler that is compatible with Solr.\nTo use Solr with the Norconex HTTP Collector you will need the Norconex HTTP Collector which is used to crawl the website that you want to collect data from, and you will need to install the Norconex Apache Solr Committer to store collected documents into Solr. When the committer is installed, you will need to configure the XML configuration file of the crawler. I would recommend that you follow this link to get started test how the crawler works and here to know how to configure the configuration file. Finally, you will need this link to configure the committer section of the configuration file with Solr.\nNote that if your goal is not to crawl web pages, Norconex also has a Filesystem Collector that can be used with the Sorl Committer as well.\n"", '\nDef Nutch !\nNutch also has a basic web front end which will let you query your search results. You might not even need to bother with SOLR depending on your requirements. If you do a Nutch/SOLR combination you should be able to take advantage of the recent work done to integrate SOLR and Nutch ...  http://issues.apache.org/jira/browse/NUTCH-442 \n']",https://stackoverflow.com/questions/1781247/does-solr-do-web-crawling,web-crawler
Submit data via web form and extract the results,"
My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this.
This is the form dom: 
<form action=""analysis.php"" method=""POST"">
<textarea cols=""75"" rows=""13"" name=""text""></textarea>
<div class=""copyright"">(NOTE: The genie works best on texts of more than 500 words.)</div>
<p>
<b>Genre:</b>
<input type=""radio"" value=""fiction"" name=""genre"">
fiction&nbsp;&nbsp;
<input type=""radio"" value=""nonfiction"" name=""genre"">
nonfiction&nbsp;&nbsp;
<input type=""radio"" value=""blog"" name=""genre"">
blog entry
</p>
<p>
</form>

results page dom:
<p>
<b>The Gender Genie thinks the author of this passage is:</b>
male!
</p>

",49k,"
            17
        ","['\nNo need to use mechanize, just send the correct form data in a POST request.\nAlso, using regular expression to parse HTML is a bad idea. You would be better off using a HTML parser like lxml.html.\nimport requests\nimport lxml.html as lh\n\n\ndef gender_genie(text, genre):\n    url = \'http://bookblog.net/gender/analysis.php\'\n    caption = \'The Gender Genie thinks the author of this passage is:\'\n\n    form_data = {\n        \'text\': text,\n        \'genre\': genre,\n        \'submit\': \'submit\',\n    }\n\n    response = requests.post(url, data=form_data)\n\n    tree = lh.document_fromstring(response.content)\n\n    return tree.xpath(""//b[text()=$caption]"", caption=caption)[0].tail.strip()\n\n\nif __name__ == \'__main__\':\n    print gender_genie(\'I have a beard!\', \'blog\')\n\n', '\nYou can use mechanize to submit and retrieve content, and the re module for getting what you want. For example, the script below does it for the text of your own question:\nimport re\nfrom mechanize import Browser\n\ntext = """"""\nMy python level is Novice. I have never written a web scraper \nor crawler. I have written a python code to connect to an api and \nextract the data that I want. But for some the extracted data I want to \nget the gender of the author. I found this web site \nhttp://bookblog.net/gender/genie.php but downside is there isn\'t an api \navailable. I was wondering how to write a python to submit data to the \nform in the page and extract the return data. It would be a great help \nif I could get some guidance on this.""""""\n\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nbrowser.select_form(nr=0)\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\nresponse = browser.submit()\n\ncontent = response.read()\n\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\', content)\n\nprint result[0]\n\nWhat does it do? It creates a mechanize.Browser and goes to the given URL:\nbrowser = Browser()\nbrowser.open(""http://bookblog.net/gender/genie.php"")\n\nThen it selects the form (since there is only one form to be filled, it will be the first):\nbrowser.select_form(nr=0)\n\nAlso, it sets the entries of the form...\nbrowser[\'text\'] = text\nbrowser[\'genre\'] = [\'nonfiction\']\n\n... and submit it:\nresponse = browser.submit()\n\nNow, we get the result:\ncontent = response.read()\n\nWe know that the result is in the form:\n<b>The Gender Genie thinks the author of this passage is:</b> male!\n\nSo we create a regex for matching and use re.findall():\nresult = re.findall(\n    r\'<b>The Gender Genie thinks the author of this passage is:</b> (\\w*)!\',\n    content)\n\nNow the result is available for your use:\nprint result[0]\n\n', '\nYou can use mechanize, see examples for details.\nfrom mechanize import ParseResponse, urlopen, urljoin\n\nuri = ""http://bookblog.net""\n\nresponse = urlopen(urljoin(uri, ""/gender/genie.php""))\nforms = ParseResponse(response, backwards_compat=False)\nform = forms[0]\n\n#print form\n\nform[\'text\'] = \'cheese\'\nform[\'genre\'] = [\'fiction\']\n\nprint urlopen(form.click()).read()\n\n']",https://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results,web-crawler
How to make a polygon radar (spider) chart in python,"
import matplotlib.pyplot as plt
import numpy as np

labels=['Siege', 'Initiation', 'Crowd_control', 'Wave_clear', 'Objective_damage']
markers = [0, 1, 2, 3, 4, 5]
str_markers = [""0"", ""1"", ""2"", ""3"", ""4"", ""5""]

def make_radar_chart(name, stats, attribute_labels=labels,
                     plot_markers=markers, plot_str_markers=str_markers):

    labels = np.array(attribute_labels)

    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)
    stats = np.concatenate((stats,[stats[0]]))
    angles = np.concatenate((angles,[angles[0]]))

    fig = plt.figure()
    ax = fig.add_subplot(111, polar=True)
    ax.plot(angles, stats, 'o-', linewidth=2)
    ax.fill(angles, stats, alpha=0.25)
    ax.set_thetagrids(angles * 180/np.pi, labels)
    plt.yticks(markers)
    ax.set_title(name)
    ax.grid(True)

    fig.savefig(""static/images/%s.png"" % name)

    return plt.show()

make_radar_chart(""Agni"", [2,3,4,4,5]) # example



Basically I want the chart to be a pentagon instead of circle. Can anyone help with this. I am using python matplotlib to save an image which will stored and displayed later. I want my chart to have the form of the second picture
EDIT:
    gridlines = ax.yaxis.get_gridlines()
    for gl in gridlines:
        gl.get_path()._interpolation_steps = 5

adding this section of code from answer below helped a lot. I am getting this chart. Still need to figure out how to get rid of the outer most ring: 
",27k,"
            17
        ","['\nThe radar chart demo shows how to make the a radar chart. The result looks like this:\n\nHere, the outer spine is polygon shaped as desired. However the inner grid lines are circular. \nSo the open question is how to make the gridlines the same shape as the spines.\nThis can be done by overriding the draw method and setting the gridlines\' path interpolation step variable to the number of variables of the RadarAxes class.\ngridlines = self.yaxis.get_gridlines()\nfor gl in gridlines:\n    gl.get_path()._interpolation_steps = num_vars\n\nComplete example:\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d,  alpha=0.25)\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\n\n', '\nAs shown in this other post the answer from @ImportanceOfBeingErnest doesn\'t work in matplotlib>3.2.2 in that you get circular grids. As shown in this PR you can use the following\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame=\'circle\'):\n    """"""Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {\'circle\' | \'polygon\'}\n        Shape of frame surrounding axes.\n\n    """"""\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n    \n    class RadarTransform(PolarAxes.PolarTransform):\n        def transform_path_non_affine(self, path):\n            # Paths with non-unit interpolation steps correspond to gridlines,\n            # in which case we force interpolation (to defeat PolarTransform\'s\n            # autoconversion to circular arcs).\n            if path._interpolation_steps > 1:\n                path = path.interpolated(num_vars)\n            return Path(self.transform(path.vertices), path.codes)\n\n    class RadarAxes(PolarAxes):\n\n        name = \'radar\'\n        \n        PolarTransform = RadarTransform\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location(\'N\')\n\n        def fill(self, *args, closed=True, **kwargs):\n            """"""Override fill so that line is closed by default""""""\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            """"""Override plot so that line is closed by default""""""\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == \'circle\':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == \'polygon\':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=""k"")\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n        def draw(self, renderer):\n            """""" Draw. If frame is polygon, make gridlines polygon-shaped """"""\n            if frame == \'polygon\':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == \'circle\':\n                return super()._gen_axes_spines()\n            elif frame == \'polygon\':\n                # spine_type must be \'left\'/\'right\'/\'top\'/\'bottom\'/\'circle\'.\n                spine = Spine(axes=self,\n                              spine_type=\'circle\',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {\'polar\': spine}\n            else:\n                raise ValueError(""unknown value for \'frame\': %s"" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [[\'Sulfate\', \'Nitrate\', \'EC\', \'OC1\', \'OC2\', \'OC3\', \'OP\', \'CO\', \'O3\'],\n        (\'Basecase\', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame=\'polygon\')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection=\'radar\'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha=\'center\')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d, alpha=0.25, label=\'_nolegend_\')\nax.set_varlabels(spoke_labels)\n\nplt.show()\n\nto get the desired:\n\n']",https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python,web-crawler
Get Scrapy crawler output/results in script file function,"
I am using a script file to run a spider within scrapy project and spider is logging the crawler output/results. But i want to use spider output/results in that script file in some function .I did not want to save output/results in any file or DB.
Here is Script code get from https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script
from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})
runner = CrawlerRunner(get_project_settings())


d = runner.crawl('my_spider')
d.addBoth(lambda _: reactor.stop())
reactor.run()

def spider_output(output):
#     do something to that output

How can i get spider output in 'spider_output' method. It is possible to get output/results.
",9k,"
            15
        ","[""\nHere is the solution that get all output/results in a list\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nfrom scrapy.signalmanager import dispatcher\n\n\ndef spider_results():\n    results = []\n\n    def crawler_results(signal, sender, item, response, spider):\n        results.append(item)\n\n    dispatcher.connect(crawler_results, signal=signals.item_scraped)\n\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(MySpider)\n    process.start()  # the script will block here until the crawling is finished\n    return results\n\n\nif __name__ == '__main__':\n    print(spider_results())\n\n"", '\nThis is an old question, but for future reference. If you are working with python 3.6+ I recommend using scrapyscript that allows you to run your Spiders and get the results in a super simple way:\nfrom scrapyscript import Job, Processor\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\nimport json\n\n# Define a Scrapy Spider, which can accept *args or **kwargs\n# https://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nclass PythonSpider(Spider):\n    name = \'myspider\'\n\n    def start_requests(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        title = response.xpath(\'//title/text()\').extract()\n        return {\'url\': response.request.url, \'title\': title}\n\n# Create jobs for each instance. *args and **kwargs supplied here will\n# be passed to the spider constructor at runtime\ngithubJob = Job(PythonSpider, url=\'http://www.github.com\')\npythonJob = Job(PythonSpider, url=\'http://www.python.org\')\n\n# Create a Processor, optionally passing in a Scrapy Settings object.\nprocessor = Processor(settings=None)\n\n# Start the reactor, and block until all spiders complete.\ndata = processor.run([githubJob, pythonJob])\n\n# Print the consolidated results\nprint(json.dumps(data, indent=4))\n\n[\n    {\n        ""title"": [\n            ""Welcome to Python.org""\n        ],\n        ""url"": ""https://www.python.org/""\n    },\n    {\n        ""title"": [\n            ""The world\'s leading software development platform \\u00b7 GitHub"",\n            ""1clr-code-hosting""\n        ],\n        ""url"": ""https://github.com/""\n    }\n]\n\n', ""\nAFAIK there is no way to do this, since crawl():\n\nReturns a deferred that is fired when the crawling is finished.\n\nAnd the crawler doesn't store results anywhere other than outputting them to logger.\nHowever returning ouput would conflict with the whole asynchronious nature and structure of scrapy, so saving to file then reading it is a prefered approach here.\nYou can simply devise pipeline that saves your items to file and simply read the file in your spider_output. You will receive your results since reactor.run() is blocking your script untill the output file is complete anyways.\n"", ""\nMy advice is to use the Python subprocess module to run spider from the script rather than using the method provided in the scrapy docs to run spider from python script. The reason for that is that with the subprocess module, you can capture the output/logs and even statements that you print from inside the spider.\nIn Python 3, execute the spider with the run method. Ex.\nimport subprocess\nprocess = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nif process.returncode == 0:\n    result = process.stdout.decode('utf-8')\nelse:\n    # code to check error using 'process.stderr'\n\nSetting the stdout/stderr to subprocess.PIPE will allow capture of output so it's very important to set this flag.\nHere command should be a sequence or a string (It it's a string, then call the run method with 1 more param: shell=True). For example:\ncommand = ['scrapy', 'crawl', 'website', '-a', 'customArg=blahblah']\n# or\ncommand = 'scrapy crawl website -a customArg=blahblah' # with shell=True\n#or\nimport shlex\ncommand = shlex.split('scrapy crawl website -a customArg=blahblah') # without shell=True\n\nAlso, process.stdout will contain the output from the script but it will be of type bytes. You need to convert it to str using decode('utf-8')\n"", ""\nIt's going to return all the results of a Spider within a list.\nfrom scrapyscript import Job, Processor\nfrom scrapy.utils.project import get_project_settings\n\n\ndef get_spider_output(spider, **kwargs):\n    job = Job(spider, **kwargs)\n    processor = Processor(settings=get_project_settings())\n    return processor.run([job])\n\n""]",https://stackoverflow.com/questions/40237952/get-scrapy-crawler-output-results-in-script-file-function,web-crawler
Using one Scrapy spider for several websites,"
I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI.
How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow.
",7k,"
            12
        ","[""\nWARNING: This answer was for Scrapy v0.7, spider manager api changed a lot since then.\nOverride default SpiderManager class, load your custom rules from a database or somewhere else and instanciate a custom spider with your own rules/regexes and domain_name\nin mybot/settings.py:\nSPIDER_MANAGER_CLASS = 'mybot.spidermanager.MySpiderManager'\n\nin mybot/spidermanager.py:\nfrom mybot.spider import MyParametrizedSpider\n\nclass MySpiderManager(object):\n    loaded = True\n\n    def fromdomain(self, name):\n        start_urls, extra_domain_names, regexes = self._get_spider_info(name)\n        return MyParametrizedSpider(name, start_urls, extra_domain_names, regexes)\n\n    def close_spider(self, spider):\n        # Put here code you want to run before spiders is closed\n        pass\n\n    def _get_spider_info(self, name):\n        # query your backend (maybe a sqldb) using `name` as primary key, \n        # and return start_urls, extra_domains and regexes\n        ...\n        return (start_urls, extra_domains, regexes)\n\nand now your custom spider class, in mybot/spider.py:\nfrom scrapy.spider import BaseSpider\n\nclass MyParametrizedSpider(BaseSpider):\n\n    def __init__(self, name, start_urls, extra_domain_names, regexes):\n        self.domain_name = name\n        self.start_urls = start_urls\n        self.extra_domain_names = extra_domain_names\n        self.regexes = regexes\n\n     def parse(self, response):\n         ...\n\nNotes:\n\nYou can extend CrawlSpider too if you want to take advantage of its Rules system\nTo run a spider use:  ./scrapy-ctl.py crawl <name>, where name is passed to SpiderManager.fromdomain and is the key to retreive more spider info from the backend system\nAs solution overrides default SpiderManager, coding a classic spider (a python module per SPIDER) doesn't works, but, I think this is not an issue for you. More info on default spiders manager TwistedPluginSpiderManager\n\n"", ""\nWhat you need is to dynamically create spider classes, subclassing your favorite generic spider class as supplied by scrapy (CrawlSpider subclasses with your rules added, or XmlFeedSpider, or whatever) and adding domain_name, start_urls, and possibly extra_domain_names (and/or start_requests(), etc), as you get or deduce them from your GUI (or config file, or whatever).\nPython makes it easy to perform such dynamic creation of class objects; a very simple example might be:\nfrom scrapy import spider\n\ndef makespider(domain_name, start_urls,\n               basecls=spider.BaseSpider):\n  return type(domain_name + 'Spider',\n              (basecls,),\n              {'domain_name': domain_name,\n               'start_urls': start_urls})\n\nallspiders = []\nfor domain, urls in listofdomainurlpairs:\n  allspiders.append(makespider(domain, urls))\n\nThis gives you a list of very bare-bone spider classes -- you'll probably want to add parse methods to them before you instantiate them.  Season to taste...;-).\n"", ""\nShameless self promotion on domo! you'll need to instantiate the crawler as given in the examples, for your project.\nAlso you'll need to make the crawler configurable on runtime, which is simply passing the configuration to crawler, and overriding the settings on runtime, when configuration changed. \n"", '\nNow it is extremely easy to configure scrapy for these purposes:\n\nAbout the first urls to visit, you can pass it as an attribute on the spider call with -a, and use the start_requests function to setup how to start the spider\nYou don\'t need to setup the allowed_domains variable for the spiders. If you don\'t include that class variable, the spider will be able to allow every domain.\n\nIt should end up to something like:\nclass MySpider(Spider):\n\n    name = ""myspider""\n\n    def start_requests(self):\n        yield Request(self.start_url, callback=self.parse)\n\n\n    def parse(self, response):\n        ...\n\nand you should call it with:\nscrapy crawl myspider -a start_url=""http://example.com""\n\n']",https://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites,web-crawler
Removing all spaces in text file with Python 3.x,"
So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this:
https://example.com/asdf.html                                (note the spaces)
https://example.com/johndoe.php                              (again)

I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files.
",51k,"
            9
        ","[""\nSeems like you can't directly edit a python file, so here is my suggestion:\n# first get all lines from file\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()\n\n# remove spaces\nlines = [line.replace(' ', '') for line in lines]\n\n# finally, write lines in the file\nwith open('file.txt', 'w') as f:\n    f.writelines(lines)\n\n"", ""\nYou can open file and read line by line and remove white space - \nPython 3.x:\nwith open('filename') as f:\n    for line in f:\n        print(line.strip())\n\nPython 2.x:\nwith open('filename') as f:\n    for line in f:\n        print line.strip()\n\nIt will remove space from each line and print it.\nHope it helps!\n"", ""\nRead text from file, remove spaces, write text to file:\nwith open('file.txt', 'r') as f:\n    txt = f.read().replace(' ', '')\n\nwith open('file.txt', 'w') as f:\n    f.write(txt)\n\nIn @Leonardo Chirivì's solution it's unnecessary to create a list to store file contents when a string is sufficient and more memory efficient.  The .replace(' ', '') operation is only called once on the string, which is more efficient than iterating through a list performing replace for each line individually.\nTo avoid opening the file twice:\nwith open('file.txt', 'r+') as f:\n    txt = f.read().replace(' ', '')\n    f.seek(0)\n    f.write(txt)\n    f.truncate()\n\nIt would be more efficient to only open the file once.  This requires moving the file pointer back to the start of the file after reading, as well as truncating any possibly remaining content left over after you write back to the file.  A drawback to this solution however is that is not as easily readable.\n"", ""\nI had something similar that I'd been dealing with.\nThis is what worked for me (Note: This converts from 2+ spaces into a comma, but if you read below the code block, I explain how you can get rid of ALL whitespaces):\nimport re\n\n# read the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt') as f:\n    read_file = f.read()\n    print(type(read_file)) # to confirm that it's a string\n\nread_file = re.sub(r'\\s{2,}', ',', read_file) # find/convert 2+ whitespace into ','\n\n# write the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt', 'w') as f:\n    f.writelines('read_file')\n\nThis helped me then send the updated data to a CSV, which suited my need, but it can help for you as well, so instead of converting it to a comma (','), you can convert it to an empty string (''), and then [or] use a read_file.replace(' ', '') method if you don't need any whitespaces at all.\n"", ""\nLets not forget about adding back the \\n to go to the next row.\nThe complete function would be :\nwith open(str_path, 'r') as file :\n    str_lines = file.readlines()\n\n# remove spaces    \nif bl_right is True:    \n    str_lines = [line.rstrip() + '\\n' for line in str_lines]\nelif bl_left is True:   \n    str_lines = [line.lstrip() + '\\n' for line in str_lines]\nelse:                   \n    str_lines = [line.strip() + '\\n' for line in str_lines]\n\n# Write the file out again\nwith open(str_path, 'w') as file:\n    file.writelines(str_lines)\n\n""]",https://stackoverflow.com/questions/43447221/removing-all-spaces-in-text-file-with-python-3-x,web-crawler
Concurrent downloads - Python,"
the plan is this:
I download a webpage, collect a list of images parsed in the DOM and then download these. After this I would iterate through the images in order to evaluate which image is best suited to represent the webpage.
Problem is that images are downloaded 1 by 1 and this can take quite some time.

It would be great if someone could point me in some direction regarding the topic.
Help would be very much appreciated.
",7k,"
            9
        ","['\nSpeeding up crawling is basically Eventlet\'s main use case.  It\'s deeply fast -- we have an application that has to hit 2,000,000 urls in a few minutes.  It makes use of the fastest event interface on your system (epoll, generally), and uses greenthreads (which are built on top of coroutines and are very inexpensive) to make it easy to write.\nHere\'s an example from the docs:\nurls = [""http://www.google.com/intl/en_ALL/images/logo.gif"",\n     ""https://wiki.secondlife.com/w/images/secondlife.jpg"",\n     ""http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif""]\n\nimport eventlet\nfrom eventlet.green import urllib2  \n\ndef fetch(url):\n  body = urllib2.urlopen(url).read()\n  return url, body\n\npool = eventlet.GreenPool()\nfor url, body in pool.imap(fetch, urls):\n  print ""got body from"", url, ""of length"", len(body)\n\nThis is a pretty good starting point for developing a more fully-featured crawler.  Feel free to pop in to #eventlet on Freenode to ask for help.\n[update: I added a more-complex recursive web crawler example to the docs.  I swear it was in the works before this question was asked, but the question did finally inspire me to finish it.  :)]\n', ""\nWhile threading is certainly a possibility, I would instead suggest asyncore -- there's an excellent example here which shows exactly the simultaneous fetching of two URLs (easy to generalize to any list of URLs!).\n"", '\nHere is an article on threading which uses url fetching as an example.\n', '\nNowadays there are excellent Python libs you might want to use - urllib3 and requests\n']",https://stackoverflow.com/questions/2360291/concurrent-downloads-python,web-crawler
Apache HTTPClient throws java.net.SocketException: Connection reset for many domains,"
I'm creating a (well behaved) web spider and I notice that some servers are causing Apache HttpClient to give me a SocketException -- specifically:
java.net.SocketException: Connection reset

The code that causes this is:
// Execute the request
HttpResponse response; 
try {
    response = httpclient.execute(httpget); //httpclient is of type HttpClient
} catch (NullPointerException e) {
    return;//deep down in apache http sometimes throws a null pointer...  
}

For most servers it's just fine.  But for others, it immediately throws a SocketException.
Example of site that causes immediate SocketException: http://www.bhphotovideo.com/
Works great (as do most websites): http://www.google.com/
Now, as you can see, www.bhphotovideo.com loads fine in a web browser.  It also loads fine when I don't use Apache's HTTP Client.  (Code like this:)
 HttpURLConnection c = (HttpURLConnection)url.openConnection();  
 BufferedInputStream in = new BufferedInputStream(c.getInputStream());  
 Reader r = new InputStreamReader(in);     

 int i;  
 while ((i = r.read()) != -1) {  
      source.append((char) i);  
 }  

So, why don't I just use this code instead?  Well there are some key features in Apache's HTTP Client that I need to use.
Does anyone know what causes some servers to cause this exception?
Research so far:

Problem occurs on my local Mac dev machines AND an AWS EC2 Instance, so it's not a local firewall.
It seems the error isn't caused by the remote machine because the exception doesn't say ""by peer""
This stack overflow seems relavent java.net.SocketException: Connection reset but the answers don't show why this would happen only from Apache HTTP Client and not other approaches.

Bonus question: I'm doing a fair amount of crawling with this system.  Is there generally a better Java class for this other than Apache HTTP Client?  I've found a number of issues (such as the NullPointerException I have to catch in the code above).  It seems that HTTPClient is very picky about server communications -- more picky than I'd like for a crawler that can't just break when a server doesn't behave.
Thanks all! 
Solution
Honestly, I don't have a perfect solution, but it works, so that's good enough for me.
As pointed out by oleg below, Bixo has created a crawler that customizes HttpClient to be more forgiving to servers.  To ""get around"" the issue more than fix it, I just used SimpleHttpFetcher provided by Bixo here:
(linked removed - SO thinks I'm a spammer, so you'll have to google it yourself)
SimpleHttpFetcher fetch = new SimpleHttpFetcher(new UserAgent(""botname"",""contact@yourcompany.com"",""ENTER URL""));
try {
    FetchedResult result = fetch.fetch(""ENTER URL"");
    System.out.println(new String(result.getContent()));
} catch (BaseFetchException e) {
    e.printStackTrace();
}

The down side to this solution is that there are a lot of dependencies for Bixo -- so this may not be a good work around for everyone.  However, you can always just work through their use of DefaultHttpClient and see how they instantiated it to get it to work.  I decided to use the whole class because it handles some things for me, like automatic redirect following (and reporting the final destination url) that are helpful.
Thanks for the help all.
Edit: TinyBixo
Hi all.  So, I loved how Bixo worked, but didn't like that it had so many dependencies (including all of Hadoop).  So, I created a vastly simplified Bixo, without all the dependencies.  If you're running into the problems above, I would recommend using it (and feel free to make pull requests if you'd like to update it!)
It's available here: https://github.com/juliuss/TinyBixo 
",44k,"
            9
        ","[""\nFirst, to answer your question:\nThe connection reset was caused by a problem on the server side. Most likely the server failed to parse the request or was unable to process it and dropped the connection as a result without returning a valid response. There is likely something in the HTTP requests generated by HttpClient that causes server side logic to fail, probably due to a server side bug. Just because the error message does not say 'by peer' does not mean the connection reset took place on the client side. \nA few remarks:\n(1) Several popular web crawlers such as bixo http://openbixo.org/ use HttpClient without major issues but pretty much of them had to tweak HttpClient behavior to make it more lenient about common HTTP protocol violations. Per default HttpClient is rather strict about the HTTP protocol compliance.\n(2) Why did not you report the NPE problem or any other problem you have been experiencing to the HttpClient project?\n"", '\nThese two settings will sometimes help:\n client.getParams().setParameter(""http.socket.timeout"", new Integer(0));\n client.getParams().setParameter(""http.connection.stalecheck"", new  Boolean(true));\n\nThe first sets the socket timeout to be infinite.\n', '\nTry getting a network trace using wireshark, and augment that with log4j logging of the HTTPClient. That should show why the connection is being reset\n']",https://stackoverflow.com/questions/5280577/apache-httpclient-throws-java-net-socketexception-connection-reset-for-many-dom,web-crawler
Web Crawling (Ajax/JavaScript enabled pages) using java,"
I am very new to this web crawling. I am using crawler4j to crawl the websites. I am collecting the required information by crawling these sites. My problem here is I was unable to crawl the content for the following site. http://www.sciencedirect.com/science/article/pii/S1568494612005741. I want to crawl the following information from the aforementioned site (Please take a look at the attached screenshot).

If you observe the attached screenshot it has three names (Highlighted in red boxes). If you click one of the link you will see a popup and that popup contains the whole information about that author. I want to crawl the information which are there in that popup.
I am using the following code to crawl the content.
public class WebContentDownloader {

private Parser parser;
private PageFetcher pageFetcher;

public WebContentDownloader() {
    CrawlConfig config = new CrawlConfig();
    parser = new Parser(config);
    pageFetcher = new PageFetcher(config);
}

private Page download(String url) {
    WebURL curURL = new WebURL();
    curURL.setURL(url);
    PageFetchResult fetchResult = null;
    try {
        fetchResult = pageFetcher.fetchHeader(curURL);
        if (fetchResult.getStatusCode() == HttpStatus.SC_OK) {
            try {
                Page page = new Page(curURL);
                fetchResult.fetchContent(page);
                if (parser.parse(page, curURL.getURL())) {
                    return page;
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    } finally {
        if (fetchResult != null) {
            fetchResult.discardContentIfNotConsumed();
        }
    }
    return null;
}

private String processUrl(String url) {
    System.out.println(""Processing: "" + url);
    Page page = download(url);
    if (page != null) {
        ParseData parseData = page.getParseData();
        if (parseData != null) {
            if (parseData instanceof HtmlParseData) {
                HtmlParseData htmlParseData = (HtmlParseData) parseData;
                return htmlParseData.getHtml();
            }
        } else {
            System.out.println(""Couldn't parse the content of the page."");
        }
    } else {
        System.out.println(""Couldn't fetch the content of the page."");
    }
    return null;
}

public String getHtmlContent(String argUrl) {
    return this.processUrl(argUrl);
}
}

I was able to crawl the content from the aforementioned link/site. But it doesn't have the information what I marked in the red boxes. I think those are the dynamic links.

My question is how can I crawl the content from the aforementioned link/website...???
How to crawl the content from Ajax/JavaScript based websites...???

Please can anyone help me on this.
Thanks & Regards,
Amar
",20k,"
            9
        ","['\nHi I found the workaround with the another library. I used \nSelinium WebDriver (org.openqa.selenium.WebDriver) library to extract the dynamic content. Here is the sample code.\npublic class CollectUrls {\n\nprivate WebDriver driver;\n\npublic CollectUrls() {\n    this.driver = new FirefoxDriver();\n    this.driver.manage().timeouts().implicitlyWait(30, TimeUnit.SECONDS);\n}\n\nprotected void next(String url, List<String> argUrlsList) {\n    this.driver.get(url);\n    String htmlContent = this.driver.getPageSource();\n}\n\nHere the ""htmlContent"" is the required one. Please let me know if you face any issues...???\nThanks,\nAmar\n', ""\nSimply said, Crawler4j is static crawler. Meaning that it can't parse the JavaScript on a page. So there is no way of getting the content you want by crawling that specific page you mentioned. Of course there are some workarounds to get it working.\nIf it is just this page you want to crawl, you could use a connection debugger. Check out this question for some tools. Find out which page the AJAX-request calls, and crawl that page.\nIf you have various websites which have dynamic content (JavaScript/ajax), you should consider using a dynamic-content-enabled crawler, like Crawljax (also written in Java).\n"", '\nI have find out the Solution of Dynamic Web page Crawling using Aperture and Selenium.Web Driver.\nAperture is Crawling Tools and Selenium is Testing Tools which can able to rendering Inspect Element. \n\n1. Extract the Aperture- core Jar file by Decompiler Tools and Create a Simple Web Crawling Java program. (https://svn.code.sf.net/p/aperture/code/aperture/trunk/)\n2. Download Selenium. WebDriver Jar Files and Added to Your Program.\n3. Go to CreatedDataObjec() method in org.semanticdesktop.aperture.accessor.http.HttpAccessor.(Aperture Decompiler).\nAdded Below Coding \n\n   WebDriver driver = new FirefoxDriver();\n   String baseurl=uri.toString();\n   driver.get(uri.toString());\n   String str = driver.getPageSource();\n        driver.close();\n stream= new ByteArrayInputStream(str.getBytes());\n\n']",https://stackoverflow.com/questions/24365154/web-crawling-ajax-javascript-enabled-pages-using-java,web-crawler
Mass Downloading of Webpages C#,"
My application requires that I download a large amount of webpages into memory for further parsing and processing. What is the fastest way to do it? My current method (shown below) seems to be too slow and occasionally results in timeouts.
for (int i = 1; i<=pages; i++)
{
    string page_specific_link = baseurl + ""&page="" + i.ToString();

    try
    {    
        WebClient client = new WebClient();
        var pagesource = client.DownloadString(page_specific_link);
        client.Dispose();
        sourcelist.Add(pagesource);
    }
    catch (Exception)
    {
    }
}

",5k,"
            8
        ","['\nThe way you approach this problem is going to depend very much on how many pages you want to download, and how many sites you\'re referencing.\nI\'ll use a good round number like 1,000. If you want to download that many pages from a single site, it\'s going to take a lot longer than if you want to download 1,000 pages that are spread out across dozens or hundreds of sites. The reason is that if you hit a single site with a whole bunch of concurrent requests, you\'ll probably end up getting blocked.\nSo you have to implement a type of ""politeness policy,"" that issues a delay between multiple requests on a single site. The length of that delay depends on a number of things. If the site\'s robots.txt file has a crawl-delay entry, you should respect that. If they don\'t want you accessing more than one page per minute, then that\'s as fast as you should crawl. If there\'s no crawl-delay, you should base your delay on how long it takes a site to respond. For example, if you can download a page from the site in 500 milliseconds, you set your delay to X. If it takes a full second, set your delay to 2X. You can probably cap your delay to 60 seconds (unless crawl-delay is longer), and I would recommend that you set a minimum delay of 5 to 10 seconds.\nI wouldn\'t recommend using Parallel.ForEach for this. My testing has shown that it doesn\'t do a good job. Sometimes it over-taxes the connection and often it doesn\'t allow enough concurrent connections. I would instead create a queue of WebClient instances and then write something like:\n// Create queue of WebClient instances\nBlockingCollection<WebClient> ClientQueue = new BlockingCollection<WebClient>();\n// Initialize queue with some number of WebClient instances\n\n// now process urls\nforeach (var url in urls_to_download)\n{\n    var worker = ClientQueue.Take();\n    worker.DownloadStringAsync(url, ...);\n}\n\nWhen you initialize the WebClient instances that go into the queue, set their OnDownloadStringCompleted event handlers to point to a completed event handler. That handler should save the string to a file (or perhaps you should just use DownloadFileAsync), and then the client, adds itself back to the ClientQueue.\nIn my testing, I\'ve been able to support 10 to 15 concurrent connections with this method. Any more than that and I run into problems with DNS resolution (`DownloadStringAsync\'  doesn\'t do the DNS resolution asynchronously). You can get more connections, but doing so is a lot of work.\nThat\'s the approach I\'ve taken in the past, and it\'s worked very well for downloading thousands of pages quickly. It\'s definitely not the approach I took with my high performance Web crawler, though.\nI should also note that there is a huge difference in resource usage between these two blocks of code:\nWebClient MyWebClient = new WebClient();\nforeach (var url in urls_to_download)\n{\n    MyWebClient.DownloadString(url);\n}\n\n---------------\n\nforeach (var url in urls_to_download)\n{\n    WebClient MyWebClient = new WebClient();\n    MyWebClient.DownloadString(url);\n}\n\nThe first allocates a single WebClient instance that is used for all requests. The second allocates one WebClient for each request. The difference is huge. WebClient uses a lot of system resources, and allocating thousands of them in a relatively short time is going to impact performance. Believe me ... I\'ve run into this. You\'re better off allocating just 10 or 20 WebClients (as many as you need for concurrent processing), rather than allocating one per request.\n', '\nWhy not just use a web crawling framework. It can handle all the stuff for you like (multithreading, httprequests, parsing links, scheduling, politeness, etc..). \nAbot (https://code.google.com/p/abot/) handles all that stuff for you and is written in c#.\n', '\nIn addition to @Davids perfectly valid answer, I want to add a slightly cleaner ""version"" of his approach.\nvar pages = new List<string> { ""http://bing.com"", ""http://stackoverflow.com"" };\nvar sources = new BlockingCollection<string>();\n\nParallel.ForEach(pages, x =>\n{\n    using(var client = new WebClient())\n    {\n        var pagesource = client.DownloadString(x);\n        sources.Add(pagesource);\n    }\n});\n\n\nYet another approach, that uses async:\nstatic IEnumerable<string> GetSources(List<string> pages)\n{\n    var sources = new BlockingCollection<string>();\n    var latch = new CountdownEvent(pages.Count);\n\n    foreach (var p in pages)\n    {\n        using (var wc = new WebClient())\n        {\n            wc.DownloadStringCompleted += (x, e) =>\n            {\n                sources.Add(e.Result);\n                latch.Signal();\n            };\n\n            wc.DownloadStringAsync(new Uri(p));\n        }\n    }\n\n    latch.Wait();\n\n    return sources;\n}\n\n', '\nYou should use parallel programming for this purpose.\nThere are a lot of ways to achieve what u want; the easiest would be something like this:\nvar pageList = new List<string>();\n\nfor (int i = 1; i <= pages; i++)\n{\n  pageList.Add(baseurl + ""&page="" + i.ToString());\n}\n\n\n// pageList  is a list of urls\nParallel.ForEach<string>(pageList, (page) =>\n{\n  try\n    {\n      WebClient client = new WebClient();\n      var pagesource = client.DownloadString(page);\n      client.Dispose();\n      lock (sourcelist)\n      sourcelist.Add(pagesource);\n    }\n\n    catch (Exception) {}\n});\n\n', '\nI Had a similar Case ,and that\'s how i solved \nusing System;\n    using System.Threading;\n    using System.Collections.Generic;\n    using System.Net;\n    using System.IO;\n\nnamespace WebClientApp\n{\nclass MainClassApp\n{\n    private static int requests = 0;\n    private static object requests_lock = new object();\n\n    public static void Main() {\n\n        List<string> urls = new List<string> { ""http://www.google.com"", ""http://www.slashdot.org""};\n        foreach(var url in urls) {\n            ThreadPool.QueueUserWorkItem(GetUrl, url);\n        }\n\n        int cur_req = 0;\n\n        while(cur_req<urls.Count) {\n\n            lock(requests_lock) {\n                cur_req = requests; \n            }\n\n            Thread.Sleep(1000);\n        }\n\n        Console.WriteLine(""Done"");\n    }\n\nprivate static void GetUrl(Object the_url) {\n\n        string url = (string)the_url;\n        WebClient client = new WebClient();\n        Stream data = client.OpenRead (url);\n\n        StreamReader reader = new StreamReader(data);\n        string html = reader.ReadToEnd ();\n\n        /// Do something with html\n        Console.WriteLine(html);\n\n        lock(requests_lock) {\n            //Maybe you could add here the HTML to SourceList\n            requests++; \n        }\n    }\n}\n\nYou should think using Paralel\'s because the slow speed is because you\'re software is waiting for I/O and why not while a thread i waiting for I/O another one get started.\n', '\nWhile the other answers are perfectly valid, all of them (at the time of this writing) are neglecting something very important: calls to the web are IO bound, having a thread wait on an operation like this is going to strain system resources and have an impact on your system resources.\nWhat you really want to do is take advantage of the async methods on the WebClient class (as some have pointed out) as well as the Task Parallel Library\'s ability to handle the Event-Based Asynchronous Pattern.\nFirst, you would get the urls that you want to download:\nIEnumerable<Uri> urls = pages.Select(i => new Uri(baseurl + \n    ""&page="" + i.ToString(CultureInfo.InvariantCulture)));\n\nThen, you would create a new WebClient instance for each url, using the TaskCompletionSource<T> class to handle the calls asynchronously (this won\'t burn a thread):\nIEnumerable<Task<Tuple<Uri, string>> tasks = urls.Select(url => {\n    // Create the task completion source.\n    var tcs = new TaskCompletionSource<Tuple<Uri, string>>();\n\n    // The web client.\n    var wc = new WebClient();\n\n    // Attach to the DownloadStringCompleted event.\n    client.DownloadStringCompleted += (s, e) => {\n        // Dispose of the client when done.\n        using (wc)\n        {\n            // If there is an error, set it.\n            if (e.Error != null) \n            {\n                tcs.SetException(e.Error);\n            }\n            // Otherwise, set cancelled if cancelled.\n            else if (e.Cancelled) \n            {\n                tcs.SetCanceled();\n            }\n            else \n            {\n                // Set the result.\n                tcs.SetResult(new Tuple<string, string>(url, e.Result));\n            }\n        }\n    };\n\n    // Start the process asynchronously, don\'t burn a thread.\n    wc.DownloadStringAsync(url);\n\n    // Return the task.\n    return tcs.Task;\n});\n\nNow you have an IEnumerable<T> which you can convert to an array and wait on all of the results using Task.WaitAll:\n// Materialize the tasks.\nTask<Tuple<Uri, string>> materializedTasks = tasks.ToArray();\n\n// Wait for all to complete.\nTask.WaitAll(materializedTasks);\n\nThen, you can just use Result property on the Task<T> instances to get the pair of the url and the content:\n// Cycle through each of the results.\nforeach (Tuple<Uri, string> pair in materializedTasks.Select(t => t.Result))\n{\n    // pair.Item1 will contain the Uri.\n    // pair.Item2 will contain the content.\n}\n\nNote that the above code has the caveat of not having an error handling.\nIf you wanted to get even more throughput, instead of waiting for the entire list to be finished, you could process the content of a single page after it\'s done downloading; Task<T> is meant to be used like a pipeline, when you\'ve completed your unit of work, have it continue to the next one instead of waiting for all of the items to be done (if they can be done in an asynchronous manner).\n', '\nI am using an active Threads count and a arbitrary limit:\nprivate static volatile int activeThreads = 0;\n\npublic static void RecordData()\n{\n  var nbThreads = 10;\n  var source = db.ListOfUrls; // Thousands urls\n  var iterations = source.Length / groupSize; \n  for (int i = 0; i < iterations; i++)\n  {\n    var subList = source.Skip(groupSize* i).Take(groupSize);\n    Parallel.ForEach(subList, (item) => RecordUri(item)); \n    //I want to wait here until process further data to avoid overload\n    while (activeThreads > 30) Thread.Sleep(100);\n  }\n}\n\nprivate static async Task RecordUri(Uri uri)\n{\n   using (WebClient wc = new WebClient())\n   {\n      Interlocked.Increment(ref activeThreads);\n      wc.DownloadStringCompleted += (sender, e) => Interlocked.Decrement(ref iterationsCount);\n      var jsonData = """";\n      RootObject root;\n      jsonData = await wc.DownloadStringTaskAsync(uri);\n      var root = JsonConvert.DeserializeObject<RootObject>(jsonData);\n      RecordData(root)\n    }\n}\n\n']",https://stackoverflow.com/questions/7474413/mass-downloading-of-webpages-c-sharp,web-crawler
Running Multiple spiders in scrapy for 1 website in parallel?,"
I want to crawl a website with 2 parts and my script is not as fast as I need.
Is it possible to launch 2 spiders, one for scraping the first part and the second one for the second part? 
I tried to have 2 different classes, and run them
scrapy crawl firstSpider
scrapy crawl secondSpider

but i think that it is not smart.
I read the documentation of scrapyd but I don't know if it's good for my case.
",21k,"
            7
        ","['\nI think what you are looking for is something like this:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\nprocess = CrawlerProcess()\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start() # the script will block here until all crawling jobs are finished\n\nYou can read more at: running-multiple-spiders-in-the-same-process.\n', '\nOr you can run with like this, you need to save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3) :\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (""Running spider %s"" % (spider_name))\n    process.crawl(spider_name,query=""dvh"") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n\n', '\nBetter solution is (if you have multiple spiders) it dynamically get spiders and run them.\nfrom scrapy import spiderloader\nfrom scrapy.utils import project\nfrom twisted.internet.defer import inlineCallbacks\n\n\n@inlineCallbacks\ndef crawl():\n    settings = project.get_project_settings()\n    spider_loader = spiderloader.SpiderLoader.from_settings(settings)\n    spiders = spider_loader.list()\n    classes = [spider_loader.load(name) for name in spiders]\n    for my_spider in classes:\n        yield runner.crawl(my_spider)\n    reactor.stop()\n\ncrawl()\nreactor.run()\n\n(Second Solution):\nBecause spiders.list() is deprecated in Scrapy 1.4 Yuda solution should be converted to something like\nfrom scrapy import spiderloader    \nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings)\nspider_loader = spiderloader.SpiderLoader.from_settings(settings)\n\nfor spider_name in spider_loader.list():\n    print(""Running spider %s"" % (spider_name))\n    process.crawl(spider_name)\nprocess.start()\n\n']",https://stackoverflow.com/questions/39365131/running-multiple-spiders-in-scrapy-for-1-website-in-parallel,web-crawler
Send parallel requests but only one per host with HttpClient and Polly to gracefully handle 429 responses,"
Intro:
I am building a single-node web crawler to simply validate URLs are 200 OK in a .NET Core console application. I have a collection of URLs at different hosts to which I am sending requests with HttpClient. I am fairly new to using Polly and TPL Dataflow.
Requirements:

I want to support sending multiple HTTP requests in parallel with a
configurable MaxDegreeOfParallelism.
I want to limit the number of parallel requests to any given host to 1 (or configurable). This is in order to gracefully handle per-host 429 TooManyRequests responses with a Polly policy. Alternatively, I could maybe use a Circuit Breaker to cancel concurrent requests to the same host on receipt of one 429 response and then proceed one-at-a-time to that specific host?
I am perfectly fine with not using TPL Dataflow at all in favor of maybe using a Polly Bulkhead or some other mechanism for throttled parallel requests, but I am not sure what that configuration would look like in order to implement requirement #2.

Current Implementation:
My current implementation works, except that I often see that I'll have x parallel requests to the same host return 429 at about the same time... Then, they all pause for the retry policy... Then, they all slam the same host again at the same time often still receiving 429s. Even if I distribute multiple instances of the same host evenly throughout the queue, my URL collection is overweighted with a few specific hosts that still start generating 429s eventually.
After receiving a 429, I think I only want to send one concurrent request to that host going forward to respect the remote host and pursue 200s. 
Validator Method:
public async Task<int> GetValidCount(IEnumerable<Uri> urls, CancellationToken cancellationToken)
{
    var validator = new TransformBlock<Uri, bool>(
        async u => (await _httpClient.GetAsync(u, HttpCompletionOption.ResponseHeadersRead, cancellationToken)).IsSuccessStatusCode,
        new ExecutionDataflowBlockOptions {MaxDegreeOfParallelism = MaxDegreeOfParallelism}
    );
    foreach (var url in urls)
        await validator.SendAsync(url, cancellationToken);
    validator.Complete();
    var validUrlCount = 0;
    while (await validator.OutputAvailableAsync(cancellationToken))
    {
        if(await validator.ReceiveAsync(cancellationToken))
            validUrlCount++;
    }
    await validator.Completion;
    return validUrlCount;
}

The Polly policy applied to the HttpClient instance used in GetValidCount() above.
IAsyncPolicy<HttpResponseMessage> waitAndRetryTooManyRequests = Policy
    .HandleResult<HttpResponseMessage>(r => r.StatusCode == HttpStatusCode.TooManyRequests)
    .WaitAndRetryAsync(3,
        (retryCount, response, context) =>
            response.Result?.Headers.RetryAfter.Delta ?? TimeSpan.FromMilliseconds(120),
        async (response, timespan, retryCount, context) =>
        {
            // log stuff
        });

Question:
How can I modify or replace this solution to add satisfaction of requirement #2?
",1k,"
            6
        ","['\nI\'d try to introduce some sort of a flag LimitedMode  to detect that this particular client is entered in limited mode. Below I declare two policies - one simple retry policy just to catch TooManyRequests and set the flag. The second policy is a out-of-the-box BulkHead policy.\n    public void ConfigureServices(IServiceCollection services)\n    {\n        /* other configuration */\n\n        var registry = services.AddPolicyRegistry();\n\n        var catchPolicy = Policy.HandleResult<HttpResponseMessage>(r =>\n            {\n                LimitedMode = r.StatusCode == HttpStatusCode.TooManyRequests;\n                return false;\n            })\n            .WaitAndRetryAsync(1, i => TimeSpan.FromSeconds(3)); \n\n        var bulkHead = Policy.BulkheadAsync<HttpResponseMessage>(1, 10, OnBulkheadRejectedAsync);\n\n        registry.Add(""catchPolicy"", catchPolicy);\n        registry.Add(""bulkHead"", bulkHead);\n\n        services.AddHttpClient<CrapyWeatherApiClient>((client) =>\n        {\n            client.BaseAddress = new Uri(""hosturl"");\n        }).AddPolicyHandlerFromRegistry(PolicySelector);\n    }\n\nThen you may want to dynamically decide on which policy to apply using the PolicySelector mechanism: in case the limited mode is active - wrap bulk head policy with catch 429 policy. If the success status code received - switch back to regular mode without a bulkhead.\n    private IAsyncPolicy<HttpResponseMessage> PolicySelector(IReadOnlyPolicyRegistry<string> registry, HttpRequestMessage request)\n    {\n        var catchPolicy = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""catchPolicy"");\n        var bulkHead = registry.Get<IAsyncPolicy<HttpResponseMessage>>(""bulkHead"");\n        if (LimitedMode)\n        {\n            return catchPolicy.WrapAsync(bulkHead);\n        }\n\n        return catchPolicy;\n    }        \n\n', '\nHere is a method that creates a TransformBlock which prevents concurrent execution for messages with the same key. The key of each message is obtained by invoking the supplied keySelector function. Messages with the same key are processed sequentially to each other (not in parallel). The key is also passed as an argument to the transform function, because it can be useful in some cases.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, Task<TOutput>> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    if (keySelector == null) throw new ArgumentNullException(nameof(keySelector));\n    if (dataflowBlockOptions == null)\n        throw new ArgumentNullException(nameof(dataflowBlockOptions));\n    keyComparer = keyComparer ?? EqualityComparer<TKey>.Default;\n\n    var internalCTS = CancellationTokenSource\n        .CreateLinkedTokenSource(dataflowBlockOptions.CancellationToken);\n\n    var maxDOP = dataflowBlockOptions.MaxDegreeOfParallelism;\n    var taskScheduler = dataflowBlockOptions.TaskScheduler;\n\n    var perKeySemaphores = new ConcurrentDictionary<TKey, SemaphoreSlim>(\n        keyComparer);\n\n    SemaphoreSlim maxDopSemaphore;\n    if (maxDOP == DataflowBlockOptions.Unbounded)\n    {\n        maxDopSemaphore = new SemaphoreSlim(Int32.MaxValue);\n    }\n    else\n    {\n        maxDopSemaphore = new SemaphoreSlim(maxDOP, maxDOP);\n\n        // The degree of parallelism is controlled by the semaphore\n        dataflowBlockOptions.MaxDegreeOfParallelism = DataflowBlockOptions.Unbounded;\n\n        // Use a limited-concurrency scheduler for preserving the processing order\n        dataflowBlockOptions.TaskScheduler = new ConcurrentExclusiveSchedulerPair(\n            taskScheduler, maxDOP).ConcurrentScheduler;\n    }\n\n    var block = new TransformBlock<TInput, TOutput>(async item =>\n    {\n        var key = keySelector(item);\n        var perKeySemaphore = perKeySemaphores\n            .GetOrAdd(key, _ => new SemaphoreSlim(1, 1));\n\n        // Continue on captured context before invoking the transform\n        await perKeySemaphore.WaitAsync(internalCTS.Token);\n        try\n        {\n            await maxDopSemaphore.WaitAsync(internalCTS.Token);\n            try\n            {\n                return await transform(item, key).ConfigureAwait(false);\n            }\n            catch (Exception ex) when (!(ex is OperationCanceledException))\n            {\n                internalCTS.Cancel(); // The block has failed\n                throw;\n            }\n            finally\n            {\n                maxDopSemaphore.Release();\n            }\n        }\n        finally\n        {\n            perKeySemaphore.Release();\n        }\n    }, dataflowBlockOptions);\n\n    dataflowBlockOptions.MaxDegreeOfParallelism = maxDOP; // Restore initial value\n    dataflowBlockOptions.TaskScheduler = taskScheduler; // Restore initial value\n    return block;\n}\n\nUsage example:\nvar validator = CreateExclusivePerKeyTransformBlock<Uri, string, bool>(\n    async (uri, host) =>\n    {\n        return (await _httpClient.GetAsync(uri, HttpCompletionOption\n            .ResponseHeadersRead, token)).IsSuccessStatusCode;\n    },\n    new ExecutionDataflowBlockOptions\n    {\n        MaxDegreeOfParallelism = 30,\n        CancellationToken = token,\n    },\n    keySelector: uri => uri.Host,\n    keyComparer: StringComparer.OrdinalIgnoreCase);\n\nAll execution options are supported (MaxDegreeOfParallelism, BoundedCapacity, CancellationToken, EnsureOrdered etc).\nBelow is an overload of the CreateExclusivePerKeyTransformBlock that accepts a synchronous delegate, and another method+overload that returns an ActionBlock instead of a TransformBlock, with the same behavior.\npublic static TransformBlock<TInput, TOutput>\n    CreateExclusivePerKeyTransformBlock<TInput, TKey, TOutput>(\n    Func<TInput, TKey, TOutput> transform,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (transform == null) throw new ArgumentNullException(nameof(transform));\n    return CreateExclusivePerKeyTransformBlock(\n        (item, key) => Task.FromResult(transform(item, key)),\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n// An ITargetBlock is similar to an ActionBlock\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Func<TInput, TKey, Task> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    var block = CreateExclusivePerKeyTransformBlock(async (item, key) =>\n        { await action(item, key).ConfigureAwait(false); return (object)null; },\n        dataflowBlockOptions, keySelector, keyComparer);\n    block.LinkTo(DataflowBlock.NullTarget<object>());\n    return block;\n}\n\npublic static ITargetBlock<TInput>\n    CreateExclusivePerKeyActionBlock<TInput, TKey>(\n    Action<TInput, TKey> action,\n    ExecutionDataflowBlockOptions dataflowBlockOptions,\n    Func<TInput, TKey> keySelector,\n    IEqualityComparer<TKey> keyComparer = null)\n{\n    if (action == null) throw new ArgumentNullException(nameof(action));\n    return CreateExclusivePerKeyActionBlock(\n        (item, key) => { action(item, key); return Task.CompletedTask; },\n        dataflowBlockOptions, keySelector, keyComparer);\n}\n\n\nCaution: This class allocates one SemaphoreSlim per key, and keeps a reference to it until the class instance is finally garbage collected. This could be an issue in case the number of different keys is huge. There is an implementation of a less allocatey async lock here, that stores internally only the SemaphoreSlims that are currently in use (plus a small pool of released SemaphoreSlims that can be reused), which could replace the ConcurrentDictionary<TKey, SemaphoreSlim> used by this implementation.\n']",https://stackoverflow.com/questions/57022754/send-parallel-requests-but-only-one-per-host-with-httpclient-and-polly-to-gracef,web-crawler
How to follow all links in CasperJS?,"
I'm having trouble clicking all JavaScript based links in a DOM and saving the
output. The links have the form 
<a id=""html"" href=""javascript:void(0);"" onclick=""goToHtml();"">HTML</a>

the following code works great:
var casper = require('casper').create();

var fs = require('fs');

var firstUrl = 'http://www.testurl.com/test.html';

var css_selector = '#jan_html';

casper.start(firstUrl);

casper.thenClick(css_selector, function(){
    console.log(""whoop"");
});

casper.waitFor(function check() {
    return this.getCurrentUrl() != firstUrl;
}, function then() {
    console.log(this.getCurrentUrl());
    var file_title = this.getTitle().split(' ').join('_') + '.html';
    fs.write(file_title, this.getPageContent());
});

casper.run();

However, how can I get this to work with a selector of ""a"", clicking all
available links and saving content? I'm not sure how to get the clickWhileSelector to remove nodes from the selector as is done here: Click on all links matching a selector
",9k,"
            5
        ","['\nI have this script that first will get all links from a page then save \'href\' attributes to an array, then will iterate over this array and then open each link one by one and echo the url :\nvar casper = require(\'casper\').create({\n    logLevel:""verbose"",\n    debug:true\n});\nvar links;\n\ncasper.start(\'http://localhost:8000\');\n\ncasper.then(function getLinks(){\n     links = this.evaluate(function(){\n        var links = document.getElementsByTagName(\'a\');\n        links = Array.prototype.map.call(links,function(link){\n            return link.getAttribute(\'href\');\n        });\n        return links;\n    });\n});\ncasper.then(function(){\n    this.each(links,function(self,link){\n        self.thenOpen(link,function(a){\n            this.echo(this.getCurrentUrl());\n        });\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n', '\nrusln\'s answer works great if all the links have a meaningful href attribute (actual URL). If you want to click every a that also triggers a javascript function, you may need to iterate some other way over the elements.\nI propose using the XPath generator from stijn de ryck for an element. \n\nYou can then sample all XPaths that are on the page. \nThen you open the page for every a that you have the XPath for and click it by XPath. \nWait a little if it is a single page application\nDo something\n\nvar startURL = \'http://localhost:8000\',\n    xPaths\n    x = require(\'casper\').selectXPath;\n\ncasper.start(startURL);\n\ncasper.then(function getLinks(){\n    xPaths = this.evaluate(function(){\n        // copied from https://stackoverflow.com/a/5178132/1816580\n        function createXPathFromElement(elm) {\n            var allNodes = document.getElementsByTagName(\'*\'); \n            for (var segs = []; elm && elm.nodeType == 1; elm = elm.parentNode) { \n                if (elm.hasAttribute(\'id\')) { \n                        var uniqueIdCount = 0; \n                        for (var n=0;n < allNodes.length;n++) { \n                            if (allNodes[n].hasAttribute(\'id\') && allNodes[n].id == elm.id) uniqueIdCount++; \n                            if (uniqueIdCount > 1) break; \n                        }; \n                        if ( uniqueIdCount == 1) { \n                            segs.unshift(\'id(""\' + elm.getAttribute(\'id\') + \'"")\'); \n                            return segs.join(\'/\'); \n                        } else { \n                            segs.unshift(elm.localName.toLowerCase() + \'[@id=""\' + elm.getAttribute(\'id\') + \'""]\'); \n                        } \n                } else if (elm.hasAttribute(\'class\')) { \n                    segs.unshift(elm.localName.toLowerCase() + \'[@class=""\' + elm.getAttribute(\'class\') + \'""]\'); \n                } else { \n                    for (i = 1, sib = elm.previousSibling; sib; sib = sib.previousSibling) { \n                        if (sib.localName == elm.localName)  i++; }; \n                        segs.unshift(elm.localName.toLowerCase() + \'[\' + i + \']\'); \n                }; \n            }; \n            return segs.length ? \'/\' + segs.join(\'/\') : null; \n        };\n        var links = document.getElementsByTagName(\'a\');\n        var xPaths = Array.prototype.map.call(links, createXPathFromElement);\n        return xPaths;\n    });\n});\ncasper.then(function(){\n    this.each(xPaths, function(self, xpath){\n        self.thenOpen(startURL);\n        self.thenClick(x(xpath));\n        // waiting some time may be necessary for single page applications\n        self.wait(1000);\n        self.then(function(a){\n            // do something meaningful here\n            this.echo(this.getCurrentUrl());\n        });\n\n        // Uncomment the following line in case each click opens a new page instead of staying at the same page\n        //self.back()\n    });\n});\ncasper.run(function(){\n    this.exit();\n});\n\n']",https://stackoverflow.com/questions/20224687/how-to-follow-all-links-in-casperjs,web-crawler
Scrapy upload file,"
I am making a form request to a website using scrapy. The form requires to upload a pdf file, How can we do it in Scrapy. I am trying this like -
FormRequest(url,callback=self.parseSearchResponse,method=""POST"",formdata={'filename':'abc.xyz','file':'path to file/abc.xyz'})

",2k,"
            4
        ","['\nAt this very moment Scrapy has no built-in support for uploading files.\nFile uploading via forms in HTTP was specified in RFC1867. According to the spec, an HTTP request with Content-Type: multipart/form-data is required (in your code it would be application/x-www-form-urlencoded).\nTo achieve file uploading with Scrapy, you would need to:\n\nGet familiar with the basic concepts of HTTP file uploading.\nStart with scrapy.Request (instead of FormRequest).\nGive it a proper Content-Type header value.\nBuild the request body yourself.\n\nSee also: How does HTTP file upload work?\n', '\nI just spent an entire day trying to figure out how to implement this.\nFinally, I came upon a Scrapy pull request from 2016 that was never merged, with an implementation of a multipart form request:\nfrom scrapy import FormRequest\nfrom six.moves.urllib.parse import urljoin, urlencode\nimport lxml.html\nfrom parsel.selector import create_root_node\nimport six\nimport string\nimport random\nfrom scrapy.http.request import Request\nfrom scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n\n\nclass MultipartFormRequest(FormRequest):\n\n    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop(\'formdata\', None)\n\n        kwargs.setdefault(\'method\', \'POST\')\n\n        super(MultipartFormRequest, self).__init__(*args, **kwargs)\n\n        content_type = self.headers.setdefault(b\'Content-Type\', [b\'multipart/form-data\'])[0]\n        method = kwargs.get(\'method\').upper()\n        if formdata and method == \'POST\' and content_type == b\'multipart/form-data\':\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            self._boundary = \'\'\n\n            # encode the data using multipart spec\n            self._boundary = to_bytes(\'\'.join(\n                random.choice(string.digits + string.ascii_letters) for i in range(20)), self.encoding)\n            self.headers[b\'Content-Type\'] = b\'multipart/form-data; boundary=\' + self._boundary\n            request_data = _multpart_encode(items, self._boundary, self.encoding)\n            self._set_body(request_data)\n\n\nclass MultipartFile(object):\n\n    def __init__(self, name, content, mimetype=\'application/octet-stream\'):\n        self.name = name\n        self.content = content\n        self.mimetype = mimetype\n\n\ndef _get_form_url(form, url):\n    if url is None:\n        return urljoin(form.base_url, form.action)\n    return urljoin(form.base_url, url)\n\n\ndef _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)\n\n\ndef _multpart_encode(items, boundary, enc):\n    body = []\n\n    for name, value in items:\n        body.append(b\'--\' + boundary)\n        if isinstance(value, MultipartFile):\n            file_name = value.name\n            content = value.content\n            content_type = value.mimetype\n\n            body.append(\n                b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""; filename=""\' + to_bytes(file_name,\n                                                                                                              enc) + b\'""\')\n            body.append(b\'Content-Type: \' + to_bytes(content_type, enc))\n            body.append(b\'\')\n            body.append(to_bytes(content, enc))\n        else:\n            body.append(b\'Content-Disposition: form-data; name=""\' + to_bytes(name, enc) + b\'""\')\n            body.append(b\'\')\n            body.append(to_bytes(value, enc))\n\n    body.append(b\'--\' + boundary + b\'--\')\n    return b\'\\r\\n\'.join(body)\n\n\ndef _get_form(response, formname, formid, formnumber, formxpath):\n    """"""Find the form element """"""\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath(\'//form\')\n    if not forms:\n        raise ValueError(""No <form> element found in %s"" % response)\n\n    if formname is not None:\n        f = root.xpath(\'//form[@name=""%s""]\' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath(\'//form[@id=""%s""]\' % formid)\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == \'form\':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        encoded = formxpath if six.PY3 else formxpath.encode(\'unicode_escape\')\n        raise ValueError(\'No <form> element found with %s\' % encoded)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(""Form number %d not found in %s"" %\n                             (formnumber, response))\n        else:\n            return form\n\n\ndef _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError(\'formdata should be a dict or iterable of tuples\')\n\n    inputs = form.xpath(\'descendant::textarea\'\n                        \'|descendant::select\'\n                        \'|descendant::input[not(@type) or @type[\'\n                        \' not(re:test(., ""^(?:submit|image|reset)$"", ""i""))\'\n                        \' and (../@checked or\'\n                        \'  not(re:test(., ""^(?:checkbox|radio)$"", ""i"")))]]\',\n                        namespaces={\n                            ""re"": ""http://exslt.org/regular-expressions""})\n    values = [(k, u\'\' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values\n\n\ndef _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == \'select\':\n        return _select_value(ele, n, v)\n    return n, v\n\n\ndef _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags wihout options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath(\'.//option[@selected]\')\n        v = [(o.get(\'value\') or o.text or u\'\').strip() for o in selected_options]\n    return n, v\n\n\ndef _get_clickable(clickdata, form):\n    """"""\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    """"""\n    clickables = [\n        el for el in form.xpath(\n            \'descendant::*[(self::input or self::button)\'\n            \' and re:test(@type, ""^submit$"", ""i"")]\'\n            \'|descendant::button[not(@type)]\',\n            namespaces={""re"": ""http://exslt.org/regular-expressions""})\n    ]\n    if not clickables:\n        return\n\n    # If we don\'t have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get(\'nr\', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get(\'name\'), el.get(\'value\') or \'\')\n\n    # We didn\'t find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u\'.//*\' + \\\n            u\'\'.join(u\'[@%s=""%s""]\' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get(\'name\'), el[0].get(\'value\') or \'\')\n    elif len(el) > 1:\n        raise ValueError(""Multiple elements found (%r) matching the criteria ""\n                         ""in clickdata: %r"" % (el, clickdata))\n    else:\n        raise ValueError(\'No clickable element matching clickdata: %r\' % (clickdata,))\n\nThis is the code I used to call the request (in my case I needed to upload an image):\nwith open(img_path, \'rb\') as file:\n    img = file.read()\n    file_name = os.path.basename(img_path)\n    multipart_file = MultipartFile(file_name, img, ""image/png"")\n    form_data = {\n        ""param"": ""value"", # this is an example of a text parameter\n        ""PicUpload"": multipart_file\n    }\n    yield MultipartFormRequest(url=upload_url, formdata=form_data,\n                               callback=self.my_callback)\n\nIt\'s a shame that so much time has passed and Scrapy still doesn\'t have a built in way to do this, especially since someone wrote a very simple implementation years ago.\n']",https://stackoverflow.com/questions/39303851/scrapy-upload-file,web-crawler
Make a JavaScript-aware Crawler,"
I want to make a script that's crawling a website and it should return the locations of all the banners showed on that page.
The locations of banners are most of the time from known domains. But banners are not in the HTML as an easy image or swf-file. Most of the times a Javascript is used to show the banner.
So if a .swf-file or image-file is loaded from a banner-domain, it should return that url.
Is that possible to do? And how could I do that roughly?
Best would be if it can also returns the landing page of that ad. How to solve that?
",2k,"
            2
        ","[""\nYou could use selenium to open the pages in a real browser and then access the DOM.\nPhantomJS might also be worth a look - it's a headless version of WebKit (the engine behind Chrome, Safari, etc.).\nHowever, none of those solutions are pure php - if that's a requirement, you'll probably have to write your own JavaScript engine in PHP (which is nothing I'd ask my worst enemy to do ;))\n"", '\nIn order to get the output of the JavaScript you will need a JavaScript engine (such as Google\'s V8 Engine). The V8 engine is written in C++ but there are some resources that tell you embed the V8 engine into PHP.\nWith that said, you have to study the output ""by hand"" and determine exactly what can be scraped and how to identify it. Once you\'ve identified some common syntax for the advertisement banners, then you can write a script to extract the banner and the landing page which is referenced.\nNone of this is easy work, but if you have an example of an ad you\'d like to collect then I can give you more advice.\n']",https://stackoverflow.com/questions/8326301/make-a-javascript-aware-crawler,web-crawler
Are Robots.txt and metadata tags enough to stop search engines to index dynamic pages that are dependent of $_GET variables?,"
I created a php page that is only accessible by means of token/pass received through $_GET
Therefore if you go to the following url you'll get a generic or blank page

http://fakepage11.com/secret_page.php

However if you used the link with the token it shows you special content

http://fakepage11.com/secret_page.php?token=344ee833bde0d8fa008de206606769e4

Of course this is not as safe as a login page, but my only concern is to create a dynamic page that is not indexable and only accessed through the provided link.
Are dynamic pages that are dependent of $_GET variables indexed by google and other search engines?
If so, will include the following be enough to hide it?

Robots.txt User-agent: * Disallow: /

metadata: <META NAME=""ROBOTS"" CONTENT=""NOINDEX"">


Even if I type into google:

site:fakepage11.com/

Thank you!
",936,"
            2
        ","['\nIf a search engine bot finds the link with the token somehow¹, it may crawl and index it.\nIf you use robots.txt to disallow crawling the page, conforming search engine bots won’t crawl the page, but they may still index its URL (which then might appear in a site: search).\nIf you use meta-robots to disallow indexing the page, conforming search engine bots won’t index the page, but they may still crawl it.\nYou can’t have both: If you disallow crawling, conforming bots can never learn that you also disallow indexing, because they are not allowed to visit the page to see your meta-robots element. \n¹ There are countless ways how search engines might find a link. For example, a user that visits the page might use a browser toolbar that automatically sends all visited URLs to a search engine.\n', '\nIf your page isn\'t discoverable then it will not be indexed.\nby ""discoverable"" we mean:\n\nit is a standard web page, i.e. index.*\nit is referenced by another link either yours or from another site\n\nSo in your case by using the get parameter for access, you achieve 1 but not necessarily 2 since someone may reference that link and hence the ""hidden"" page.\nYou can use the robots.txt that you gave and in that case the page will not get indexed by a bot that respects that (not all will do). Not indexing your page doesn\'t mean of course that the ""hidden"" page URL will not be in the wild.\nFurthermore another issue - depending on your requirements - is that you use unencrypted HTTP, that means that your ""hidden"" URLs and content of pages are visible to every server between your server and the user.\nApart from search engines take care that certain services are caching/resolving content when URLs are exchanged for example in Skype or Facebook messenger. In that cases they will visit the URL and try to extract metadata and  maybe cache it if applicable. Of course this scenario does not expose your URL to the public but it is exposed to the systems of those services and with them the content that you have ""hidden"".\nUPDATE:\nAnother issue to consider is the exposing of a ""hidden"" page by linking to another page. In that case in the logs of the server that hosts the linked URL your page will be seen as a referral and thus be visible, that expands also to Google Analytics etc. Thus if you want to remain stealth do not link to another pages from the hidden page.\n']",https://stackoverflow.com/questions/35504252/are-robots-txt-and-metadata-tags-enough-to-stop-search-engines-to-index-dynamic,web-crawler
redirect all bots using htaccess apache,"
What .htaccess rewriterule should i use to detect known bots, for example the big ones:
altavista, google, bing, yahoo
I know i can check for their ips, or hosts, but is there a better way?
",3k,"
            1
        ",['\nRewriteCond %{HTTP_USER_AGENT} AltaVista [OR]\nRewriteCond %{HTTP_USER_AGENT} Googlebot [OR]\nRewriteCond %{HTTP_USER_AGENT} msnbot [OR]\nRewriteCond %{HTTP_USER_AGENT} Slurp\nRewriteRule ^.*$ IHateBots.html [L]\n\n'],https://stackoverflow.com/questions/2691956/redirect-all-bots-using-htaccess-apache,web-crawler
Puppeteer not giving accurate HTML code for page with shadow roots,"
I am trying to download the HTML code for the website intersight.com/help/. But puppeteer is not returning the HTML code with hrefs as we can see in the page (example https://intersight.com/help/getting_started is not present in the downloaded HTML). On inspecting the HTML in browser I came to know that all the missing HTML is present inside the <an-hulk></an-hulk> tags. I don't know what these tags mean.
const puppeteer = require('puppeteer');
const fs = require('fs');
(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  const data = await page.goto('https://intersight.com/help/', { waitUntil: 'domcontentloaded' });
  // Tried all the below lines, neither worked
  // await page.waitForSelector('.helplet-links')
  // document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"").shadowRoot.querySelector(""div > div > div:nth-child(1) > div:nth-child(1) > div.helplet-links > ul > li:nth-child(1) > a > span"")
  // await page.evaluateHandle(`document.querySelector(""#app > an-hulk"").shadowRoot.querySelector(""#content"").shadowRoot.querySelector(""#main > div > div > div > an-hulk-home"")`);
  await page.evaluateHandle(`document.querySelector(""an-hulk"").shadowRoot.querySelector(""#aside"").shadowRoot.querySelectorAll("".item"")`)
  const result = await page.content()
  fs.writeFile('./intersight.html', result, (err) => {
    if (err) console.log(err)
    else console.log('done!!')
  })
  // console.log(result)
  await browser.close();
})();

",938,"
            1
        ","['\nAs mentioned in the comments, you\'re dealing with a page that uses shadow roots. Traditional selectors that attempt to pierce shadow roots won\'t work through the console or Puppeteer without help. Short of using a library, the idea is to identify any shadow root elements by their .shadowRoot property, then dive into them recursively and repeat the process until you get the data you\'re after.\nThis code should grab all of the hrefs on the page (I didn\'t do a manual count) following this strategy:\nconst puppeteer = require(""puppeteer"");\n\nlet browser;\n(async () => {\n  browser = await puppeteer.launch({headless: true});\n  const [page] = await browser.pages();\n  const url = ""https://intersight.com/help/"";\n  const data = await page.goto(url, {\n    waitUntil: ""networkidle0""\n  });\n  await page.waitForSelector(""an-hulk"", {visible: true});\n  const hrefs = await page.evaluate(() => {\n    const walk = root => [\n      ...[...root.querySelectorAll(""a[href]"")]\n        .map(e => e.getAttribute(""href"")),\n      ...[...root.querySelectorAll(""*"")]\n        .filter(e => e.shadowRoot)\n        .flatMap(e => walk(e.shadowRoot))\n    ];\n    return walk(document);\n  });\n  console.log(hrefs);\n  console.log(hrefs.length); // => 44 at the time I ran this\n\n  // Bonus example of diving manually into shadow roots...\n  //const html = await page.evaluate(() =>\n  //  document\n  //    .querySelector(""#app > an-hulk"")\n  //    .shadowRoot\n  //    .querySelector(""#content"")\n  //    .shadowRoot\n  //    .querySelector(""#main an-hulk-home"")\n  //    .shadowRoot\n  //    .querySelector("".content"")\n  //    .innerHTML\n  //);\n  //console.log(html);\n})()\n  .catch(err => console.error(err))\n  .finally(() => browser?.close());\n;\n\nNote that the sidebar and other parts of the page use event listeners on spans and divs to implement links, so these don\'t count as hrefs as far as the above code is concerned. If you want to access these URLs, there are a variety of strategies you can try, including clicking them and extracting the URL after navigation. This is speculative since it\'s not clear that you want to do this.\n\nA few remarks about your code:\n\nPuppeteer wait until page is completely loaded is an important resource. { waitUntil: \'domcontentloaded\' } is a weaker condition than { waitUntil: \'networkidle0\' }. Using page.waitForSelector(selector, {visible: true}) and page.waitForFunction(predicate) are important to use to ensure the elements have been rendered before you begin manipulating them. Even without the shadow root, it\'s not clear to me that the top-level ""an-hulk"" is going to be available by the time you run evaluate.\nAdd console listeners to your page to help debug. Try your queries one step at a time and break them into multiple stages to see where they go wrong.\nfs.writeFile should be await fs.promises.writeFile since you\'re in an async function.\n\n\nAdditional resources and similar threads:\n\nWhat is shadow root\nPuppeteer: Query nodes within shadow roots #858\nHow to get text from shadow root element?\nSelect element within shadow root\npuppeteer: clicking button in shadowroot\nManipulate / set style shadowRoot using Puppeteer\nPuppeteer: get full HTML content of a webpage, like innerHTML, but including any shadow roots?\nPopup form visible, but html code missing in Puppeteer\nCan\'t locate and click on a terms of conditions button\n\n']",https://stackoverflow.com/questions/68525115/puppeteer-not-giving-accurate-html-code-for-page-with-shadow-roots,web-crawler
Difference between BeautifulSoup and Scrapy crawler?,"
I want to make a website that shows the comparison between amazon and e-bay product price.
Which of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler.
",89k,"
            159
        ","['\nScrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling.\nWhile\nBeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria.\nIn simple words, with Beautiful Soup you can build something similar to Scrapy.\nBeautiful Soup is a library while Scrapy is a complete framework.\nSource\n', ""\nI think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page.\nAfter that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags.\nIf you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop.\nTake a look at the scrapy documentation, it's very simple to use.\n"", ""\nScrapy\nIt is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below.\n\nFeed exports: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. \nAsynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request).\nSelectors: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup.\nSetting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically.\nItem Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server.\nCookies: scrapy automatically handles cookies for us.\n\netc.\n\nTLDR: scrapy is a framework that provides everything that one might\n  need to build large scale crawls. It provides various features that\n  hide complexity of crawling the webs. one can simply start writing web\n  crawlers without worrying about the setup burden.\n\nBeautiful soup\nBeautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing.\nTo sum up.\n\nScrapy is a rich framework that you can use to start writing crawlers\nwithout any hassale.\nBeautiful soup is a library that you can use to parse a webpage. It\ncannot be used alone to scrape web.\n\nYou should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.\n"", '\nBoth are using to parse data.\nScrapy:\n\nScrapy is a fast high-level web crawling and web scraping framework,\nused to crawl websites and extract structured data from their pages.\nBut it has some limitations when data comes from java script or\nloading dynamicaly, we can over come it by using packages like splash, \nselenium etc.\n\nBeautifulSoup:\n\nBeautiful Soup is a Python library for pulling data out of HTML and\nXML files.\nwe can use this package for getting data from java script or \ndynamically loading pages.\n\nScrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents \n', ""\nThe way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup.\nThe APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.\n"", '\nBeautifulSoup is a library that lets you extract information from a web page.\nScrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data.\nYou can check this blog to get started with Scrapy\nhttps://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/\n', ""\nUsing scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method.\nBig project takes both advantages.\n"", '\nBeautifulsoup is web scraping small library. it does your job but sometime it does not satisfy your needs.i mean if you scrape  websites in large amount of data  so here in this case beautifulsoup fails.\nIn this case  you should use Scrapy which is  a complete  scraping framework  which will do you job.\nAlso scrapy has support for databases(all kind of databases) so it is a huge\nof scrapy over other web  scraping libraries.\n', '\nThe differences are many and selection of any tool/technology depends on individual needs.\nFew major differences are:\n\nBeautifulSoup is comparatively is easy to learn than Scrapy. \nThe extensions, support, community is larger for Scrapy than for BeautifulSoup.\nScrapy should be considered as a Spider while BeautifulSoup is a Parser.\n\n']",https://stackoverflow.com/questions/19687421/difference-between-beautifulsoup-and-scrapy-crawler,web-crawler
Get a list of URLs from a site [closed],"






Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.
                        
                    










 We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



I'm deploying a replacement site for a client but they don't want all their old pages to end in 404s. Keeping the old URL structure wasn't possible because it was hideous.
So I'm writing a 404 handler that should look for an old page being requested and do a permanent redirect to the new page. Problem is, I need a list of all the old page URLs.
I could do this manually, but I'd be interested if there are any apps that would provide me a list of relative (eg: /page/path, not http:/.../page/path) URLs just given the home page. Like a spider but one that doesn't care about the content other than to find deeper pages.
",496k,"
            117
        ","[""\nI didn't mean to answer my own question but I just thought about running a sitemap generator. First one I found http://www.xml-sitemaps.com has a nice text output. Perfect for my needs.\n"", ""\ndo wget -r -l0 www.oldsite.com\nThen just find www.oldsite.com would reveal all urls, I believe.\nAlternatively, just serve that custom not-found page on every 404 request!\nI.e. if someone used the wrong link, he would get the page telling that page wasn't found, and making some hints about site's content.\n"", '\nHere is a list of sitemap generators (from which obviously you can get the list of URLs from a site): http://code.google.com/p/sitemap-generators/wiki/SitemapGenerators\n\nWeb Sitemap Generators\nThe following are links to tools that generate or maintain files in\n  the XML Sitemaps format, an open standard defined on sitemaps.org and\n  supported by the search engines such as Ask, Google, Microsoft Live\n  Search and Yahoo!. Sitemap files generally contain a collection of\n  URLs on a website along with some meta-data for these URLs. The\n  following tools generally generate ""web-type"" XML Sitemap and URL-list\n  files (some may also support other formats).\nPlease Note: Google has not tested or verified the features or\n  security of the third party software listed on this site. Please\n  direct any questions regarding the software to the software\'s author.\n  We hope you enjoy these tools!\nServer-side Programs\n\nEnarion phpSitemapsNG (PHP)\nGoogle Sitemap Generator (Linux/Windows, 32/64bit, open-source)\nOutil en PHP (French, PHP)\nPerl Sitemap Generator (Perl)\nPython Sitemap Generator (Python)\nSimple Sitemaps (PHP)\nSiteMap XML Dynamic Sitemap Generator (PHP) $\nSitemap generator for OS/2 (REXX-script)\nXML Sitemap Generator (PHP) $\n\nCMS and Other Plugins:\n\nASP.NET - Sitemaps.Net\nDotClear (Spanish)\nDotClear (2)\nDrupal\nECommerce Templates (PHP) $\nEcommerce Templates (PHP or ASP) $\nLifeType\nMediaWiki Sitemap generator\nmnoGoSearch\nOS Commerce\nphpWebSite\nPlone\nRapidWeaver\nTextpattern\nvBulletin\nWikka Wiki (PHP)\nWordPress\n\nDownloadable Tools\n\nGSiteCrawler (Windows)\nGWebCrawler & Sitemap Creator (Windows)\nG-Mapper (Windows)\nInspyder Sitemap Creator (Windows) $\nIntelliMapper (Windows) $\nMicrosys A1 Sitemap Generator (Windows) $\nRage Google Sitemap Automator $ (OS-X)\nScreaming Frog SEO Spider and Sitemap generator (Windows/Mac) $\nSite Map Pro (Windows) $\nSitemap Writer (Windows) $\nSitemap Generator by DevIntelligence (Windows)\nSorrowmans Sitemap Tools (Windows)\nTheSiteMapper (Windows) $\nVigos Gsitemap (Windows)\nVisual SEO Studio (Windows)\nWebDesignPros Sitemap Generator (Java Webstart Application)\nWeblight (Windows/Mac) $\nWonderWebWare Sitemap Generator (Windows)\n\nOnline Generators/Services\n\nAuditMyPc.com Sitemap Generator\nAutoMapIt\nAutositemap $\nEnarion phpSitemapsNG\nFree Sitemap Generator\nNeuroticweb.com Sitemap Generator\nROR Sitemap Generator\nScriptSocket Sitemap Generator\nSeoUtility Sitemap Generator (Italian)\nSitemapDoc\nSitemapspal\nSitemapSubmit\nSmart-IT-Consulting Google Sitemaps XML Validator\nXML Sitemap Generator\nXML-Sitemaps Generator\n\nCMS with integrated Sitemap generators\n\nConcrete5\n\nGoogle News Sitemap Generators   The following plugins allow\n  publishers to update Google News Sitemap files, a variant of the\n  sitemaps.org protocol that we describe in our Help Center. In addition\n  to the normal properties of Sitemap files, Google News Sitemaps allow\n  publishers to describe the types of content they publish, along with\n  specifying levels of access for individual articles. More information\n  about Google News can be found in our Help Center and Help Forums.\n\nWordPress Google News plugin\n\nCode Snippets / Libraries\n\nASP script\nEmacs Lisp script\nJava library\nPerl script\nPHP class\nPHP generator script\n\nIf you believe that a tool should be added or removed for a legitimate\n  reason, please leave a comment in the Webmaster Help Forum.\n\n', '\nThe best on I have found is http://www.auditmypc.com/xml-sitemap.asp which uses Java, and has no limit on pages, and even lets you export results as a raw URL list.\nIt also uses sessions, so if you are using a CMS, make sure you are logged out before you run the crawl.\n', '\nSo, in an ideal world you\'d have a spec for all pages in your site. You would also have a test infrastructure that could hit all your pages to test them.\nYou\'re presumably not in an ideal world. Why not do this...?\n\nCreate a mapping between the well\nknown old URLs and the new ones.\nRedirect when you see an old URL.\nI\'d possibly consider presenting a\n""this page has moved, it\'s new url\nis XXX, you\'ll be redirected\nshortly"".\nIf you have no mapping, present a\n    ""sorry - this page has moved. Here\'s\n    a link to the home page"" message and\n    redirect them if you like.\nLog all redirects - especially the\n    ones with no mapping. Over time, add\n    mappings for pages that are\n    important.\n\n', ""\nwget from a linux box might also be a good option as there are switches to spider and change it's output.\nEDIT: wget is also available on Windows: http://gnuwin32.sourceforge.net/packages/wget.htm\n"", '\nWrite a spider which reads in every html from disk and outputs every ""href"" attribute of an ""a"" element (can be done with a parser). Keep in mind which links belong to a certain page (this is common task for a MultiMap datastructre). After this you can produce a mapping file which acts as the input for the 404 handler.\n', '\nI would look into any number of online sitemap generation tools.  Personally, I\'ve used this one (java based)in the past, but if you do a google search for ""sitemap builder"" I\'m sure you\'ll find lots of different options.\n']",https://stackoverflow.com/questions/857653/get-a-list-of-urls-from-a-site,web-crawler
crawler vs scraper,"
Can somebody distinguish between a crawler and scraper in terms of scope and functionality.
",40k,"
            78
        ","[""\nA crawler gets web pages -- i.e., given a starting address (or set of starting addresses) and some conditions (e.g., how many links deep to go, types of files to ignore) it downloads whatever is linked to from the starting point(s).\nA scraper takes pages that have been downloaded or, in a more general sense, data that's formatted for display, and (attempts to) extract data from those pages, so that it can (for example) be stored in a database and manipulated as desired.\nDepending on how you use the result, scraping may well violate the rights of the owner of the information and/or user agreements about use of web sites (crawling violates the latter in some cases as well). Many sites include a file named robots.txt in their root (i.e. having the URL http://server/robots.txt) to specify how (and if) crawlers should treat that site -- in particular, it can list (partial) URLs that a crawler should not attempt to visit. These can be specified separately per crawler (user-agent) if desired.\n"", ""\nCrawlers surf the web, following links.  An example would be the Google robot that gets pages to index.  Scrapers extract values from forms, but don't necessarily have anything to do with the web.\n"", '\nWeb crawler gets links (Urls - Pages) in a logic and scraper get values (extracting) from HTML.\nThere are so many web crawler tools. Visit page to see some. Any XML - HTML parser can used to extract (scrape) data from crawled pages. (I recommend Jsoup for parsing and extracting data)\n', ""\nGenerally, crawlers would follow the links to reach numerous pages while scrapers is, in some sense, just pulling the contents displayed online and would not reach the deeper links. \nThe most typical crawler is google bots, which would follow the links to reach all the web pages on your website and would index the contents if they found it useful(that's why you need robots.txt to tell which contents you do not want to be indexed). So we could search such kind of contents on its website. While the purpose of scrapers is just to pull the contents for personal uses and would not have much effects on others. \nHowever, there's no distinct difference about crawlers and scrapers now as some automated web scraping tools also allow you to crawl the website by following the links, like Octoparse and import.io. They are not the crawlers like google bots, but they are able to automatically crawl the websites to get numerous data without coding.\n"", '\nScrapers and crawlers do not always distinguish, I mean - you can find crawlers which scrape, in fact, Scraper Crawler is doing both and is named accordingly:\n\nit crawls to a URL i.e. indexes all the URL in that main URL\ndepth of crawling is how far the indexing goes in the URL tree\nthen it scrapes whatever you define in a regexp\n\n', ""\nI know this question is quite old, but I'll respond anyway for the newcomer that will wonder here.\nFrom what I can gather and understand it seems that those two terms are often confused with each other due to their similarity and people will often refer to them as the same thing.\nHowever, they are not quite the same. A crawler(or spider) will follow each link in the page it crawls from the starter page. This is why it is also referred to as a spider bot since it will create a kind of a spider web of pages.\nA scraper will extract the data from a page, usually from the pages downloaded with the crawler.\nIf you are interested in either of those, you can try the Norconex HTTP Collector.\n""]",https://stackoverflow.com/questions/3207418/crawler-vs-scraper,web-crawler
Detect Search Crawlers via JavaScript,"
I am wondering how would I go abouts in detecting search crawlers? The reason I ask is because I want to suppress certain JavaScript calls if the user agent is a bot.
I have found an example of how to to detect a certain browser, but am unable to find examples of how to detect a search crawler: 
/MSIE (\d+\.\d+);/.test(navigator.userAgent); //test for MSIE x.x
Example of search crawlers I want to block:
Google 
Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) 
Googlebot/2.1 (+http://www.googlebot.com/bot.html) 
Googlebot/2.1 (+http://www.google.com/bot.html) 

Baidu 
Baiduspider+(+http://www.baidu.com/search/spider_jp.html) 
Baiduspider+(+http://www.baidu.com/search/spider.htm) 
BaiDuSpider 

",46k,"
            62
        ","['\nThis is the regex the ruby UA agent_orange library uses to test if a userAgent looks to be a bot. You can narrow it down for specific bots by referencing the bot userAgent list here:\n/bot|crawler|spider|crawling/i\n\nFor example you have some object, util.browser, you can store what type of device a user is on:\nutil.browser = {\n   bot: /bot|googlebot|crawler|spider|robot|crawling/i.test(navigator.userAgent),\n   mobile: ...,\n   desktop: ...\n}\n\n', '\nTry this. It\'s based on the crawlers list on available on https://github.com/monperrus/crawler-user-agents\nvar botPattern = ""(googlebot\\/|bot|Googlebot-Mobile|Googlebot-Image|Google favicon|Mediapartners-Google|bingbot|slurp|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|phpcrawl|msnbot|jyxobot|FAST-WebCrawler|FAST Enterprise Crawler|biglotron|teoma|convera|seekbot|gigablast|exabot|ngbot|ia_archiver|GingerCrawler|webmon |httrack|webcrawler|grub.org|UsineNouvelleCrawler|antibot|netresearchserver|speedy|fluffy|bibnum.bnf|findlink|msrbot|panscient|yacybot|AISearchBot|IOI|ips-agent|tagoobot|MJ12bot|dotbot|woriobot|yanga|buzzbot|mlbot|yandexbot|purebot|Linguee Bot|Voyager|CyberPatrol|voilabot|baiduspider|citeseerxbot|spbot|twengabot|postrank|turnitinbot|scribdbot|page2rss|sitebot|linkdex|Adidxbot|blekkobot|ezooms|dotbot|Mail.RU_Bot|discobot|heritrix|findthatfile|europarchive.org|NerdByNature.Bot|sistrix crawler|ahrefsbot|Aboundex|domaincrawler|wbsearchbot|summify|ccbot|edisterbot|seznambot|ec2linkfinder|gslfbot|aihitbot|intelium_bot|facebookexternalhit|yeti|RetrevoPageAnalyzer|lb-spider|sogou|lssbot|careerbot|wotbox|wocbot|ichiro|DuckDuckBot|lssrocketcrawler|drupact|webcompanycrawler|acoonbot|openindexspider|gnam gnam spider|web-archive-net.com.bot|backlinkcrawler|coccoc|integromedb|content crawler spider|toplistbot|seokicks-robot|it2media-domain-crawler|ip-web-crawler.com|siteexplorer.info|elisabot|proximic|changedetection|blexbot|arabot|WeSEE:Search|niki-bot|CrystalSemanticsBot|rogerbot|360Spider|psbot|InterfaxScanBot|Lipperhey SEO Service|CC Metadata Scaper|g00g1e.net|GrapeshotCrawler|urlappendbot|brainobot|fr-crawler|binlar|SimpleCrawler|Livelapbot|Twitterbot|cXensebot|smtbot|bnf.fr_bot|A6-Indexer|ADmantX|Facebot|Twitterbot|OrangeBot|memorybot|AdvBot|MegaIndex|SemanticScholarBot|ltx71|nerdybot|xovibot|BUbiNG|Qwantify|archive.org_bot|Applebot|TweetmemeBot|crawler4j|findxbot|SemrushBot|yoozBot|lipperhey|y!j-asr|Domain Re-Animator Bot|AddThis)"";\nvar re = new RegExp(botPattern, \'i\');\nvar userAgent = navigator.userAgent; \nif (re.test(userAgent)) {\n    console.log(\'the user agent is a crawler!\');\n}\n\n', ""\nThe following regex will match the biggest search engines according to this post.\n/bot|google|baidu|bing|msn|teoma|slurp|yandex/i\n    .test(navigator.userAgent)\n\nThe matches search engines are:\n\nBaidu\nBingbot/MSN\nDuckDuckGo (duckduckbot)\nGoogle\nTeoma\nYahoo!\nYandex\n\nAdditionally, I've added bot as a catchall for smaller crawlers/bots.\n"", '\nThis might help to detect the robots user agents while also keeping things more organized:\nJavascript\nconst detectRobot = (userAgent) => {\n  const robots = new RegExp([\n    /bot/,/spider/,/crawl/,                            // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,        // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,       // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                         // OTHER\n  ].map((r) => r.source).join(""|""),""i"");               // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\nTypescript\nconst detectRobot = (userAgent: string): boolean => {\n  const robots = new RegExp(([\n    /bot/,/spider/,/crawl/,                               // GENERAL TERMS\n    /APIs-Google/,/AdsBot/,/Googlebot/,                   // GOOGLE ROBOTS\n    /mediapartners/,/Google Favicon/,\n    /FeedFetcher/,/Google-Read-Aloud/,\n    /DuplexWeb-Google/,/googleweblight/,\n    /bing/,/yandex/,/baidu/,/duckduck/,/yahoo/,           // OTHER ENGINES\n    /ecosia/,/ia_archiver/,\n    /facebook/,/instagram/,/pinterest/,/reddit/,          // SOCIAL MEDIA\n    /slack/,/twitter/,/whatsapp/,/youtube/,\n    /semrush/,                                            // OTHER\n  ] as RegExp[]).map((r) => r.source).join(""|""),""i"");     // BUILD REGEXP + ""i"" FLAG\n\n  return robots.test(userAgent);\n};\n\n\nUse on server:\nconst userAgent = req.get(\'user-agent\');\nconst isRobot = detectRobot(userAgent);\n\nUse on ""client"" / some phantom browser a bot might be using:\nconst userAgent = navigator.userAgent;\nconst isRobot = detectRobot(userAgent);\n\nOverview of Google crawlers:\nhttps://developers.google.com/search/docs/advanced/crawling/overview-google-crawlers\n', ""\nisTrusted property could help you.\n\nThe isTrusted read-only property of the Event interface is a Boolean\nthat is true when the event was generated by a user action, and false\nwhen the event was created or modified by a script or dispatched via\nEventTarget.dispatchEvent().\n\neg:\nisCrawler() {\n  return event.isTrusted;\n}\n\n⚠ Note that IE isn't compatible.\nRead more from doc: https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted\n"", '\nPeople might light to check out the new navigator.webdriver property, which allows bots to inform you that they are bots:\nhttps://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver\n\nThe webdriver read-only property of the navigator interface indicates whether the user agent is controlled by automation.\n\n\nIt defines a standard way for co-operating user agents to inform the document that it is controlled by WebDriver, for example, so that alternate code paths can be triggered during automation.\n\nIt is supported by all major browsers and respected by major browser automation software like Puppeteer. Users of automation software can of course disable it, and so it should only be used to detect ""good"" bots.\n', '\nI combined some of the above and removed some redundancy. I use this in .htaccess on a semi-private site:\n(google|bot|crawl|spider|slurp|baidu|bing|msn|teoma|yandex|java|wget|curl|Commons-HttpClient|Python-urllib|libwww|httpunit|nutch|biglotron|convera|gigablast|archive|webmon|httrack|grub|netresearchserver|speedy|fluffy|bibnum|findlink|panscient|IOI|ips-agent|yanga|Voyager|CyberPatrol|postrank|page2rss|linkdex|ezooms|heritrix|findthatfile|Aboundex|summify|ec2linkfinder|facebook|slack|instagram|pinterest|reddit|twitter|whatsapp|yeti|RetrevoPageAnalyzer|sogou|wotbox|ichiro|drupact|coccoc|integromedb|siteexplorer|proximic|changedetection|WeSEE|scrape|scaper|g00g1e|binlar|indexer|MegaIndex|ltx71|BUbiNG|Qwantify|lipperhey|y!j-asr|AddThis)\n', '\nThe ""test for MSIE x.x"" example is just code for testing the userAgent against a Regular Expression. In your example the Regexp is the\n/MSIE (\\d+\\.\\d+);/\n\npart. Just replace it with your own Regexp you want to test the user agent against. It would be something like\n/Google|Baidu|Baiduspider/.test(navigator.userAgent)\n\nwhere the vertical bar is the ""or"" operator to match the user agent against all of your mentioned robots. For more information about Regular Expression you can refer to this site since javascript uses perl-style RegExp.\n', ""\nI found this isbot package that has the built-in isbot() function. It seams to me that the package is properly maintained and that they keep everything up-to-date.\nUSAGE:\nconst isBot = require('isbot');\n\n...\n\nisBot(req.get('user-agent'));\n\nPackage: https://www.npmjs.com/package/isbot\n""]",https://stackoverflow.com/questions/20084513/detect-search-crawlers-via-javascript,web-crawler
Node.JS: How to pass variables to asynchronous callbacks? [duplicate],"






This question already has answers here:
                        
                    



JavaScript closure inside loops – simple practical example

                                (44 answers)
                            

Closed 6 years ago.



I'm sure my problem is based on a lack of understanding of asynch programming in node.js but here goes.
For example: I have a list of links I want to crawl. When each asynch request returns I want to know which URL it is for. But, presumably because of race conditions, each request returns with the URL set to the last value in the list.
var links = ['http://google.com', 'http://yahoo.com'];
for (link in links) {
    var url = links[link];
    require('request')(url, function() {
        console.log(url);
    });
}

Expected output:
http://google.com
http://yahoo.com

Actual output:
http://yahoo.com
http://yahoo.com

So my question is either:

How do I pass url (by value) to the call back function? OR
What is the proper way of chaining the HTTP requests so they run sequentially? OR
Something else I'm missing?

PS: For 1. I don't want a solution which examines the callback's parameters but a general way of a callback knowing about variables 'from above'.
",49k,"
            46
        ","[""\nYour url variable is not scoped to the for loop as JavaScript only supports global and function scoping.  So you need to create a function scope for your request call to capture the url value in each iteration of the loop by using an immediate function:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        require('request')(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\nBTW, embedding a require in the middle of loop isn't good practice.  It should probably be re-written as:\nvar request = require('request');\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\n    (function(url) {\n        request(url, function() {\n            console.log(url);\n        });\n    })(links[link]);\n}\n\n"", ""\nCheck this blog out. A variable can be passed by using .bind() method. In your case it would be like this:\nvar links = ['http://google.com', 'http://yahoo.com'];\nfor (link in links) {\nvar url = links[link];\n\nrequire('request')(url, function() {\n\n    console.log(this.urlAsy);\n\n}.bind({urlAsy:url}));\n}\n\n"", ""\nSee https://stackoverflow.com/a/11747331/243639 for a general discussion of this issue.\nI'd suggest something like\nvar links = ['http://google.com', 'http://yahoo.com'];\n\nfunction createCallback(_url) {\n    return function() {\n        console.log(_url);\n    }\n};\n\nfor (link in links) {\n    var url = links[link];\n    require('request')(url, createCallback(url));\n}\n\n""]",https://stackoverflow.com/questions/13221769/node-js-how-to-pass-variables-to-asynchronous-callbacks,web-crawler
How do I lock read/write to MySQL tables so that I can select and then insert without other programs reading/writing to the database?,"
I am running many instances of a webcrawler in parallel.
Each crawler selects a domain from a table, inserts that url and a start time into a log table, and then starts crawling the domain.
Other parallel crawlers check the log table to see what domains are already being crawled before selecting their own domain to crawl.
I need to prevent other crawlers from selecting a domain that has just been selected by another crawler but doesn't have a log entry yet.  My best guess at how to do this is to lock the database from all other read/writes while one crawler selects a domain and inserts a row in the log table (two queries).
How the heck does one do this?  I'm afraid this is terribly complex and relies on many other things.  Please help get me started.

This code seems like a good solution (see the error below, however):
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT companies.id FROM companies
        LEFT OUTER JOIN crawlLog
        ON companies.id = crawlLog.companyId
        WHERE crawlLog.companyId IS NULL
        LIMIT 1
    ),
    now()
)

but I keep getting the following mysql error:
You can't specify target table 'crawlLog' for update in FROM clause

Is there a way to accomplish the same thing without this problem?  I've tried a couple different ways.  Including this:
INSERT INTO crawlLog (companyId, timeStartCrawling)
VALUES
(
    (
        SELECT id
        FROM companies
        WHERE id NOT IN (SELECT companyId FROM crawlLog) LIMIT 1
    ),
    now()
)

",104k,"
            38
        ","['\nYou can lock tables using the MySQL LOCK TABLES command like this:\nLOCK TABLES tablename WRITE;\n\n# Do other queries here\n\nUNLOCK TABLES;\n\nSee:\nhttp://dev.mysql.com/doc/refman/5.5/en/lock-tables.html\n', '\nWell, table locks are one way to deal with that; but this makes parallel requests impossible. If the table is InnoDB you could force a row lock instead, using SELECT ... FOR UPDATE within a transaction. \nBEGIN;\n\nSELECT ... FROM your_table WHERE domainname = ... FOR UPDATE\n\n# do whatever you have to do\n\nCOMMIT;\n\nPlease note that you will need an index on domainname (or whatever column you use in the WHERE-clause) for this to work, but this makes sense in general and I assume you will have that anyway.\n', '\nYou probably don\'t want to lock the table.  If you do that you\'ll have to worry about trapping errors when the other crawlers try to write to the database - which is what you were thinking when you said ""...terribly complex and relies on many other things.""\nInstead you should probably wrap the group of queries in a MySQL transaction (see http://dev.mysql.com/doc/refman/5.0/en/commit.html) like this:\nSTART TRANSACTION;\nSELECT @URL:=url FROM tablewiththeurls WHERE uncrawled=1 ORDER BY somecriterion LIMIT 1;\nINSERT INTO loggingtable SET url=@URL;\nCOMMIT;\n\nOr something close to that.\n[edit]  I just realized - you could probably do everything you need in a single query and not even have to worry about transactions.  Something like this:\nINSERT INTO loggingtable (url) SELECT url FROM tablewithurls u LEFT JOIN loggingtable l ON l.url=t.url WHERE {some criterion used to pick the url to work on} AND l.url IS NULL.\n\n', ""\nI got some inspiration from @Eljakim's answer and started this new thread where I figured out a great trick.  It doesn't involve locking anything and is very simple.\nINSERT INTO crawlLog (companyId, timeStartCrawling)\nSELECT id, now()\nFROM companies\nWHERE id NOT IN\n(\n    SELECT companyId\n    FROM crawlLog AS crawlLogAlias\n)\nLIMIT 1\n\n"", ""\nI wouldn't use locking, or transactions.\nThe easiest way to go is to INSERT a record in the logging table if it's not yet present, and then check for that record.\nAssume you have tblcrawels (cra_id) that is filled with your crawlers and tblurl (url_id) that is filled with the URLs, and a table tbllogging (log_cra_id, log_url_id) for your logfile.\nYou would run the following query if crawler 1 wants to start crawling url 2:\nINSERT INTO tbllogging (log_cra_id, log_url_id) \nSELECT 1, url_id FROM tblurl LEFT JOIN tbllogging on url_id=log_url \nWHERE url_id=2 AND log_url_id IS NULL;\n\nThe next step is to check whether this record has been inserted.\nSELECT * FROM tbllogging WHERE log_url_id=2 AND log_cra_id=1\n\nIf you get any results then crawler 1 can crawl this url. If you don't get any results this means that another crawler has inserted in the same line and is already crawling.\n"", ""\nIt's better to use row lock or transactional based query so that other parallel request context can access the table.\n""]",https://stackoverflow.com/questions/6621303/how-do-i-lock-read-write-to-mysql-tables-so-that-i-can-select-and-then-insert-wi,web-crawler
how to totally ignore 'debugger' statement in chrome?,"
'never pause here' can not work

after I continue：

still paused
",29k,"
            38
        ","['\nTo totally ignore all breakpoints in Chrome, you must do as follows:\n\nOpen your page in the Chrome browser.\n\nPress F12 or right-click on the page and select Inspect.\n\nIn the Source panel, press Ctrl+F8 to deactivate all breakpoints. (or: At the top-right corner, select deactivate breakpoints.)\n\n\nAll breakpoints and debugger statements will be deactivated.\nI tested it in Chrome 79.0.3945.88 (64-bit) and I found that the debugger statement is ignored.\n\n', '\nTo stop hitting debugger statements, you must either set a ""never pause here"" breakpoint, OR you must pause stopping on exceptions.\nThis works because debugger breakpoints are considered exceptions by the browser.\n\n']",https://stackoverflow.com/questions/45767855/how-to-totally-ignore-debugger-statement-in-chrome,web-crawler
Passing arguments to process.crawl in Scrapy python,"
I would like to get the same result as this command line :
scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json
My script is as follows :
import scrapy
from linkedin_anonymous_spider import LinkedInAnonymousSpider
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

spider = LinkedInAnonymousSpider(None, ""James"", ""Bond"")
process = CrawlerProcess(get_project_settings())
process.crawl(spider) ## <-------------- (1)
process.start()

I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()?
linkedin_anonymous :
from logging import INFO

import scrapy

class LinkedInAnonymousSpider(scrapy.Spider):
    name = ""linkedin_anonymous""
    allowed_domains = [""linkedin.com""]
    start_urls = []

    base_url = ""https://www.linkedin.com/pub/dir/?first=%s&last=%s&search=Search""

    def __init__(self, input = None, first= None, last=None):
        self.input = input  # source file name
        self.first = first
        self.last = last

    def start_requests(self):
        print self.first ## <------------- (2)
        if self.first and self.last: # taking input from command line parameters
                url = self.base_url % (self.first, self.last)
                yield self.make_requests_from_url(url)

    def parse(self, response): . . .

",14k,"
            35
        ","[""\npass the spider arguments on the process.crawl method:\nprocess.crawl(spider, input='inputargument', first='James', last='Bond')\n\n"", '\nYou can do it the easy way:\nfrom scrapy import cmdline\n\ncmdline.execute(""scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json"".split())\n\n', ""\nif you have Scrapyd and you want to schedule the spider, do this\ncurl http://localhost:6800/schedule.json -d project=projectname -d spider=spidername -d first='James' -d last='Bond'\n""]",https://stackoverflow.com/questions/34382356/passing-arguments-to-process-crawl-in-scrapy-python,web-crawler
How do I use the Python Scrapy module to list all the URLs from my website?,"
I want to use the Python Scrapy module to scrape all the URLs from my website and write the list to a file. I looked in the examples but didn't see any simple example to do this.
",22k,"
            24
        ","[""\nHere's the python program that worked for me:\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\n\nDOMAIN = 'example.com'\nURL = 'http://%s' % DOMAIN\n\nclass MySpider(BaseSpider):\n    name = DOMAIN\n    allowed_domains = [DOMAIN]\n    start_urls = [\n        URL\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for url in hxs.select('//a/@href').extract():\n            if not ( url.startswith('http://') or url.startswith('https://') ):\n                url= URL + url \n            print url\n            yield Request(url, callback=self.parse)\n\nSave this in a file called spider.py.\nYou can then use a shell pipeline to post process this text:\nbash$ scrapy runspider spider.py > urls.out\nbash$ cat urls.out| grep 'example.com' |sort |uniq |grep -v '#' |grep -v 'mailto' > example.urls\n\nThis gives me a list of all the unique urls in my site.\n"", '\nsomething cleaner (and maybe more useful) would be using LinkExtractor\nfrom scrapy.linkextractors import LinkExtractor\n\n    def parse(self, response):\n        le = LinkExtractor() # empty for getting everything, check different options on documentation\n        for link in le.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n\n']",https://stackoverflow.com/questions/9561020/how-do-i-use-the-python-scrapy-module-to-list-all-the-urls-from-my-website,web-crawler
Java Web Crawler Libraries,"
I wanted to make a Java based web crawler for an experiment. I heard that making a Web Crawler in Java was the way to go if this is your first time. However, I have two important questions.

How will my program 'visit' or 'connect' to web pages? Please give a brief explanation. (I understand the basics of the layers of abstraction from the hardware up to the software, here I am interested in the Java abstractions)
What libraries should I use? I would assume I need a library for connecting to web pages, a library for HTTP/HTTPS protocol, and a library for HTML parsing.

",45k,"
            22
        ","['\nCrawler4j is the best solution for you,\nCrawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in 5 minutes!\nAlso visit. for more java based web crawler tools and brief explanation for each.\n', '\nThis is How your program \'visit\' or \'connect\' to web pages.  \n    URL url;\n    InputStream is = null;\n    DataInputStream dis;\n    String line;\n\n    try {\n        url = new URL(""http://stackoverflow.com/"");\n        is = url.openStream();  // throws an IOException\n        dis = new DataInputStream(new BufferedInputStream(is));\n\n        while ((line = dis.readLine()) != null) {\n            System.out.println(line);\n        }\n    } catch (MalformedURLException mue) {\n         mue.printStackTrace();\n    } catch (IOException ioe) {\n         ioe.printStackTrace();\n    } finally {\n        try {\n            is.close();\n        } catch (IOException ioe) {\n            // nothing to see here\n        }\n    }\n\nThis will download source of html page.\nFor HTML parsing see this\nAlso take a look at jSpider and jsoup\n', ""\nRight now there is a inclusion of many java based HTML parser that support visiting and parsing the HTML pages.\n\nJsoup\nJaunt API\nHtmlCleaner\nJTidy\nNekoHTML\nTagSoup\n\nHere's the complete list of HTML parser with basic comparison.\n"", '\nHave a look at these existing projects if you want to learn how it can be done:\n\nApache Nutch\ncrawler4j\ngecco\nNorconex HTTP Collector\nvidageek crawler\nwebmagic\nWebmuncher\n\nA typical crawler process is a loop consisting of fetching, parsing, link extraction, and processing of the output (storing, indexing). Though the devil is in the details, i.e. how to be ""polite"" and respect robots.txt, meta tags, redirects, rate limits, URL canonicalization, infinite depth, retries, revisits, etc.\n\nFlow diagram courtesy of Norconex HTTP Collector.\n', ""\nFor parsing content, I'm using Apache Tika.\n"", '\nI come up with another solution to propose that no one mention. There is a library called Selenum it is is an open-source automating testing tool used for automating web applications for testing purposes, but is certainly not limited to only this . You can write a web crawler and get benefited from this automation testing tool just as a human would do.\nAs an illustration, i will provide to you a quick tutorial to get a better look of how it works. if you are being bored to read this post take a look at this Video to understand what capabilities this library can offer in order to crawl web pages.\nSelenium Components\nTo begin with Selenium consist of various components that coexisted in a unique process and perform their action on the java program. This main component is called Webdriver and it must be included in your program in order to make it working properly.\nGo to the following site here and download the latest release for your computer OS (Windows, Linux, or MacOS). It is a ZIP archive containing chromedriver.exe. Save it on your computer and then extract it to a convenient location just as C:\\WebDrivers\\User\\chromedriver.exe We will use this location later in the java program.\nThe next step is to inlude the jar library. Assuming you are using maven project to build the java programm you need to add the follow dependency to your pom.xml\n<dependency>\n <groupId>org.seleniumhq.selenium</groupId>\n <artifactId>selenium-java</artifactId>\n <version>3.8.1</version>\n</dependency>\n\nSelenium Web driver Setup\nLet us get started with Selenium. The first step is to create a ChromeDriver instance:\nSystem.setProperty(""webdriver.chrome.driver"", ""C:\\WebDrivers\\User\\chromedriver.exe);\nWebDriver driver = new ChromeDriver();\n\nNow its time to get deeper in code.The following example shows a simple programma that open a web page and extract some useful Html components. It is easy to understand, as it has comments that explain the steps clearly. Please take a brief look to understand how to capture the objects\n//Launch website\n      driver.navigate().to(""http://www.calculator.net/"");\n\n      //Maximize the browser\n      driver.manage().window().maximize();\n\n      // Click on Math Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[3]/a"")).click();\n\n      // Click on Percent Calculators\n      driver.findElement(By.xpath("".//*[@id = \'menu\']/div[4]/div[3]/a"")).click();\n\n      // Enter value 10 in the first number of the percent Calculator\n      driver.findElement(By.id(""cpar1"")).sendKeys(""10"");\n\n      // Enter value 50 in the second number of the percent Calculator\n      driver.findElement(By.id(""cpar2"")).sendKeys(""50"");\n\n      // Click Calculate Button\n      driver.findElement(By.xpath("".//*[@id = \'content\']/table/tbody/tr[2]/td/input[2]"")).click();\n\n\n      // Get the Result Text based on its xpath\n      String result =\n         driver.findElement(By.xpath("".//*[@id = \'content\']/p[2]/font/b"")).getText();\n\n\n      // Print a Log In message to the screen\n      System.out.println("" The Result is "" + result);\n\nOnce you are done with your work, the browser window can be closed with:\ndriver.quit();\n\nSelenium Browser Options\nThere too much functionality you can implement when you working with this library, For example, assuming you are using chrome you can add in your code\nChromeOptions options = new ChromeOptions();\n\nTake look at how we can use WebDriver to open Chrome extensions using ChromeOptions\noptions.addExtensions(new File(""src\\test\\resources\\extensions\\extension.crx""));\n\nThis is for using Incognito mode\noptions.addArguments(""--incognito"");\n\nthis one for disabling javascript and info bars\noptions.addArguments(""--disable-infobars"");\noptions.addArguments(""--disable-javascript"");\n\nthis one if you want to make the browser scraping silently and hide browser crawling in the background\noptions.addArguments(""--headless"");\n\nonce you have done with it then\nWebDriver driver = new ChromeDriver(options);\n\nTo sum up let\'s see what Selenium has to offer and make it a unique choice compared with the other solutions that proposed on this post thus far.\n\nLanguage and Framework Support\nOpen Source Availability\nMulti-Browser Support\nSupport Across Various Operating Systems\nEase Of Implementation\nReusability and Integrations\nParallel Test Execution and Faster Go-to-Market\nEasy to Learn and Use\nConstant Updates\n\n', '\nI recommend you to use the HttpClient library. You can found examples here.\n', '\nI think jsoup is better than others, jsoup runs on Java 1.5 and up, Scala, Android, OSGi, and Google App Engine.\n', '\nI would prefer crawler4j. Crawler4j is an open source Java crawler which provides a simple interface for crawling the Web. You can setup a multi-threaded web crawler in few hours. \n', '\nYou can explore.apache droid or apache nutch to  get the feel of java based crawler\n', '\nThough mainly used for Unit Testing web applications, HttpUnit traverses a website, clicks links, analyzes tables and form elements, and gives you meta data about all the pages.  I use it for Web Crawling, not just for Unit Testing. - http://httpunit.sourceforge.net/\n', '\nHere is a list of available crawler:\nhttps://java-source.net/open-source/crawlers\nBut I suggest using Apache Nutch\n']",https://stackoverflow.com/questions/11282503/java-web-crawler-libraries,web-crawler
